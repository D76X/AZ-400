
---

# Q040:

You clone a Git repository to a Linux server.

You plan to implement a Git hook that will be triggered automatically 
when the commit command is invoked in the repository.

You need to use one of the predefined Git hook files.

What should you do to ensure that the predefined file will be executed?

Select only one answer.

1. Add an extension to the script file.
2. Modify the location of the file.
3. Modify the permissions of the file.
4. Remove the extension from the script file.

---

4. Remove the extension from the script file.

To ensure that the predefined Git hook script can be executed, you **must** 
remove its existing extension (`.sample`), rather than adding the extension. 

**On a Windows server** there is no need to modify the existing permissions
of the script file. 

The file must reside in its default location.

---

[Implement Git hooks](https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/3-implement)  

Here are a few instances of where you can use Git hooks to 
uphold arrangements, guarantee consistency, and control your environment:

> Enforcing preconditions for merging
> Verifying work Item ID association in your commit message Preventing you and your team from committing faulty code
> Sending notifications to your team's chat room (Teams, Slack, HipChat)

In this recipe, we'll use the **pre-commit Git hook** to scan the commit 
for keywords from a predefined list to block the commit if it contains 
any of these keywords.

### Enable the Git hook

Open the folder `.git\hooks`, you'll find a file called `pre-commit.sample`. 
To enable it, rename it to `pre-commit` by removing the .sample extension and 
making the script executable.

> you'll find that there are a bunch of samples, but they're disabled by default.
> You commit successfully if your pre-commit script exits with a 0 (zero) otherwise, the commit fails. 

### Enable the Git hook on Windows

**If you're using Windows** simply renaming the file won't work!
Git will fail to find the shell in the chosen path specified in the script.
**The problem** is lurking in the first line of the script, the **shebang declaration**:

```
#!/bin/sh
```

On **Unix-like OSs** the `#!` (aka shebang) tells the program loader that it's a script
to be interpreted, and `/bin/sh` is the path to the interpreter you want to use, `sh` in this case.

**Windows isn't a Unix-like OS**. 
Git for Windows supports Bash commands and shell scripts via **Cygwin** that is an application 
installed on Windows with Git that is used as the interpreter!

> By default, what does it find when it looks for sh.exe at /bin/sh?
Nothing, nothing at all. 
Fix it **by providing the path to the sh executable on your system**. 
It's using the 64-bit version of Git for Windows, so the baseline looks like this:

```
#!C:/Program\ Files/Git/usr/bin/sh.exe
```

```
#!C:/Program\ Files/Git/usr/bin/sh.exe
matches=$(git diff-index --patch HEAD | grep '^+' | grep -Pi 'password|keyword2|keyword3')
if [ ! -z "$matches" ]
then
    cat <<\EOT
Error: Words from the blocked list were present in the diff:
EOT
    echo $matches
    exit 1
fi
```

### There's more

The repo `.git\hooks` folder **isn't committed into source control**. 
You may wonder how you share the goodness of the automated scripts you create with the team.
**From Git version 2.9** you can now map Git hooks to a folder that can be committed into 
source control.

You could do that by **updating the global settings configuration for your Git repository**:

`Git config --global core.hooksPath '~/.githooks'`

If you ever need to overwrite the Git hooks you have set up on the client-side, 
you can do so by using the `no-verify` switch:

`Git commit --no-verify`

---

## [Server-side service hooks with Azure Repos](https://learn.microsoft.com/en-us/azure/devops/service-hooks/events?view=azure-devops)  

So far, we've looked at the client-side Git Hooks on Windows. 

**Azure Repos** also exposes **server-side hooks**. 
Azure DevOps uses the exact mechanism itself to create Pull requests. 

#### [What Is Service Hooks?](https://k21academy.com/microsoft-azure/az-400/azure-devops-service-hooks-subscription-of-service-hooks-release-approvals/)  

Service hooks let you run tasks on other services when events happen in your Azure DevOps Services projects.
For example, build a CI in Azure DevOps and then start the CD on the Jenkins or we can send 
the notifications on the Slack channel or Teams channels if the build or deployment fails 
or even we can send the slack for Release approvals too.

Service hooks can also be used in custom apps and services as a more efficient way to drive 
activities when events happen in your projects.

Service hook publishers define a set of events. Subscriptions listen for the events and define
actions to take based on the event. Subscriptions also target consumers, which are external 
services that can run their own actions, when an event occurs.

> Creating A Subscription Of Service Hooks: Project Settings > Service Hooks > +Create Subscription
> Select a target service: there is a long list of target services:
- Jenkins
- Trello
- Web Hooks
- Windows Azure Service Bus
- Azure Storage
- Zapier
..etc.

> Configure the event: sel;ect the event you want the subscription on
These is the list of Azure DevOps Service Side events i.e. `code is pushed` then `branch: main`..

> Configure the action
The list of available actions may be limited based on the event type you selected and oviously the
service that is subscriber and therefore target of the Azure DevOps Event.
At this point you would normally have to provide a **Token** that is a secret generated on the 
subscriber i.e. Trello on your Trello's account that will be used by Azure DevOps as authentication
token when the event is raised and the wookhook is run to send to the subscriber the event payload.

---

[Nuget WebHooks Receivers](https://www.nuget.org/packages/Microsoft.AspNet.WebHooks.Receivers.vsts) 
package provides support for receiving Webhooks from Azure DevOps.
[Troubleshoot service hooks](https://learn.microsoft.com/en-us/azure/devops/service-hooks/troubleshoot?toc=%2Fazure%2Fdevops%2Fmarketplace-extensibility%2Ftoc.json&view=azure-devops)  
[Azure DevOps Integrate with service hooks](https://learn.microsoft.com/en-us/azure/devops/service-hooks/overview?view=azure-devops)

---

[ASP.NET WebHooks receivers](https://learn.microsoft.com/en-us/aspnet/webhooks/receiving/receivers)

---

### Run an Azure DevOps Pipeline triggering it throuigh a public (protected) WebHook

[Azure Pipeline - resources.webhooks.webhook definition](https://learn.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/resources-webhooks-webhook?view=azure-pipelines)  

A webhook resource enables you to integrate your pipeline with an external service to automate the workflow.

```
webhooks:
- webhook: string # Required as first property. Name of the webhook.
  connection: string # Required. Name of the connection. In case of offline webhook this will be the type of Incoming Webhook otherwise it will be the type of the webhook extension.
  type: string # Name of the webhook extension. Leave this empty if it is an offline webhook.
  filters: [ filter ] # List of trigger filters.
```

```
resources:
  webhooks:
    - webhook: WebHook
      connection: IncomingWH

steps:  
- script: echo ${{ parameters.WebHook.resource.message.title }}
```

To trigger your pipeline using the webhook, you need to make a POST request to https://dev.azure.com/<org_name>/_apis/public/distributedtask/webhooks/<webhook_connection_name>?api-version=6.0-preview. This endpoint is publicly available, and no authorization is needed. The request should have the following body.

```
{
    "resource": {
        "message": {
            "title": "Hello, world!",
            "subtitle": "I'm using WebHooks!"
        }
    }
}
```

---

# Q039:

You have an Azure Repos Git-based repository named repo1.

In repo1, you plan to store approximately 500 MB of data.

You need to minimize how long it takes to clone repo1 to a Windows computer.

Which Git extension should you use when you clone repo1?

Select only one answer.

1. Git LFS
2. Git Machete
3. GitFlow
4. GitX

---

1. Git LFS

Git LFS is a Git extension that provides the fastest response time when using Azure DevOps Git-based repositories that contain large files. 

Git LFS handles large files by storing in the repository only pointers to them, rather than the actual files. When you clone the repository, the pointer file is used to locate the large file. Git LFS supports the `git lfs clone` command that offers superior performance as compared to the traditional `git clone command`. The command downloads any required Git LFS files as a batch after the checkout is complete. The download uses parallelized downloads, which significantly improves the performance of the Windows operating system

- GitFlow : [gitflow](https://github.com/nvie/gitflow)  
is a Git extension that implements the GitFlow branching model. 

- Git Machete : https://github.com/VirtusLab/git-machete 
is a Git extension that simplifies and automates repository organization. 

- GitX 
is a Git extension that provides an improved development workflow.

---

[Work with large repositories](https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/2-work-large-repositories)

---

# Q038:

You have an Azure App Service web app named WebApp1 that has Application Insights enabled.

You need to identify the places in WebApp1 where users drop out.

Which Application Insights feature should you use?

Select only one answer.

1. availability tests
2. Profiler
3. Snapshot Debugger
4. usage analysis

---

4. usage analysis

Usage analysis provides insights into how users use an app, including whether they
drop out at certain points. 

- Snapshot Debugger 
is useful to show the state of the source code and variables when an exception occurs.

- Profiler 
provides performance traces but not application usage. 

- Availability tests 
are used to test application availability.

---

# Q037:

You implement the monitoring of a distributed application named App1 
by using Application Insights. 

App1 has several external dependencies.

You need to identify which requirement must be met for the dependencies 
to appear on the Application Map for App1.

Which requirement should you identify?

Select only one answer.

1. access to the code of the dependency component
2. access to the telemetry of the dependency component
3. access via HTTP dependency calls from App1
4. the same instrumentation key as App1


---

3. access via HTTP dependency calls from App1

For dependencies of App1 to appear on the Application Map, dependency resources
**must be accessible via HTTP dependency calls**. 

- Access to the code of dependency resources
- access to the telemetry of dependency resources 
- using the same instrumentation key 

are not requirements.

---

# Q036:

You plan to implement an Azure Pipelines release pipeline that will deploy 
Azure resources to development and production environments.

You need to prevent deployment to the production environment if the 
Azure platform raises alerts about issues affecting the development environment.

Which integration should you add to the release pipeline?

Select only one answer.

1. artifact policy
2. Azure Monitor
3. exclusive lock
4. GitHub Actions

---

2. Azure Monitor

Azure Monitor can be used in a release pipeline to detect whether active alerts
are triggered and block or allow the next step. 

- An artifact policy 
is used to check and evaluate artifacts. 

- Exclusive lock 
allows only a single run from the pipeline to proceed. 

- GitHub Actions 
are used to trigger an Azure pipeline to run directly from a GitHub Actions workflow.

---

[AzureMonitor@1 - Query Azure Monitor alerts v1 task](https://learn.microsoft.com/en-gb/azure/devops/pipelines/tasks/reference/azure-monitor-v1?view=azure-pipelines&viewFallbackFrom=azure-devops)  

```
# Query Azure Monitor alerts v1
# Observe the configured Azure Monitor rules for active alerts.
- task: AzureMonitor@1
  inputs:
    connectedServiceNameARM: # string. Required. Azure subscription. 
    ResourceGroupName: # string. Required. Resource group. 
  # Advanced
    filterType: 'none' # 'resource' | 'alertrule' | 'none'. Required. Filter type. Default: none.
    #resource: # string. Required when filterType = resource. Resource. 
    #alertRule: # string. Required when filterType = alertrule. Alert rule. 
    #severity: 'Sev0,Sev1,Sev2,Sev3,Sev4' # 'Sev0' | 'Sev1' | 'Sev2' | 'Sev3' | 'Sev4'. Severity. Default: Sev0,Sev1,Sev2,Sev3,Sev4.
    #timeRange: '1h' # '1h' | '1d' | '7d' | '30d'. Time range. Default: 1h.
    #alertState: 'Acknowledged,New' # 'New' | 'Acknowledged' | 'Closed'. Alert state. Default: Acknowledged,New.
    #monitorCondition: 'Fired' # 'Fired ' | 'Resolved'. Monitor condition. Default: Fired.
```

---

# Q035:

You manage 100 on-premises servers that run Linux or Windows.

You need to collect logs from all the servers and make the logs available 
for analysis directly from the Azure portal. 

The solution must meet the following requirements:

> Provide the ability to send data from Linux virtual machines to multiple Log Analytics workspaces.
> Support the use of XPATH queries to filter Windows events for collection.
> Minimize the number of agents installed on the servers.

Which agent should you install?

Select only one answer.

1. Azure Connected Machine agent
2. Azure Monitor Agent
3. Dependency agent
4. Telegraph agent

---

2. Azure Monitor Agent

The Azure Monitor Agent **replaces** 

- the Log Analytics agent 
- the diagnostic extension 
- the Telegraph agent 

It can centrally configure data collection for different data from different sets of virtual machines,
sending the data from Linux virtual machines to multiple workspaces and using XPATH queries to filter
Windows events for collection. 

- The Telegraph agent 
supports only Linux operating systems. 

- The Azure Connected Machine agent 
is used by Azure Arc, which is not required in this scenario. 

- The Dependency agent 
increases the number of agents installed on the target servers.

---

[Collect events and performance counters from virtual machines with Azure Monitor Agent](https://learn.microsoft.com/en-gb/azure/azure-monitor/agents/data-collection-rule-azure-monitor-agent?tabs=portal)  


---

# Q034:

You manage the deployment of an Azure App Service web app named App1 
to multiple on-premises locations.

You plan to implement Application Insights to monitor App1.
You need to authorize App1 to access an Application Insights resource.

Which authorization method should you use?

Select only one answer.

1. access key
2. instrumentation key
3. Microsoft Entra managed identity
4. Microsoft Entra security principal

---

2. instrumentation key

An instrumentation key is used to authorize access to an Application Insights resource.

- An access key 
is used to authorize access to an Azure Storage account. 

- Microsoft Entra security principals and managed identities 
are not supported for authorizing access to an Application Insights resource.

---

# Q033:

You manage the deployment of an Azure App Service web app named App1 
to multiple Azure regions.
You need to identify which Azure service enables you to validate the
availability of App1 from multiple locations around the world.

What should you identify?

Select only one answer.

1. Application Insights
2. Azure Advisor
3. Azure App Configuration
4. Azure Service Health

---

1. Application Insights

Application Insights **includes a built-in functionality** that allows you 
**to validate the availability of an app from multiple locations** around 
the world. 

- App Configuration 
is a key/value store that you can use to facilitate the implementation of 
feature flags. 

- Azure Advisor 
detects misconfiguration issues. 

- Azure Service Health 
provides information about the status of platform services, not individual apps.

---

[Application Insights availability tests](https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-overview)  

After you deploy your web app or website, you can set up recurring tests to monitor **availability and responsiveness**. **Application Insights sends web requests to your application at regular intervals from points around the world**.
It can **alert** you if your application isn't responding or responds too slowly.

You **can set up availability tests for any HTTP or HTTPS endpoint that's accessible from the public internet**.
**You don't have to make any changes to the website you're testing**. 
In fact, **it doesn't even have to be a site that you own**. 
You **can test the availability of a REST API that your service depends on**.

### Types of Tests:

- Standard test: 
This single request test is **similar to the URL ping test**. 
It includes: 
  > TLS/SSL certificate validity
  > proactive lifetime check
  > HTTP request verb (for example, GET, HEAD, or POST)
  > custom headers
  > custom data associated with your HTTP request.

- Custom TrackAvailability test: 
If you decide to create a custom application to run availability tests, 
you can use the `TrackAvailability()` method to send the results to Application Insights.

- Classic tests (older versions of availability tests):
 - URL ping test: 
You can **create this test through the Azure portal** to validate whether an endpoint is
 responding and measure performance associated with that response. You can also set custom
 success criteria coupled with more advanced features, like parsing dependent requests and
 allowing for retries.

  - Multi-step web test (deprecated): 
You can play back this recording of a sequence of web requests to test more complex scenarios.
Multi-step web tests **are created in Visual Studio Enterprise** and **uploaded to the portal**,
where you can run them.

#### Dewpendance on the DNS public infrastructure

The older classic tests: 
- URL ping test 
- multi-step web test 

**rely on the DNS infrastructure of the public internet to resolve the domain names** of the 
tested endpoints. If you're using private DNS, you must ensure that the public domain name 
servers can resolve every domain name of your test. 
**When that's not possible, you can use custom TrackAvailability tests instead**.

---

# Q032:

You have an Azure DevOps organization named Organization1 that contains 
a project named Project1. 
Project1 contains a CI/CD YAML pipeline named Pipeline1 that deploys 
resources to an Azure subscription.

You need to ensure that an attempt to access the subscription from any YAML
pipelines other than Pipeline1 must be explicitly authorized.

What should you configure?

Select only one answer.

1. Microsoft Entra managed identity
2. Organization permissions
3. Pipeline permissions
4. Project permissions

---

3. Pipeline permissions

**Using Pipeline permissions, you can specify which pipeline can consume the service connection**.
If any other YAML pipelines refer to the service connection, an authorization request is raised,
**which must be approved by a connection administrator**. 

**Project permissions** are used to control service connection sharing between projects but not for pipelines. 

**Organization permissions** define security groups that control the service connections within an organization. 

A **Microsoft Entra managed identity** is used instead of a service principal for a service connection, 
but not to request approval to use the service connection.

---

[Manage service connections - Pipeline permissions](https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#pipeline-permissions)  

[Training -Provision and test environments](https://learn.microsoft.com/en-gb/training/modules/configure-provision-environments/)  

---

# Q031:

You plan to configure GitHub Actions to access GitHub secrets.
You have the following YAML.

```
01  steps:
02    - shell: pwsh
03    
04        DB_PASSWORD: ${{ secrets.DBPassword }}
```

You need to complete the YAML to reference the secret.
The solution must minimize the possibility of exposing secrets.
Which element should you use at line 03?

Select only one answer.

1. args:
2. $env:
3. env:
4. run:

---

3. env:

**The most secure way to pass secrets to run commands** is to reference them
as environment variables, **rather than arguments**. This requires the use of
the `env:` element. 

The `$env:` notation is used to reference an environment variable, 
but the intention of this question is to define rather than reference. 

The `run:` element defines which command to run, so **it follows** the env: notation.

---

[Learn continuous integration with GitHub Actions  - Use secrets in a workflow](https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/9-use-secrets-workflow)

Secrets aren't passed automatically to the runners when workflows are executed.
Instead, when you include an action that requires access to a secret, you use the 
secrets context to provide it.

```
steps:

  - name: Test Database Connectivity
    with:
      db_username: ${{ secrets.DBUserName }}
      db_password: ${{ secrets.DBPassword }}
```

> Command-line secrets:

Secrets shouldn't be passed directly as command-line arguments as 
they may be visible to others. Instead, treat them like environment variables:

> Limitations
Workflows can use **up to 100 secrets**, and they're **limited to 64 KB** in size.

```
steps:
  - shell: pwsh
    env:
      DB_PASSWORD: ${{ secrets.DBPassword }}
    run: |
      db_test "$env:DB_PASSWORD"
```

---

# Q030:

You have a Git repository.

You apply a commit that introduces an unintended change to the target branch of the repository.
You need to run a Git command that will undo the change applied by the commit.

Which command you should run?

Select only one answer.

1. git clean
2. git rebase
4. git reset --mixed HEAD
4. git revert

--

`git revert` 
allows you **to undo a change in a shared branch**. 

`git rebase` 
integrates source branch commits into the target branch, it does not undo changes. 

`git clean` [git clean](https://git-scm.com/docs/git-clean)
deletes untracked files but does not undo a commit. 

`git reset --mixed HEAD` 
undoes the staging of files but keeps file changes. 
It does not affect commits.

---

[Azure Repo Undo Changes](https://learn.microsoft.com/en-gb/azure/devops/repos/git/undo?view=azure-devops&tabs=visual-studio-2022#revert-changes-in-shared-commits)

---

# Q029:

You have a project in Azure DevOps that has a Git repository.

You plan to define a pull request strategy for the repository.
You need to identify the Git branch policy that will condense 
the source branch commits into a single commit on the target branch.

Which branch policy should you implement?

Select only one answer.

1. Automatically include code reviewers.
2. Check for comment resolution.
3. Limit allowed merge types.
4. Require a minimum number of reviewers.

---

3. Limit allowed merge types.

You can limit the allowed merge types to:
- squash merge 
which can be used to condense the history of changes in your default branch. 

Requiring a minimum number of reviewers is not used to reduce the history of merge actions. 

Checking for comment resolution is useful to verify that reviewers’ comments are resolved 
but not to condense the history of changes. 

A policy can be defined to include specific reviewers, 
but it is unable to clean the branch history.

---

[Branch policies and settings](https://learn.microsoft.com/en-gb/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser#limit-merge-types)  

---

# Q028:

You have an Azure DevOps Agile-based project that includes several team members
who use build and release pipelines based on continuous integration.

You notice that some pull requests are not associated with requirements captured by 
user stories.

You need to ensure that pull requests are blocked unless such an association exists.

Which branch policy you should implement?

Select only one answer.

1. Automatically include code reviewers.
2. Check for comment resolution.
3. Check for linked work items.
4. Require a minimum number of reviewers.

---

3. Check for linked work items.

In Agile projects, requirements are implemented as User Story work item types. You can require associations between pull requests and work items. Linking work items provides more context for changes and ensures that updates go through your work item tracking process. 

Requiring a minimum number of reviewers is not used to link a work item with a pull request. 

Checking for comment resolution is useful to verify that reviewers’ comments are resolved, 
but not to link a work item with a pull request. 

A policy can be defined to include specific reviewers, 
but it cannot link a work item to the pull request.


[Branch policies and settings - Check for linked work items](https://learn.microsoft.com/en-gb/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser#check-for-linked-work-items)

[Collaborate with pull requests in Azure Repos](https://learn.microsoft.com/en-gb/training/modules/collaborate-pull-requests-azure-repos/)  

---

# Q027:

You have an Azure DevOps organization.
You plan to use a new Git repository.
You need to define a branch strategy. 
The solution must track all detected bugs in the code in an isolated code configuration.

Which branch strategy should you implement?

Select only one answer.

1. development
2. feature
3. release
4. tags

---

2. feature

Feature provides isolation to work with new features or bugs. 

Development:
is used in **Team Foundation version control** and is not specific to bug fix isolation. 

Release:
is used when the code is ready to be implemented. 

Tags:
 are useful to mark a point in the code history as important. 
 Tags introduce extra steps that are unnecessary if branches are used for releases.

---

# Q026:

You plan to use a forking workflow as the Git branching strategy for your company.

You need to identify the minimum number of repositories that each developer should use.

What is the minimum number of repositories you should identify?

Select only one answer.

1
2
3
4

---

2

When using a **forking workflow**, each developer should have two repositories, 
**one private local side** and **the other public server-side**. 

While it is technically possible to use only a server-side repository, this
violates the principle of the forking workflow.

---

[Forks on Azure Repos](https://learn.microsoft.com/en-gb/azure/devops/repos/git/forks?view=azure-devops&tabs=visual-studio)  

Git repo forks are useful when people want to make **experimental, risky, or confidential** 
changes to a codebase, but those **changes need to be isolated from the codebase** in the 
original repo.

A new fork is basically a clone of the original repo pushed to a new remote repo. 
**The fork is independent of the original repo**, and is a complete copy unless you specify 
a single branch.



### Share code between forks

The **original repo** is often referred to as the **upstream repo**. 
**You can create PRs to merge changes in either direction**: 
- from fork to upstream 
- upstream to fork 

The most common direction is from fork to upstream. 
The destination repo's permissions, policies, builds, and work items will apply to the PR.

### Choose between branches and forks

For a **small team of 2-5 developers** a forking workflow might not be necessary because
everyone can work in feature branches and branch policies can protect the default branch.

> When the forking workflow may be used:

However, if your team expands and outgrows this arrangement they can switch to a forking workflow.
If your repo has **a large number of casual or infrequent committers**, such as an open source 
project might, we recommend the forking workflow.
Typically, only core contributors to your project should have direct commit rights to your original
repo.
Other collaborators should use a forking workflow to isolate their proposed changes until the 
core contributors have a chance to review their work.

[Enable forks on Azure DevOps](https://learn.microsoft.com/en-gb/azure/devops/repos/git/repository-settings?view=azure-devops&tabs=browser#enable-forks)  

> Project Settings > Repositories > repository > Settings > set Forks to On/Off

[The forking workflow](https://learn.microsoft.com/en-gb/azure/devops/repos/git/forks?view=azure-devops&tabs=visual-studio#the-forking-workflow)  


---

# Q025:

You plan to use a trunk-based development workflow as the Git branching strategy
for your company.

You are documenting the steps of the workflow.

You need to identify the step that immediately follows creating a feature branch.

Which step should you identify?

Select only one answer.

1. adding commits
2. code review
3. deployment
4. opening a pull request

---

1. adding commits

**Adding commits follows creating a feature branch in a trunk-based development workflow**. 

**Opening a pull request** follows adding commits to a trunk-based development workflow. 
**Code review follows opening a pull request** in a trunk-based development workflow. 
**Deployment follows code review** in a trunk-based development workflow.

- trunk-based development workflow:
> Create a Feature Branch > Add Commits > Open a PR > Code Review > Deployment

---

[Explore feature branch workflow](https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/3-explore-feature-branch-workflow)

The core idea behind the Feature Branch Workflow is that all feature development
should take place in a dedicated branch instead of the main branch.

The encapsulation makes it easy for multiple developers to work on a particular feature
without disturbing the main codebase. It also means the main branch will never contain
broken code, a huge advantage for continuous integration environments.

Encapsulating feature development also makes it possible to use pull requests, which 
are a way to start discussions around a branch. They allow other developers to sign out
on a feature before it integrates into the official project. Or, if you get stuck in 
the middle of a feature, you can open a pull request asking for suggestions from your 
colleagues.

Pull requests make it incredibly easy for your team to comment on each other's work. 
Also, feature branches can (and should) be pushed to the central repository. It allows
 sharing a feature with other developers without touching any official code.

Since the main is the only "special" branch, storing several feature branches on the 
central repository doesn't pose any problems. It's also a convenient way to back up
everybody's local commits.

---

# Q024:

You plan a Git branching strategy for your company.

You need to ensure that the strategy will allow individual developers to 
have their own server-side repository.

Which branching strategy should you use?

Select only one answer.

1. centralized
2. forking
3. GitFlow
4. trunk-based

---

2. forking

A forking workflow is **the only Git-based workflow** in which **each developer** 
**has their own server-side repository**. 

For all others, developers share the same server-side repository.

---

[Branch strategically](https://learn.microsoft.com/en-gb/azure/devops/repos/tfvc/branch-strategically?view=azure-devops)

---


# Q023:

You plan a Git branching strategy for your company.

You need to ensure that the strategy will dictate that pull requests must 
deploy to production for testing before they can merge to the main branch.

Which branching strategy should you use?

Select only one answer.

1. centralized
2. forking
3. GitHub Flow
4. trunk-based

---

3. GitHub Flow

**GitHub Flow is a popular trunk-based development release flow**, which stipulates 
that pull requests must deploy to production for testing before they can merge to the 
main branch. This process means that all pull requests wait in the deployment queue
for the merge. 

This provision is not part of the centralized, forking, or trunk-based strategy.

---

[Explore branch workflow types](https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/2-explore-branch-workflow-types)

> Forking workflow:
The Forking Workflow is fundamentally different than the other workflows discussed 
in this tutorial.

Instead of using a single server-side repository to act as the "central" codebase, 
it gives every developer a server-side repository.

It means that each contributor has two Git repositories:
- A private local one.
- A public server-side one.

---

[Adopt a Git branching strategy](https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops)  

[How Microsoft develops with DevOps](https://learn.microsoft.com/en-gb/devops/develop/how-microsoft-develops-devops)  


---

[Branch Strategies](https://app.pluralsight.com/ilx/video-courses/675a1cc4-be1f-4660-8afd-4c2d6f3d81d7/a112424c-bbdb-4eaa-9f94-ecbcce6ecba9/5947b0ee-74ca-4893-b3c8-2b5d028998eb)  

1. [Trunk-Based Branching]
This is to be used for **quick branches**. 
In this strategy any single change goes straint into the main branch (The Trunk).

| Pros                            |  Cons                      |
| ------------------------------- | -------------------------- |
| Applicable to very small teams  |  Large code review process |

2. [Feature (Task) Branching]

This is **a branch per User Story** strategy.

| Pros                                            |  Cons                      |
| ----------------------------------------------- | -------------------------- |
| Enables independent and experimental innovation | **Long running feature branches may become difficult to merge back into the main branch |
| Easy to segment                                 |  Large code review process    |   
| It makes it easy to build CI/CD workflows       |                               |

3. [Feature Flag Branching]

This is the same as **Feature (Task) Branching** but if fixes the problem ** by using flags to activate or deactivate 
the feature so that even incomplete features cn be safely and often merged back into the main branch without being 
active and therefore even when they are incopleted or not thorouhhly tested.

4. [Release Branching]

This strategy merges all the **User Story Branches** and **merges them all in a single branch**.
Branches are created **for all features per each release** from the main branch. 
At the beginning of a Release Cycle **a Release Branch is created from the main branch** then  
all **Feature Branches and Bug Fixes branches** are created starting from the feauture branch. 
The feature branches and bug fixes branches are merged back into the release branch and only 
when the Release Branch is consolidated and released as a new version of the software it is 
then **merged back into the main branch** before stating the next release cycle. 

| Pros                                            |  Cons                      |
| ----------------------------------------------- | -------------------------- |
| If you must support multiple versions       *1  | Difficult to maintain |
| If you must support multiple custumizations *2  | Cannot have many changes or contributors |
| It allows the team to focus on a specific topic | It generates more work for the team per each version |


---

# Q023:

Your team uses clones of an Azure DevOps Git repository to collaborate on a shared project. 
Commits pushed to the repository include large binary files.

A new employee joins the team and clones the repository but cannot find any large binary 
files in the local clone. All the other team members can locate large binary files in their
local clones.

You need to recommend a solution that will enable the new employee to locate the large binary 
files after creating a local clone.

What should you recommend?

Select only one answer.

1. Ask the employee to run az repos create.
2. Ask the employee to run git clone git@ssh.dev.azure.com.
3. Install the GitX client on the new employee’s computer.
4. Install the Git LFS client on the new employee’s computer.

---

4. Install the Git LFS client on the new employee’s computer.

The Git LFS client must be installed and configured properly to allow you 
to see binary committed files. 

`az repos create`: creates a new Git repository by using Azure Repos. 

`git clone git@ssh.dev.azure.com`: 
clones a repository by using **SSH**, which is **unsupported in Azure Repos** 
**when Git LFS is used to track files**. 

`GitX`:
is a Git extension that provides an improved development workflow.

---

[Training: Manage Git repositories](https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/)

> Work with large repositories:

There are two primary causes for large repositories:
- Long history
- Large binary files

> Shallow clone:
If developers don't need all the available history in their local repositories, 
**a good option** is to implement a shallow clone.
`git clone --depth [depth] [clone-url]`

> VFS for Git:
VFS for Git helps with large repositories. It requires a **Git LFS client**.

[VFS for Git](https://github.com/microsoft/VFSForGit)
VFS for Git virtualizes the file system beneath your Git repository so that Git 
and all tools see what appears to be a regular working directory, but VFS for Git
only downloads objects as they are needed.

> Scalar:
Scalar is a .NET Core application available for Windows and macOS. 
With tools and extensions for Git to allow very large repositories to maximize your
Git command performance. **Microsoft uses it for Windows and Office repositories**.
It achieves by enabling some advanced Git features, such as:

- Partial clone: 
reduces time to get a working repository by not downloading all Git objects right away.
- Background prefetch: 
downloads Git object data from all remotes every hour, reducing the time for foreground
git fetch calls.
- Sparse-checkout: 
limits the size of your working directory.
- File system monitor: 
tracks the recently modified files and eliminates the need for Git to scan the entire work tree.
- Commit-graph:
accelerates commit walks and reachability calculations, speeding up commands like git log.
- Multi-pack-index: 
enables fast object lookups across many pack files.
- Incremental repack: 
Repacks the packed Git data into fewer pack files without disrupting concurrent commands 
using the multi-pack-index.

---


# Q022:

You have an Azure DevOps organization that contains multiple projects, 
including a project named Project1. 
All projects contain CI/CD pipelines.

You detect that the pipeline in Project1 is blocking other pipelines 
from starting due to its long run times.
You need to identify which pipeline setting will reduce the duration of 
the pipeline runs.

Which setting should you identify?

Select only one answer.

1. dependencies
2. maxParallel configuration
3. multi-job configuration
4. timeouts

---

3. multi-job configuration

Multi-job configuration is used for pipeline performance optimization, 
**allowing you to run multiple jobs on multiple agents in parallel**. 

- maxParallel: 
is used to limit parallelism, since if it is not set, no limit is applied.

**Timeouts** are used to set a limit on how long a job can run, but they 
do not improve performance. 

**Dependencies** are used to control multiple jobs in a single agent.

---

[Azure Pipelines - Multi-job configuration](https://learn.microsoft.com/en-gb/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml#multi-job-configuration)

**From a single job** you author, you can **run multiple jobs on multiple agents in parallel**. 
Some examples include:

**Multi-configuration builds**: 
You can build multiple configurations in parallel. For example, you could build 
a Visual C++ app for both debug and release configurations on both x86 and x64 
platforms. 
To learn more, see Visual Studio Build - multiple configurations for multiple platforms.

**Multi-configuration deployments**: 
You can run multiple deployments in parallel, for example, to different geographic regions.

**Multi-configuration testing**: 
You can run test multiple configurations in parallel.

Multi-configuration will always generate at least one job, even if a 
multi-configuration variable is empty.

```
jobs:
- job: Test
  strategy:
    maxParallel: 2
    matrix: 
      US_IE:
        Location: US
        Browser: IE
      US_Chrome:
        Location: US
        Browser: Chrome
      Europe_Chrome:
        Location: Europe
        Browser: Chrome
```

---

[Implement multi-agent builds](https://learn.microsoft.com/en-gb/training/modules/implement-pipeline-strategy/3-implement-multi-agent-builds)   

You can use multiple build agents to:

- support multiple build machines 
- distribute the load
- run builds in parallel 
- or use different agent capabilities

Adding multiple jobs to a pipeline lets you:

- Break your pipeline into sections that need different agent pools or self-hosted agents.
- Publish artifacts in one job and consume them in one or more subsequent jobs.
- Enable conditional execution of tasks
- Build faster by running multiple jobs in parallel:
At the organization level, you can configure the number of parallel jobs that are made available.
The free tier allows for one parallel job of up to 1800 minutes per month.
You can define a build as a collection of jobs rather than as a single job. 
Each job consumes one of these parallel jobs that run on an agent. 
If there aren't enough parallel jobs available for your organization, 
the jobs will be queued and run sequentially.

---

[Azure Pipelines agents - Parallel jobs](https://learn.microsoft.com/en-gb/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=yaml%2Cbrowser#parallel-jobs)

Parallel jobs represents the number of jobs you can run at the same time in your organization. If your organization has a single parallel job, you can run a single job at a time in your organization, with any additional concurrent jobs being queued until the first job completes. To run two jobs at the same time, you need two parallel jobs. In Azure Pipelines, you can run parallel jobs on Microsoft-hosted infrastructure or on your own (self-hosted) infrastructure.

Microsoft provides a free tier of service by default in every organization that includes at least one parallel job. Depending on the number of concurrent pipelines you need to run, you might need more parallel jobs to use multiple Microsoft-hosted or self-hosted agents at the same time. For more information on parallel jobs and different free tiers of service, see Parallel jobs in Azure Pipelines.

---

[Specify jobs in your pipeline - Slicing](https://learn.microsoft.com/en-gb/azure/devops/pipelines/process/phases?view=azure-devops&tabs=yaml#slicing)  

An agent job can be used **to run a suite of tests in parallel**. 
For example, you can run a large suite of 1000 tests on a single agent. 
Or, you can use two agents and run 500 tests on each one in parallel.

**To apply slicing**, the tasks in the job should be smart enough to understand
the slice they belong to.

The **Visual Studio Test task** is one such task that **supports test slicing**. 
If you **installed multiple agents**, you can specify how the Visual Studio Test task
runs in parallel on these agents.

---

# Q021:

You have an Azure DevOps organization that hosts a project named Project1. 
Project1 includes multiple tests.

You need to identify which tests in Project1 provide different outcomes, 
such as pass or fail, even when there are no changes to the source code.

Which Azure DevOps feature should you use?

Select only one answer.

1. Flaky test detection
2. Pipeline pass rate
3. Test pass rate
4. Test Results Trend

---

1. Flaky test detection

Flaky test detection **is a feature configured at the project level** and
**supports system and custom detection**. 

**Test pass rate** and **Pipeline pass rate** are (IN-CONTEXT) reports on the 
**pipeline’s Analytics tab**. 

**Test Results Trend** is a **widget** used to track test results.

---

[Manage flaky tests](https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/flaky-test-management?view=azure-devops)  

> Azure DevIOps Pipeline > Test Management > Flaky test options:

1. Falky tests included in tests pass percentage
Uncheck to prevent pipeline failure due to flaky tests.

2. Allow users to manually mark & unmark flaky tests

- System detection: 
The in-product flaky detection uses test rerun data. The detection is via 
`VSTest task` rerunning of failed tests capability or retry of stage in the pipeline.
You can select specific pipelines in the project for which you would like to detect 
flaky tests.

- Custom detection: 
You can integrate your own flaky detection mechanism with Azure Pipelines and use 
the reporting capability. With custom detection, you need to update the test results 
metadata for flaky tests. For details, see Test Results, Result Meta Data - Update REST API.

[Run quality tests in your build pipeline by using Azure Pipelines](https://learn.microsoft.com/en-gb/training/modules/run-quality-tests-build-pipeline/)  

---

# Q020:

You manage a GitHub organization named contoso that contains multiple repositories.
For each repository, you plan to create a multi-job workflow by using GitHub Actions
and share artifacts between jobs.

You need to ensure that the artifacts are retained for up to 10 days for all the 
GitHub Actions. 

The solution must minimize development effort.

At which levels should you configure the retention limit?

Select only one answer.

1. actions
2. organization
3. repositories
4. workflow

---

2. organization

Setting the retention limits at the organization level enforces 
the maximum retention limit for all actions. 

While **you can define the retention limit for each action or repository**, 
this requires more effort to implement and maintain compared to configuring
this at the organization level. 

Retention limits **are not configurable at the workflow level**.

[CI Integration with HitHub Actions](https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/4-share-artifacts-between-jobs)  

### Share artifacts between jobs

When using Actions to create CI or CD workflows, you'll often need to pass artifacts created by one job to another.

> Upload-artifact Action:

This action can upload one or more files from your workflow to be shared between jobs.

```
- uses: actions/upload-artifact
  with:
    name: harness-build-log
    path: bin/output/logs/harness.log
     retention-days: 12 # You can set a custom retention period when uploading, 
                        # but it can't exceed the defaults for the repository, organization, or enterprise.
    #  path: bin/output/logs/harness[ab]?/* # can use wildcards
```

For en entire folder:

```
- uses: actions/upload-artifact
  with:
    name: harness-build-logs
    path: bin/output/logs/
```

You can specify multiple paths:

```
- uses: actions/upload-artifact
  with:
    name: harness-build-logs
    path: |
        bin/output/logs/harness.log
        bin/output/logs/harnessbuild.txt
```

> Download-artifact

```
- uses: actions/download-artifact
  with:
    name: harness-build-log

```

---

# Q019:

You use Azure Pipelines to deploy applications.

You need to publish Microsoft Power BI reports that show Azure Pipelines analytics. 
The solution must minimize development effort.

What should you create?

Select only one answer.

1. a GraphQL feed
2. an Azure Data Factory pipeline
3. an Azure Synapse Analytics pipeline
4. an OData feed

---

4. an OData feed

An **OData feed** provides a straightforward approach to publishing reports 
that show Azure Pipelines analytics. 

**GraphQL, Azure Data Factory, and Azure Synapse Analytics** pipelines can potentially
be used in this case, but they require more significant development effort.

---

[Exercise - Monitor the health of your pipeline](https://learn.microsoft.com/en-gb/training/modules/create-release-pipeline/6-monitor-pipeline-health)  

Examine the analytics features that Azure Pipelines provide.

### How can I track the health of my pipeline?

You could use a dashboard.
let's look at some of the built-in analytics that Azure Pipelines provides.

### What information does pipeline analytics provide?

Every pipeline provides reports that include **metrics, trends, and insights**.

Built-in Reports include (in-context repoerts):

- The overall pass rate of your pipeline.
- The pass rate of any tests in your pipeline.
- The average duration of your pipeline runs; including the build tasks, which take the most time to complete.

---

# Q018:

You have an Azure DevOps organization that uses self-hosted agents to execute long-running jobs.
You plan to replace self-hosted agents with Microsoft-hosted agents.
You need to identify the maximum duration of a job run on a Microsoft-hosted agent.

What should you identify?

Select only one answer.

1. 2 hours
2. 6 hours
3. 12 hours
4. 24 hours

---

2. 6 hours
The maximum duration of a build running on a Microsoft-hosted agent is six hours.

[Integrate with Azure Pipelines](https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/)  
[Microsoft-hosted agents for Azure Pipelines Capabilities and limitations](https://learn.microsoft.com/en-gb/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#capabilities-and-limitations)

---

# Q017:

You have an Azure subscription that contains 1,000 virtual machines.

You have an on-premises site that contains 100 physical servers.

You need to recommend an Azure service to centralize configuration management
across both environments. The solution must minimize administrative effort 
and support a declarative approach.

Which Azure service should you recommend?

Select only one answer.

1. Azure Automation State Configuration
2. Azure Resource Manager (ARM)
3. Microsoft Purview
4. Microsoft Sentinel

---

1. Azure Automation State Configuration

**Azure Automation State Configuration can work centrally for Azure and on-premises virtual machines**.

Guest configuration in Azure Policy requires Azure Arc-enabled resources to be used outside of Azure. 
ARM requires Azure Stack Hub to be used outside of Azure. 
Microsoft Sentinel is a SIEM and SOAR solution. 

[Microsoft Purview](https://www.microsoft.com/en-us/security/business/microsoft-purview ) 
is a data governance, protection, and management solution.


---

# Q016:

You plan to use Azure DevOps to deploy applications in a multi-cloud environment.

You need to identify which technology allows you to implement infrastructure as code (IaC)
in a consistent manner, regardless of which cloud provider you choose for the deployment.

What should you use?

Select only one answer.

1. Azure Blueprints
2. Azure Resource Manager (ARM) templates
3. Bicep
4. Terraform

--

4. Terraform

Terraform provides IaC functionality in multi-cloud scenarios. 
ARM templates, Azure Blueprints, and Bicep provide IaC functionality in Azure.

---

# Q015:

You plan to implement the automated validation of Azure Resource Manager (ARM) 
templates for your company.

You need to identify two sections that must be present in every ARM template.
Which two sections should you identify? 

Each correct answer presents part of the solution.

Select all answers that apply.

1. apiProfile
2. contentVersion
3. functions
4. parameters
5. schema

---

2. contentVersion
5. schema

The **schema** and **contentVersion** sections are mandatory in ARM templates.

functions, apiProfile, and parameters are optional in ARM templates.

---

[Explore Azure Resource Manager template structure](https://learn.microsoft.com/en-gb/training/modules/create-azure-resource-manager-template-vs-code/2-explore-template-structure?tabs=azure-cli)  

When you're writing an ARM template, you need to understand all the parts that make up
the template and what they do.

- schema:
A **required section** that defines the location of the JSON schema file that describes 
the structure of JSON data. The version number you use depends on the scope of the deployment
and your JSON editor.

- contentVersion:
A **required section** that defines the version of your template (such as 1.0.0.0). 
You can use this value to document significant changes in your template to ensure you're 
deploying the right template.

- apiProfile:
An **optional section** that defines a collection of API versions for resource types. 
You can use this value **to avoid having to specify API versions for each resource in the template**.

- parameters:
An **optional section** where you define values that are provided during deployment. 
These values can be provided by a parameter file, by command-line parameters, or in the Azure portal.

- variables:
An **optional section** where you define values that are used to simplify template language expressions.

- functions:
An **optional section** where you can **define user-defined functions** that are available within 
the template. User-defined functions can simplify your template when complicated expressions are used
repeatedly in your template.

- resources:
A **required section** that defines the actual items you want **to deploy or update in a resource group** 
or a subscription.

-output:
An **optional section** where you specify the values that will be returned at the end of the deployment.

### Deploy an ARM template to Azure

- Deploy a local template
- Deploy a linked template
- Deploy in a continuous deployment pipeline

`az deployment group create`

```
templateFile="{provide-the-path-to-the-template-file}"
az deployment group create \
  --name blanktemplate \
  --resource-group myResourceGroup \
  --template-file $templateFile
```

---

```
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.1",
  "apiProfile": "",
  "parameters": {},
  "variables": {},
  "functions": [],
  "resources": [
    {
      "type": "Microsoft.Storage/storageAccounts",
      "apiVersion": "2019-06-01",
      "name": "learntemplatestorage123",
      "location": "westus",
      "sku": {
        "name": "Standard_LRS"
      },
      "kind": "StorageV2",
      "properties": {
        "supportsHttpsTrafficOnly": true
      }
    }
  ],
  "outputs": {}
}
```

---

# Q014:

You plan to control the configuration of Azure virtual machines by using 
PowerShell Desired State Configuration (DSC).

You need to identify which service provides built-in PowerShell DSC pull 
server functionality.

Which service should you identify?

Select only one answer.

1. Azure App Configuration
2. Azure Application Gateway
3. Azure Automation
4. Azure Policy

---

3. Azure Automation

**Azure Automation** provides a built-in PowerShell DSC pull server. 

**App Configuration** facilitates implementing feature flags, but 
it does not provide a built-in PowerShell DSC pull server. 

**Application Gateway is a load balancer**. 

**Azure Policy provides governance** and **policy enforcement**, 
but it does not function as a PowerShell DSC pull server.

---

[Explore Azure Automation State configuration (DSC)](https://learn.microsoft.com/en-gb/training/modules/implement-desired-state-configuration-dsc/4-explore-azure-automation)  

Azure Automation State configuration DSC is an Azure cloud-based implementation
of PowerShell DSC, **available as part of Azure Automation**. It allows you to 

> write, manage, and compile PowerShell DSC configurations** 
> import DSC Resources 
> assign configurations to target nodes, all in the cloud.

### Why use Azure Automation DSC?

- Built-in pull server. 
Provides a DSC pull server **like the Windows Feature DSC service** so that
target nodes automatically receive configurations, conform to the desired state, 
and report back on their compliance. The built-in pull server in Azure Automation 
**eliminates the need to set up and maintain your pull server**.

- Management of all your DSC artifacts. 
You can manage all your DSC configurations, resources, and target nodes from the 
Azure portal or PowerShell.

- Import reporting data into Log Analytics. 
Nodes managed with Azure Automation state configuration send detailed reporting status
data to the built-in pull server. You can configure Azure Automation state configuration
to send this data to your Log Analytics workspace.

---

# Q013:

You have a web app that runs in Azure App Service and on on-premises web servers
in multiple locations across the world. 
Each web app has a unique DNS name.

You plan to use Azure Traffic Manager to load balance requests across all instances
of the web apps.

You need to identify which Traffic Manager traffic distribution method will direct
incoming requests to the endpoint that is closest to the origin of the request 
according to network latency.

Which Traffic Manager traffic distribution method should you identify?

Select only one answer.

1. Geographic
2. Performance
3. Priority
4. Weighted

---

2. Performance

**The Performance** distribution method directs incoming requests to the endpoint 
that is closest to the origin of the request by **evaluating network latency**. 

**The Geographic** distribution method directs incoming requests to the endpoint 
that is in the same geographic area as the origin of the request using the DNS
of the request.
 
**The Priority** distribution method always distributes all requests to the endpoint
with the highest priority, **if available**. 

**The Weighted** distribution method distributes all requests across endpoints based
on the weight values assigned to them.

---

# Q012:

You have a web app that runs on Azure virtual machines in multiple Azure regions. 
The virtual machines are accessed by using the public IPv6 addresses assigned to 
their network adapters. 
The IPv6 addresses are NOT associated with DNS names.

You plan to use **Azure Traffic Manager** to **load balance requests** across all
instances of the web apps. You need to identify which Traffic Manager traffic 
distribution method supports targeting IPv6 addresses as its endpoints.

Which Traffic Manager traffic distribution method should you identify?

Select only one answer.

1. MultiValue
2. Performance
3. Priority
4. Weighted

---

1. MultiValue

MultiValue is the only Traffic Manager traffic distribution method that provides
the ability to specify the IPv4 and IPv6 addresses of its endpoints. 

All others, including Performance, Priority, and Weighted, require that the 
endpoints be designated as DNS names only.

---

[Examine Traffic Manager](https://learn.microsoft.com/en-gb/training/modules/implement-canary-releases-dark-launching/3-examine-traffic-manager)

To control traffic in Azure, you can use a component called Azure Traffic Manager.
It is a **DNS-based traffic load balancer** that enables you to distribute traffic optimally to 
services **across global Azure regions** while providing high availability and responsiveness.

Traffic Manager **uses DNS to direct client requests to the most appropriate service endpoint** 
**based on a traffic-routing method and the health** of the endpoints.

An **endpoint is an Internet-facing service** hosted inside or outside of Azure.

Traffic Manager is resilient to failure, including the breakdown of an entire Azure region. 

 Traffic Manager currently provides **six options to distribute traffic**:

 1. Priority:
 Select Priority when you want to use a primary service endpoint for all traffic 
 and provide backups if the primary or the backup endpoints are unavailable.

 2. Weighted:
 Select Weighted when you want to distribute traffic across a set of endpoints, 
 either evenly or according to weights, which you define.

 3. Performance:
 Select Performance when you have endpoints in different geographic locations, 
 and you want end users to use the "closest" endpoint for the lowest network latency.

 4. Geographic:
Select Geographic so that users are directed to specific endpoints (Azure, External, or Nested) 
**based on which geographic location their DNS query originates from**. It empowers Traffic Manager
customers to enable scenarios where knowing a user's geographic region and routing them based on 
that is necessary. 
Examples include following: 
 > **data sovereignty mandates**
 > localization of content & user experience 
 > measuring traffic from different regions.

 5. Subnet:
 Select the Subnet traffic-routing method to map sets of end-user IP address ranges to a specific
 endpoint within a Traffic Manager profile. The endpoint returned will be mapped for that request's
 source IP address when a request is received.

 6. Muntivalue
Select MultiValue for Traffic Manager profiles that can only have IPv4/IPv6 addresses as endpoints
**for example VMs that have IPv6 or IPv4 addresses aasigned to their network cards but these IPs 
are NOT associated with DNS names**. 
When a query is received for this profile, all healthy endpoints are returned.


---

# Q011:

You plan to create an Azure Pipelines release pipeline that will be used for 
blue-green deployments of a .NET Core application named App1. 
The code of App1 implements feature flags.

You need to identify which service to use as the Feature Manager for the 
feature flag management of App1. 
The solution must minimize administrative effort.

Which service should you identify?

Select only one answer.

1. Azure Advisor
2. Azure App Configuration
3. Azure Automation
4. Azure Logic Apps

---

2. Azure App Configuration

App Configuration provides built-in Feature Manager functionality. 

**Logic Apps** and **Azure Automation** can potentially be configured for 
feature flag management, but this requires more effort compared to App Configuration. 

**Azure Advisor** does not provide Feature Manager functionality.

---

[Describe feature toggle maintenance](https://learn.microsoft.com/en-gb/training/modules/implement-blue-green-deployment-feature-toggles/6-describe-feature-toggle-maintenance)  

A feature toggle is just code. And to be more specific, conditional code. 
**It adds complexity to the code and increases the technical debt**.
Be aware of that when you write them, and **clean up when you don't need them anymore**.
While feature flags **can be helpful, they can also introduce many issues of their own**.
The idea of a toggle is that **it's short-lived** and only stays in the software when it's
necessary to release it to the customers.

You can classify the different types of toggles based on two dimensions as described by Martin Fowler.
He states that you can look at the dimension of:
 1. how long a toggle should be in your codebase 
 2. how dynamic the toggle needs to be

### Planning feature flag lifecycles

**The most important thing is to remember that you need to remove the toggles from the software**.
If you don't do that, they'll become a form of technical debt if you keep them around for too long.
As soon as you introduce a feature flag, you've added to your overall technical debt.

Like other technical debt, they're easy to add, but the longer they're part of your code,
 the bigger the technical debt becomes because you've added scaffolding logic needed for
 the branching within the code.

**The cyclomatic complexity of your code keeps increasing as you add more feature flags**,
as the number of possible paths through the code increases.

**Using feature flags can make your code less solid and can also add these issues**:

> The code is harder to test effectively as the number of logical combinations increases.
> The code is harder to maintain because it's more complex.
> The code might even be less secure.
> It can be harder to duplicate problems when they're found.

A plan for managing the lifecycle of feature flags is critical. 
As soon as you add a flag, you need to plan for when it will be removed.

**Feature flags shouldn't be repurposed**. There have been high-profile failures because
teams decided to reuse an old flag that they thought was no longer part of the code for a
new purpose.

### Tooling for release flag management

The amount of effort required to manage feature flags shouldn't be underestimated. 
It's essential to consider using tooling that tracks:

> Which flags exist.
> Which flags are enabled in which environments, situations, or target customer categories.
> The plan for when the flags will be used in production.
> The plan for when the flags will be removed.

Azure App Configuration offers a Feature Manager. 
See: 

---

[Tutorial: Manage feature flags in Azure App Configuration](https://learn.microsoft.com/en-us/azure/azure-app-configuration/manage-feature-flags)  

You can store all feature flags in Azure App Configuration and administer them from a single place. App Configuration has a portal UI named Feature Manager that's designed specifically for feature flags. App Configuration also natively supports the .NET Core feature-flag data schema.

---

[Enable staged rollout of features for targeted audiences](https://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-targetingfilter-aspnet-core)  

The `Microsoft.FeatureManagement` library includes `TargetingFilter`, which enables a feature flag 
for **a specified list of users and groups, or for a specified percentage of users**. 

> TargetingFilter is "sticky." 
This means that once an individual user receives a feature, they'll continue to see that feature
on all future requests. You can use `TargetingFilter` to enable a feature for: 
- a specific account during a demo 
- to progressively roll out new features to users in different groups or "rings," 
and much more.

---

[Use feature filters to enable conditional feature flags](https://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-feature-filters-aspnet-core)

---

# Q010:

You have a project in Azure DevOps named Project1 that contains a build pipeline. 
The build pipeline generates an artifact and stores the artifact in Azure Repos.

You create a release pipeline in Project1.
You need to automatically execute the release pipeline whenever the build pipeline 
generates a new artifact.

What should you configure?

Select only one answer.

1. the default approver settings of the first stage of the release pipeline
2. the release gate settings of the first stage of the release pipeline
3. the service connection settings of an artifact in the build pipeline
4. the trigger settings of an artifact in the build pipeline

---

4. the trigger settings of an artifact in the build pipeline

**The trigger settings of an artifact in the build pipeline** allow you to
configure the automatic execution of the release pipeline whenever the build
pipeline generates a new artifact. 

**A service connection** is not required since the artifact is part of Azure Repos
for the same project. 

**The default approver settings** do not require explicit approvals, so they do not
need to be configured to automatically execute the release pipeline if the build 
pipeline artifact trigger is enabled. 

**The default release gate** settings allow for automatic stage execution, so they 
do not need to be configured to automatically execute the release pipeline if the build
pipeline artifact trigger is enabled.

---

[Explore release recommendations](https://learn.microsoft.com/en-gb/training/modules/explore-release-strategy-recommendations/)  
[Explore release approvals](https://learn.microsoft.com/en-gb/training/modules/explore-release-strategy-recommendations/4-explore-release-approvals)  

[Configure build completion triggers (classic) - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/pipelines/process/pipeline-triggers-classic?view=azure-devops)  

- Add a build completion trigger:
In the classic editor, pipeline triggers are called build completion triggers. 
You can select any other build in the same project to be the triggering pipeline.
After you add a build completion trigger, select the triggering build.
example: `features/modules/*`

- Download artifacts from the triggering build
In many cases, you'll want to download artifacts from the triggering build. 
To do this:

> 1 Edit your build pipeline.
> 2 Add the Download Build Artifacts task to one of your jobs under Tasks.
> 3 For Download artifacts produced by, select Specific build.
> 4 Select the team Project that contains the triggering build pipeline.
> 5 Select the triggering Build pipeline.
> 6 Select When appropriate, download artifacts from the triggering build.
> 7:
Even though you specified that you want to download artifacts from the triggering build,
you must still select a value for Build. The option you choose here determines which
build will be the source of the artifacts whenever your triggered build is run because
of any other reason than BuildCompletion (e.g. Manual, IndividualCI, Schedule, and so on).

> 8 Specify the Artifact name and make sure it matches the name of the artifact published by the triggering build.
> 9 Specify the Destination directory to which you want to download the artifacts. 
For example: `$(Build.BinariesDirectory)`

---

# Q009:

You plan a versioning strategy for a NuGet package.
You need to implement a unique prerelease label based on the date and time of the package.
Which semantic versioning should you use?

Select only one answer.

1. a custom scheme
2. a generated script
3. $(Major).$(Minor).$(Patch).$(date:yyyyMMdd)
4. $(Major).$(Minor).$(rev:.r)

---

1. a custom scheme

In a **case where a unique label is required**, a custom scheme must be implemented
by using date and time as unique values. 

`$(Major).$(Minor).$(Patch).$(date:yyyyMMdd)` uses variables for major, minor, patch, and date. 
**It does not generate unique values**. 

A script can be used to generate the version in the build pipeline. 

`$(Major).$(Minor).$(rev:.r)` is a format of semantic versioning that uses variables. 
It does not generate unique values based on date and time.

---

[Publish NuGet packages with Pipeline tasks or the classic editor - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml#package-versioning)

Azure Pipelines and can be configured in your NuGet task as follows:

- Use the date and time (Classic): 
- byPrereleaseNumber (YAML):
Your package version is in the format: `Major.Minor.Patch-ci-datetime` 
where you have the flexibility to choose the values of your Major, Minor, and Patch.

- Use an environment variable (Classic): 
- byEnvVar (YAML). 
Your package version is set to the value of the environment variable you specify.

- Use the build number (Classic): 
- byBuildNumber (YAML). 
Your package version is set to the build number. Make sure you set your build number 
format under your pipeline Options to: 
`$(BuildDefinitionName)_$(Year:yyyy).$(Month).$(DayOfMonth)$(Rev:.r)`. 

To do set the format in YAML, add a property `name`: 
at the root of your pipeline and add your format.

---

# Q008:

You have a project in Azure DevOps that contains build and release pipelines.
You need to change the number of parallel jobs that will be available to the 
agent pool allocated to the project.

At which level should you add the parallel jobs?

Select only one answer.

1. build pipeline
2. organization
3. project
4. release pipeline

---

2. organization

Parallel jobs are added at the organization level, 
not the project, build pipeline, or release pipeline levels.

---

[Configure and pay for parallel jobs](https://learn.microsoft.com/en-gb/azure/devops/pipelines/licensing/concurrent-jobs?view=azure-devops&tabs=ms-hosted)  
[Understand parallel jobs](https://learn.microsoft.com/en-gb/training/modules/describe-pipelines-concurrency/2-understand-parallel-jobs)  
[Estimate parallel jobs](https://learn.microsoft.com/en-gb/training/modules/describe-pipelines-concurrency/3-estimate-parallel-jobs)  

A simple rule of thumb: 
Estimate that you'll need one parallel job for every four to five users in your organization.

---

# Q007:

You plan to create a project in Azure DevOps.
You need to identify which Azure DevOps feature enables the sharing of arbitrary
values across all the definitions in the project.
Which Azure DevOps feature should you identify?

Select only one answer.

1. predefined variables
2. release pipeline variables
3. stage variables
4. variable groups

---

4. variable groups

Variable groups provide the ability to share arbitrary values across
all the definitions in the same project. 

The values of **predefined variables** are assigned automatically. 
**stage and pipeline variables** have a smaller scope than the entire project.

---

[Explore variables in release pipelines](https://learn.microsoft.com/en-gb/training/modules/manage-modularize-tasks-templates/4-explore-variables-release-pipelines)  
[Publish NuGet packages with Azure Pipelines (YAML/Classic)](https://learn.microsoft.com/en-gb/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml)  

---

# Q006:

You have a project in Azure DevOps that uses packages from NuGet and Maven public registries.
You need to verify that project-level package feeds use original packages rather than copies.
Which Azure Artifacts feature should you implement?

Select only one answer.

1. public feeds
2. security policies
3. upstream sources
4. WinDbg

---

3. upstream sources

One of the advantages of **upstream sources is the control over which package is downloaded**, 
allowing you to verify that project-level package feeds use original packages. 

**Public feeds** are used to show and control packages, **but upstream sources are not allowed**. 

**A security policy** is used inside of a project-scoped feed, allowing you to control how you 
can access the feed, but does not provide public registry access or control. 

**To debug Azure Artifacts** by using symbol servers, you can use `WinDbg`. 
This feature does not provide upstream source control.

---

[Upstream sources overview - Azure Artifacts | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-sources?view=azure-devops#advantages)

Enabling upstream sources offers several advantages for managing your product's dependencies within a single feed:

- Simplicity: 
When you publish all your packages to a single feed, it simplifies your configuration files like 
`NuGet.config, npmrc, or settings.xml`. With just one feed in your config file, you reduce the chances of errors 
and bugs, streamlining your setup.

- Determinism: 
your feed resolves package requests in order, resulting in more consistency when rebuilding your code.

- Provenance: 
Your feed retains information about the packages it saved from upstream sources. This allows you to verify 
that you're using the original package and not a copy or a potentially malicious version.

- Peace of mind: 
Every package installed from upstream sources is automatically saved to your feed. This means that even if
the upstream source is disabled, removed, or undergoing maintenance, you can continue developing and building
with confidence because you have a copy of that package in your feed.

---

[Configure upstream behavior](https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-behavior?view=azure-devops&tabs=nuget%2Cget)  
[Training: Explore package dependencies](https://learn.microsoft.com/en-gb/training/modules/explore-package-dependencies/)
---


# Q005:

Your company plans to implement Azure Artifacts.
The company intends to use public and internal npm packages.
You need to recommend a method of creating separate groupings of public 
and private npm packages without the risk of name collisions.

What should you recommend?

1. audit
2. npm publish
3. NPMRC
4. scopes

---

4. scopes

**Scopes allow you to group npm packages**. 

**Audits** scan npm packages, but they do not group them. 

**NPMRC** is a file used to provide npm configuration settings. 

The **npm publish** command is used to upload packages to the feed 
and make the feed available for consumption.

---

[Use Npm scopes in Azure Artifacts](https://learn.microsoft.com/en-gb/azure/devops/artifacts/npm/scopes?view=azure-devops)  
Npm scopes serve as a means to categorize related packages into groups.
hese scopes enable you to create packages with identical names to those created 
by different users without encountering conflicts.
By using scopes, you have the **ability to segregate public and private packages** 
by adding the **scope prefix** `@scopeName` **and configuring the .npmrc file** 
to exclusively use a feed with that particular scope.

**Azure Artifacts** provides the capability to publish and download both scoped 
and nonscoped packages from feeds or public registries. 

> Use Cases
Npm scopes are particularly valuable when working with self-hosted on-premises 
servers lacking internet access, as configuring upstream sources in such scenarios
isn't feasible.  

 > We don't have to worry about name collisions.
 > No need to change the npm registry in order to install or publish our packages.
 > Each npm organization/user has their own scope, and only the owner or the scope members can publish packages to their scope.

---

# Q004:

You are creating an Azure Artifacts artifact.
You need to provide assurances of backward compatibility.
Which element of semantic versioning should you use?

1. label
2. major
3. minor
4. patch

---

4. patch
**A patch element is the only answer that provides assurances** of backward compatibility. 
Other answer choices either do not convey versioning, such as label, or do not provide any
assurances of backward compatibility.

---

# Q003: 

You plan a dependency management solution for a software package for your company.
You need to recommend which element of semantic versioning to use to designate a 
beta prerelease of the software package.
Which element of semantic versioning should you recommend?

Select only one answer.

1. label
2. major
3. minor
4. patch

---

1. label

**A label** element represents prereleases, such as alpha/beta. 

**A major** element represents a version of content that changed significantly, 
which results in some degree of incompatibility with the previous major version. 

**A minor** element represents a version of content that changed but not as 
significantly as the major version, **making it more likely** to be compatible 
with the previous minor version. 

**A patch** element represents a fix that preserves backward compatibility.

---

# Q002: 

You have a project in Azure DevOps named Project1 that contains a continuous integration pipeline named Pipeline1.
You plan to **use Windows-based self-hosted agents for UI tests in Pipeline1**.
You need to identify the option you must configure to apply to the agents.
Which option should you identify?

1. Enable Autologon.
2. Run a screen resolution task.
3. Run a unit test.
4. Run tests in parallel.

---

1. Enable Autologon.

When **self-hosted agents are used, autologon must be enabled** to allow UI tests to run. 

**A screen resolution task** allows additional configurations to be performed**, 
but an autologon configuration is needed first to allow the test to run. 

To **reduce the duration of the test activities**, running tests in parallel can be useful, 
but this strategy does not address this scenario. 

A unit test is the first step to adding testing to the development process.

---

# Q001: 

You plan to design a DevSecOps security validation process for your company.
You need to identify which stage in the process will include an automated Open Source Software (OSS) vulnerability scan.
Which stage should you identify?

1. continuous deployment
2. continuous integration
3. IDE/pull requests
4. nightly test runs

---

2. continuous integration
Continuous integration should include an OSS vulnerability scan. 
The integrated development environment/pull request step should include static code analysis and code reviews. 
Nightly test runs should include an infrastructure scan. 
Continuous deployment should include passive penetration tests, an SSL scan, and an infrastructure scan.

| Stage        | Scans Types     |
| ------------ | --------------- |
|  IDE/PR      | static code analysis and code reviews  |
|  CI          | OSS vulnerability scan  |
|  Nightly     | infrastructure scan  |
|  CD          | passive penetration tests, an SSL scan, and an infrastructure scan  |

---

[VERACODE - What are SSL and TLS Vulnerabilities?](https://www.veracode.com/security/ssl-tls-vulnerabilities)

The SSL Scanner uses testssl.sh, a command-line tool that checks a server’s service 
on any port to support TLS/SSL ciphers, protocols as well as recent cryptographic 
flaws, and more.
All issues found are further deciphered by our SSL Scanner and appropriately designed 
into a comprehensible report.

---

