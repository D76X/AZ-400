
---

# Q020:

You manage a GitHub organization named contoso that contains multiple repositories.
For each repository, you plan to create a multi-job workflow by using GitHub Actions
and share artifacts between jobs.

You need to ensure that the artifacts are retained for up to 10 days for all the 
GitHub Actions. 

The solution must minimize development effort.

At which levels should you configure the retention limit?

Select only one answer.

1. actions
2. organization
3. repositories
4. workflow

---

2. organization

Setting the retention limits at the organization level enforces 
the maximum retention limit for all actions. 

While **you can define the retention limit for each action or repository**, 
this requires more effort to implement and maintain compared to configuring
this at the organization level. 

Retention limits **are not configurable at the workflow level**.

[CI Integration with HitHub Actions](https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/4-share-artifacts-between-jobs)  

### Share artifacts between jobs

When using Actions to create CI or CD workflows, you'll often need to pass artifacts created by one job to another.

> Upload-artifact Action:

This action can upload one or more files from your workflow to be shared between jobs.

```
- uses: actions/upload-artifact
  with:
    name: harness-build-log
    path: bin/output/logs/harness.log
     retention-days: 12 # You can set a custom retention period when uploading, 
                        # but it can't exceed the defaults for the repository, organization, or enterprise.
    #  path: bin/output/logs/harness[ab]?/* # can use wildcards
```

For en entire folder:

```
- uses: actions/upload-artifact
  with:
    name: harness-build-logs
    path: bin/output/logs/
```

You can specify multiple paths:

```
- uses: actions/upload-artifact
  with:
    name: harness-build-logs
    path: |
        bin/output/logs/harness.log
        bin/output/logs/harnessbuild.txt
```

> Download-artifact

```
- uses: actions/download-artifact
  with:
    name: harness-build-log

```

---

# Q019:

You use Azure Pipelines to deploy applications.

You need to publish Microsoft Power BI reports that show Azure Pipelines analytics. 
The solution must minimize development effort.

What should you create?

Select only one answer.

1. a GraphQL feed
2. an Azure Data Factory pipeline
3. an Azure Synapse Analytics pipeline
4. an OData feed

---

4. an OData feed

An **OData feed** provides a straightforward approach to publishing reports 
that show Azure Pipelines analytics. 

**GraphQL, Azure Data Factory, and Azure Synapse Analytics** pipelines can potentially
be used in this case, but they require more significant development effort.

---

[Exercise - Monitor the health of your pipeline](https://learn.microsoft.com/en-gb/training/modules/create-release-pipeline/6-monitor-pipeline-health)  

Examine the analytics features that Azure Pipelines provide.

### How can I track the health of my pipeline?

You could use a dashboard.
let's look at some of the built-in analytics that Azure Pipelines provides.

### What information does pipeline analytics provide?

Every pipeline provides reports that include **metrics, trends, and insights**.

Built-in Reports include (in-context repoerts):

- The overall pass rate of your pipeline.
- The pass rate of any tests in your pipeline.
- The average duration of your pipeline runs; including the build tasks, which take the most time to complete.

---

# Q018:

You have an Azure DevOps organization that uses self-hosted agents to execute long-running jobs.
You plan to replace self-hosted agents with Microsoft-hosted agents.
You need to identify the maximum duration of a job run on a Microsoft-hosted agent.

What should you identify?

Select only one answer.

1. 2 hours
2. 6 hours
3. 12 hours
4. 24 hours

---

2. 6 hours
The maximum duration of a build running on a Microsoft-hosted agent is six hours.

[Integrate with Azure Pipelines](https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/)  
[Microsoft-hosted agents for Azure Pipelines Capabilities and limitations](https://learn.microsoft.com/en-gb/azure/devops/pipelines/agents/hosted?view=azure-devops&tabs=yaml#capabilities-and-limitations)

---

# Q017:

You have an Azure subscription that contains 1,000 virtual machines.

You have an on-premises site that contains 100 physical servers.

You need to recommend an Azure service to centralize configuration management
across both environments. The solution must minimize administrative effort 
and support a declarative approach.

Which Azure service should you recommend?

Select only one answer.

1. Azure Automation State Configuration
2. Azure Resource Manager (ARM)
3. Microsoft Purview
4. Microsoft Sentinel

---

1. Azure Automation State Configuration

**Azure Automation State Configuration can work centrally for Azure and on-premises virtual machines**.

Guest configuration in Azure Policy requires Azure Arc-enabled resources to be used outside of Azure. 
ARM requires Azure Stack Hub to be used outside of Azure. 
Microsoft Sentinel is a SIEM and SOAR solution. 

[Microsoft Purview](https://www.microsoft.com/en-us/security/business/microsoft-purview ) 
is a data governance, protection, and management solution.


---

# Q016:

You plan to use Azure DevOps to deploy applications in a multi-cloud environment.

You need to identify which technology allows you to implement infrastructure as code (IaC)
in a consistent manner, regardless of which cloud provider you choose for the deployment.

What should you use?

Select only one answer.

1. Azure Blueprints
2. Azure Resource Manager (ARM) templates
3. Bicep
4. Terraform

--

4. Terraform

Terraform provides IaC functionality in multi-cloud scenarios. 
ARM templates, Azure Blueprints, and Bicep provide IaC functionality in Azure.

---

# Q015:

You plan to implement the automated validation of Azure Resource Manager (ARM) 
templates for your company.

You need to identify two sections that must be present in every ARM template.
Which two sections should you identify? 

Each correct answer presents part of the solution.

Select all answers that apply.

1. apiProfile
2. contentVersion
3. functions
4. parameters
5. schema

---

2. contentVersion
5. schema

The **schema** and **contentVersion** sections are mandatory in ARM templates.

functions, apiProfile, and parameters are optional in ARM templates.

---

[Explore Azure Resource Manager template structure](https://learn.microsoft.com/en-gb/training/modules/create-azure-resource-manager-template-vs-code/2-explore-template-structure?tabs=azure-cli)  

When you're writing an ARM template, you need to understand all the parts that make up
the template and what they do.

- schema:
A **required section** that defines the location of the JSON schema file that describes 
the structure of JSON data. The version number you use depends on the scope of the deployment
and your JSON editor.

- contentVersion:
A **required section** that defines the version of your template (such as 1.0.0.0). 
You can use this value to document significant changes in your template to ensure you're 
deploying the right template.

- apiProfile:
An **optional section** that defines a collection of API versions for resource types. 
You can use this value **to avoid having to specify API versions for each resource in the template**.

- parameters:
An **optional section** where you define values that are provided during deployment. 
These values can be provided by a parameter file, by command-line parameters, or in the Azure portal.

- variables:
An **optional section** where you define values that are used to simplify template language expressions.

- functions:
An **optional section** where you can **define user-defined functions** that are available within 
the template. User-defined functions can simplify your template when complicated expressions are used
repeatedly in your template.

- resources:
A **required section** that defines the actual items you want **to deploy or update in a resource group** 
or a subscription.

-output:
An **optional section** where you specify the values that will be returned at the end of the deployment.

### Deploy an ARM template to Azure

- Deploy a local template
- Deploy a linked template
- Deploy in a continuous deployment pipeline

`az deployment group create`

```
templateFile="{provide-the-path-to-the-template-file}"
az deployment group create \
  --name blanktemplate \
  --resource-group myResourceGroup \
  --template-file $templateFile
```

---

```
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.1",
  "apiProfile": "",
  "parameters": {},
  "variables": {},
  "functions": [],
  "resources": [
    {
      "type": "Microsoft.Storage/storageAccounts",
      "apiVersion": "2019-06-01",
      "name": "learntemplatestorage123",
      "location": "westus",
      "sku": {
        "name": "Standard_LRS"
      },
      "kind": "StorageV2",
      "properties": {
        "supportsHttpsTrafficOnly": true
      }
    }
  ],
  "outputs": {}
}
```

---

# Q014:

You plan to control the configuration of Azure virtual machines by using 
PowerShell Desired State Configuration (DSC).

You need to identify which service provides built-in PowerShell DSC pull 
server functionality.

Which service should you identify?

Select only one answer.

1. Azure App Configuration
2. Azure Application Gateway
3. Azure Automation
4. Azure Policy

---

3. Azure Automation

**Azure Automation** provides a built-in PowerShell DSC pull server. 

**App Configuration** facilitates implementing feature flags, but 
it does not provide a built-in PowerShell DSC pull server. 

**Application Gateway is a load balancer**. 

**Azure Policy provides governance** and **policy enforcement**, 
but it does not function as a PowerShell DSC pull server.

---

[Explore Azure Automation State configuration (DSC)](https://learn.microsoft.com/en-gb/training/modules/implement-desired-state-configuration-dsc/4-explore-azure-automation)  

Azure Automation State configuration DSC is an Azure cloud-based implementation
of PowerShell DSC, **available as part of Azure Automation**. It allows you to 

> write, manage, and compile PowerShell DSC configurations** 
> import DSC Resources 
> assign configurations to target nodes, all in the cloud.

### Why use Azure Automation DSC?

- Built-in pull server. 
Provides a DSC pull server **like the Windows Feature DSC service** so that
target nodes automatically receive configurations, conform to the desired state, 
and report back on their compliance. The built-in pull server in Azure Automation 
**eliminates the need to set up and maintain your pull server**.

- Management of all your DSC artifacts. 
You can manage all your DSC configurations, resources, and target nodes from the 
Azure portal or PowerShell.

- Import reporting data into Log Analytics. 
Nodes managed with Azure Automation state configuration send detailed reporting status
data to the built-in pull server. You can configure Azure Automation state configuration
to send this data to your Log Analytics workspace.

---

# Q013:

You have a web app that runs in Azure App Service and on on-premises web servers
in multiple locations across the world. 
Each web app has a unique DNS name.

You plan to use Azure Traffic Manager to load balance requests across all instances
of the web apps.

You need to identify which Traffic Manager traffic distribution method will direct
incoming requests to the endpoint that is closest to the origin of the request 
according to network latency.

Which Traffic Manager traffic distribution method should you identify?

Select only one answer.

1. Geographic
2. Performance
3. Priority
4. Weighted

---

2. Performance

**The Performance** distribution method directs incoming requests to the endpoint 
that is closest to the origin of the request by **evaluating network latency**. 

**The Geographic** distribution method directs incoming requests to the endpoint 
that is in the same geographic area as the origin of the request using the DNS
of the request.
 
**The Priority** distribution method always distributes all requests to the endpoint
with the highest priority, **if available**. 

**The Weighted** distribution method distributes all requests across endpoints based
on the weight values assigned to them.

---

# Q012:

You have a web app that runs on Azure virtual machines in multiple Azure regions. 
The virtual machines are accessed by using the public IPv6 addresses assigned to 
their network adapters. 
The IPv6 addresses are NOT associated with DNS names.

You plan to use **Azure Traffic Manager** to **load balance requests** across all
instances of the web apps. You need to identify which Traffic Manager traffic 
distribution method supports targeting IPv6 addresses as its endpoints.

Which Traffic Manager traffic distribution method should you identify?

Select only one answer.

1. MultiValue
2. Performance
3. Priority
4. Weighted

---

1. MultiValue

MultiValue is the only Traffic Manager traffic distribution method that provides
the ability to specify the IPv4 and IPv6 addresses of its endpoints. 

All others, including Performance, Priority, and Weighted, require that the 
endpoints be designated as DNS names only.

---

[Examine Traffic Manager](https://learn.microsoft.com/en-gb/training/modules/implement-canary-releases-dark-launching/3-examine-traffic-manager)

To control traffic in Azure, you can use a component called Azure Traffic Manager.
It is a **DNS-based traffic load balancer** that enables you to distribute traffic optimally to 
services **across global Azure regions** while providing high availability and responsiveness.

Traffic Manager **uses DNS to direct client requests to the most appropriate service endpoint** 
**based on a traffic-routing method and the health** of the endpoints.

An **endpoint is an Internet-facing service** hosted inside or outside of Azure.

Traffic Manager is resilient to failure, including the breakdown of an entire Azure region. 

 Traffic Manager currently provides **six options to distribute traffic**:

 1. Priority:
 Select Priority when you want to use a primary service endpoint for all traffic 
 and provide backups if the primary or the backup endpoints are unavailable.

 2. Weighted:
 Select Weighted when you want to distribute traffic across a set of endpoints, 
 either evenly or according to weights, which you define.

 3. Performance:
 Select Performance when you have endpoints in different geographic locations, 
 and you want end users to use the "closest" endpoint for the lowest network latency.

 4. Geographic:
Select Geographic so that users are directed to specific endpoints (Azure, External, or Nested) 
**based on which geographic location their DNS query originates from**. It empowers Traffic Manager
customers to enable scenarios where knowing a user's geographic region and routing them based on 
that is necessary. 
Examples include following: 
 > **data sovereignty mandates**
 > localization of content & user experience 
 > measuring traffic from different regions.

 5. Subnet:
 Select the Subnet traffic-routing method to map sets of end-user IP address ranges to a specific
 endpoint within a Traffic Manager profile. The endpoint returned will be mapped for that request's
 source IP address when a request is received.

 6. Muntivalue
Select MultiValue for Traffic Manager profiles that can only have IPv4/IPv6 addresses as endpoints
**for example VMs that have IPv6 or IPv4 addresses aasigned to their network cards but these IPs 
are NOT associated with DNS names**. 
When a query is received for this profile, all healthy endpoints are returned.


---

# Q011:

You plan to create an Azure Pipelines release pipeline that will be used for 
blue-green deployments of a .NET Core application named App1. 
The code of App1 implements feature flags.

You need to identify which service to use as the Feature Manager for the 
feature flag management of App1. 
The solution must minimize administrative effort.

Which service should you identify?

Select only one answer.

1. Azure Advisor
2. Azure App Configuration
3. Azure Automation
4. Azure Logic Apps

---

2. Azure App Configuration

App Configuration provides built-in Feature Manager functionality. 

**Logic Apps** and **Azure Automation** can potentially be configured for 
feature flag management, but this requires more effort compared to App Configuration. 

**Azure Advisor** does not provide Feature Manager functionality.

---

[Describe feature toggle maintenance](https://learn.microsoft.com/en-gb/training/modules/implement-blue-green-deployment-feature-toggles/6-describe-feature-toggle-maintenance)  

A feature toggle is just code. And to be more specific, conditional code. 
**It adds complexity to the code and increases the technical debt**.
Be aware of that when you write them, and **clean up when you don't need them anymore**.
While feature flags **can be helpful, they can also introduce many issues of their own**.
The idea of a toggle is that **it's short-lived** and only stays in the software when it's
necessary to release it to the customers.

You can classify the different types of toggles based on two dimensions as described by Martin Fowler.
He states that you can look at the dimension of:
 1. how long a toggle should be in your codebase 
 2. how dynamic the toggle needs to be

### Planning feature flag lifecycles

**The most important thing is to remember that you need to remove the toggles from the software**.
If you don't do that, they'll become a form of technical debt if you keep them around for too long.
As soon as you introduce a feature flag, you've added to your overall technical debt.

Like other technical debt, they're easy to add, but the longer they're part of your code,
 the bigger the technical debt becomes because you've added scaffolding logic needed for
 the branching within the code.

**The cyclomatic complexity of your code keeps increasing as you add more feature flags**,
as the number of possible paths through the code increases.

**Using feature flags can make your code less solid and can also add these issues**:

> The code is harder to test effectively as the number of logical combinations increases.
> The code is harder to maintain because it's more complex.
> The code might even be less secure.
> It can be harder to duplicate problems when they're found.

A plan for managing the lifecycle of feature flags is critical. 
As soon as you add a flag, you need to plan for when it will be removed.

**Feature flags shouldn't be repurposed**. There have been high-profile failures because
teams decided to reuse an old flag that they thought was no longer part of the code for a
new purpose.

### Tooling for release flag management

The amount of effort required to manage feature flags shouldn't be underestimated. 
It's essential to consider using tooling that tracks:

> Which flags exist.
> Which flags are enabled in which environments, situations, or target customer categories.
> The plan for when the flags will be used in production.
> The plan for when the flags will be removed.

Azure App Configuration offers a Feature Manager. 
See: 

---

[Tutorial: Manage feature flags in Azure App Configuration](https://learn.microsoft.com/en-us/azure/azure-app-configuration/manage-feature-flags)  

You can store all feature flags in Azure App Configuration and administer them from a single place. App Configuration has a portal UI named Feature Manager that's designed specifically for feature flags. App Configuration also natively supports the .NET Core feature-flag data schema.

---

[Enable staged rollout of features for targeted audiences](https://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-targetingfilter-aspnet-core)  

The `Microsoft.FeatureManagement` library includes `TargetingFilter`, which enables a feature flag 
for **a specified list of users and groups, or for a specified percentage of users**. 

> TargetingFilter is "sticky." 
This means that once an individual user receives a feature, they'll continue to see that feature
on all future requests. You can use `TargetingFilter` to enable a feature for: 
- a specific account during a demo 
- to progressively roll out new features to users in different groups or "rings," 
and much more.

---

[Use feature filters to enable conditional feature flags](https://learn.microsoft.com/en-us/azure/azure-app-configuration/howto-feature-filters-aspnet-core)

---

# Q010:

You have a project in Azure DevOps named Project1 that contains a build pipeline. 
The build pipeline generates an artifact and stores the artifact in Azure Repos.

You create a release pipeline in Project1.
You need to automatically execute the release pipeline whenever the build pipeline 
generates a new artifact.

What should you configure?

Select only one answer.

1. the default approver settings of the first stage of the release pipeline
2. the release gate settings of the first stage of the release pipeline
3. the service connection settings of an artifact in the build pipeline
4. the trigger settings of an artifact in the build pipeline

---

4. the trigger settings of an artifact in the build pipeline

**The trigger settings of an artifact in the build pipeline** allow you to
configure the automatic execution of the release pipeline whenever the build
pipeline generates a new artifact. 

**A service connection** is not required since the artifact is part of Azure Repos
for the same project. 

**The default approver settings** do not require explicit approvals, so they do not
need to be configured to automatically execute the release pipeline if the build 
pipeline artifact trigger is enabled. 

**The default release gate** settings allow for automatic stage execution, so they 
do not need to be configured to automatically execute the release pipeline if the build
pipeline artifact trigger is enabled.

---

[Explore release recommendations](https://learn.microsoft.com/en-gb/training/modules/explore-release-strategy-recommendations/)  
[Explore release approvals](https://learn.microsoft.com/en-gb/training/modules/explore-release-strategy-recommendations/4-explore-release-approvals)  

[Configure build completion triggers (classic) - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/pipelines/process/pipeline-triggers-classic?view=azure-devops)  

- Add a build completion trigger:
In the classic editor, pipeline triggers are called build completion triggers. 
You can select any other build in the same project to be the triggering pipeline.
After you add a build completion trigger, select the triggering build.
example: `features/modules/*`

- Download artifacts from the triggering build
In many cases, you'll want to download artifacts from the triggering build. 
To do this:

> 1 Edit your build pipeline.
> 2 Add the Download Build Artifacts task to one of your jobs under Tasks.
> 3 For Download artifacts produced by, select Specific build.
> 4 Select the team Project that contains the triggering build pipeline.
> 5 Select the triggering Build pipeline.
> 6 Select When appropriate, download artifacts from the triggering build.
> 7:
Even though you specified that you want to download artifacts from the triggering build,
you must still select a value for Build. The option you choose here determines which
build will be the source of the artifacts whenever your triggered build is run because
of any other reason than BuildCompletion (e.g. Manual, IndividualCI, Schedule, and so on).

> 8 Specify the Artifact name and make sure it matches the name of the artifact published by the triggering build.
> 9 Specify the Destination directory to which you want to download the artifacts. 
For example: `$(Build.BinariesDirectory)`

---

# Q009:

You plan a versioning strategy for a NuGet package.
You need to implement a unique prerelease label based on the date and time of the package.
Which semantic versioning should you use?

Select only one answer.

1. a custom scheme
2. a generated script
3. $(Major).$(Minor).$(Patch).$(date:yyyyMMdd)
4. $(Major).$(Minor).$(rev:.r)

---

1. a custom scheme

In a **case where a unique label is required**, a custom scheme must be implemented
by using date and time as unique values. 

`$(Major).$(Minor).$(Patch).$(date:yyyyMMdd)` uses variables for major, minor, patch, and date. 
**It does not generate unique values**. 

A script can be used to generate the version in the build pipeline. 

`$(Major).$(Minor).$(rev:.r)` is a format of semantic versioning that uses variables. 
It does not generate unique values based on date and time.

---

[Publish NuGet packages with Pipeline tasks or the classic editor - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml#package-versioning)

Azure Pipelines and can be configured in your NuGet task as follows:

- Use the date and time (Classic): 
- byPrereleaseNumber (YAML):
Your package version is in the format: `Major.Minor.Patch-ci-datetime` 
where you have the flexibility to choose the values of your Major, Minor, and Patch.

- Use an environment variable (Classic): 
- byEnvVar (YAML). 
Your package version is set to the value of the environment variable you specify.

- Use the build number (Classic): 
- byBuildNumber (YAML). 
Your package version is set to the build number. Make sure you set your build number 
format under your pipeline Options to: 
`$(BuildDefinitionName)_$(Year:yyyy).$(Month).$(DayOfMonth)$(Rev:.r)`. 

To do set the format in YAML, add a property `name`: 
at the root of your pipeline and add your format.

---

# Q008:

You have a project in Azure DevOps that contains build and release pipelines.
You need to change the number of parallel jobs that will be available to the 
agent pool allocated to the project.

At which level should you add the parallel jobs?

Select only one answer.

1. build pipeline
2. organization
3. project
4. release pipeline

---

2. organization

Parallel jobs are added at the organization level, 
not the project, build pipeline, or release pipeline levels.

---

[Configure and pay for parallel jobs](https://learn.microsoft.com/en-gb/azure/devops/pipelines/licensing/concurrent-jobs?view=azure-devops&tabs=ms-hosted)  
[Understand parallel jobs](https://learn.microsoft.com/en-gb/training/modules/describe-pipelines-concurrency/2-understand-parallel-jobs)  
[Estimate parallel jobs](https://learn.microsoft.com/en-gb/training/modules/describe-pipelines-concurrency/3-estimate-parallel-jobs)  

A simple rule of thumb: 
Estimate that you'll need one parallel job for every four to five users in your organization.

---

# Q007:

You plan to create a project in Azure DevOps.
You need to identify which Azure DevOps feature enables the sharing of arbitrary
values across all the definitions in the project.
Which Azure DevOps feature should you identify?

Select only one answer.

1. predefined variables
2. release pipeline variables
3. stage variables
4. variable groups

---

4. variable groups

Variable groups provide the ability to share arbitrary values across
all the definitions in the same project. 

The values of **predefined variables** are assigned automatically. 
**stage and pipeline variables** have a smaller scope than the entire project.

---

[Explore variables in release pipelines](https://learn.microsoft.com/en-gb/training/modules/manage-modularize-tasks-templates/4-explore-variables-release-pipelines)  
[Publish NuGet packages with Azure Pipelines (YAML/Classic)](https://learn.microsoft.com/en-gb/azure/devops/pipelines/artifacts/nuget?view=azure-devops&tabs=yaml)  

---

# Q006:

You have a project in Azure DevOps that uses packages from NuGet and Maven public registries.
You need to verify that project-level package feeds use original packages rather than copies.
Which Azure Artifacts feature should you implement?

Select only one answer.

1. public feeds
2. security policies
3. upstream sources
4. WinDbg

---

3. upstream sources

One of the advantages of **upstream sources is the control over which package is downloaded**, 
allowing you to verify that project-level package feeds use original packages. 

**Public feeds** are used to show and control packages, **but upstream sources are not allowed**. 

**A security policy** is used inside of a project-scoped feed, allowing you to control how you 
can access the feed, but does not provide public registry access or control. 

**To debug Azure Artifacts** by using symbol servers, you can use `WinDbg`. 
This feature does not provide upstream source control.

---

[Upstream sources overview - Azure Artifacts | Microsoft Learn](https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-sources?view=azure-devops#advantages)

Enabling upstream sources offers several advantages for managing your product's dependencies within a single feed:

- Simplicity: 
When you publish all your packages to a single feed, it simplifies your configuration files like 
`NuGet.config, npmrc, or settings.xml`. With just one feed in your config file, you reduce the chances of errors 
and bugs, streamlining your setup.

- Determinism: 
your feed resolves package requests in order, resulting in more consistency when rebuilding your code.

- Provenance: 
Your feed retains information about the packages it saved from upstream sources. This allows you to verify 
that you're using the original package and not a copy or a potentially malicious version.

- Peace of mind: 
Every package installed from upstream sources is automatically saved to your feed. This means that even if
the upstream source is disabled, removed, or undergoing maintenance, you can continue developing and building
with confidence because you have a copy of that package in your feed.

---

[Configure upstream behavior](https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-behavior?view=azure-devops&tabs=nuget%2Cget)  
[Training: Explore package dependencies](https://learn.microsoft.com/en-gb/training/modules/explore-package-dependencies/)
---


# Q005:

Your company plans to implement Azure Artifacts.
The company intends to use public and internal npm packages.
You need to recommend a method of creating separate groupings of public 
and private npm packages without the risk of name collisions.

What should you recommend?

1. audit
2. npm publish
3. NPMRC
4. scopes

---

4. scopes

**Scopes allow you to group npm packages**. 

**Audits** scan npm packages, but they do not group them. 

**NPMRC** is a file used to provide npm configuration settings. 

The **npm publish** command is used to upload packages to the feed 
and make the feed available for consumption.

---

[Use Npm scopes in Azure Artifacts](https://learn.microsoft.com/en-gb/azure/devops/artifacts/npm/scopes?view=azure-devops)  
Npm scopes serve as a means to categorize related packages into groups.
hese scopes enable you to create packages with identical names to those created 
by different users without encountering conflicts.
By using scopes, you have the **ability to segregate public and private packages** 
by adding the **scope prefix** `@scopeName` **and configuring the .npmrc file** 
to exclusively use a feed with that particular scope.

**Azure Artifacts** provides the capability to publish and download both scoped 
and nonscoped packages from feeds or public registries. 

> Use Cases
Npm scopes are particularly valuable when working with self-hosted on-premises 
servers lacking internet access, as configuring upstream sources in such scenarios
isn't feasible.  

 > We don't have to worry about name collisions.
 > No need to change the npm registry in order to install or publish our packages.
 > Each npm organization/user has their own scope, and only the owner or the scope members can publish packages to their scope.

---

# Q004:

You are creating an Azure Artifacts artifact.
You need to provide assurances of backward compatibility.
Which element of semantic versioning should you use?

1. label
2. major
3. minor
4. patch

---

4. patch
**A patch element is the only answer that provides assurances** of backward compatibility. 
Other answer choices either do not convey versioning, such as label, or do not provide any
assurances of backward compatibility.

---

# Q003: 

You plan a dependency management solution for a software package for your company.
You need to recommend which element of semantic versioning to use to designate a 
beta prerelease of the software package.
Which element of semantic versioning should you recommend?

Select only one answer.

1. label
2. major
3. minor
4. patch

---

1. label

**A label** element represents prereleases, such as alpha/beta. 

**A major** element represents a version of content that changed significantly, 
which results in some degree of incompatibility with the previous major version. 

**A minor** element represents a version of content that changed but not as 
significantly as the major version, **making it more likely** to be compatible 
with the previous minor version. 

**A patch** element represents a fix that preserves backward compatibility.

---

# Q002: 

You have a project in Azure DevOps named Project1 that contains a continuous integration pipeline named Pipeline1.
You plan to **use Windows-based self-hosted agents for UI tests in Pipeline1**.
You need to identify the option you must configure to apply to the agents.
Which option should you identify?

1. Enable Autologon.
2. Run a screen resolution task.
3. Run a unit test.
4. Run tests in parallel.

---

1. Enable Autologon.

When **self-hosted agents are used, autologon must be enabled** to allow UI tests to run. 

**A screen resolution task** allows additional configurations to be performed**, 
but an autologon configuration is needed first to allow the test to run. 

To **reduce the duration of the test activities**, running tests in parallel can be useful, 
but this strategy does not address this scenario. 

A unit test is the first step to adding testing to the development process.

---

# Q001: 

You plan to design a DevSecOps security validation process for your company.
You need to identify which stage in the process will include an automated Open Source Software (OSS) vulnerability scan.
Which stage should you identify?

1. continuous deployment
2. continuous integration
3. IDE/pull requests
4. nightly test runs

---

2. continuous integration
Continuous integration should include an OSS vulnerability scan. 
The integrated development environment/pull request step should include static code analysis and code reviews. 
Nightly test runs should include an infrastructure scan. 
Continuous deployment should include passive penetration tests, an SSL scan, and an infrastructure scan.

| Stage        | Scans Types     |
| ------------ | --------------- |
|  IDE/PR      | static code analysis and code reviews  |
|  CI          | OSS vulnerability scan  |
|  Nightly     | infrastructure scan  |
|  CD          | passive penetration tests, an SSL scan, and an infrastructure scan  |

---

[VERACODE - What are SSL and TLS Vulnerabilities?](https://www.veracode.com/security/ssl-tls-vulnerabilities)

The SSL Scanner uses testssl.sh, a command-line tool that checks a server’s service 
on any port to support TLS/SSL ciphers, protocols as well as recent cryptographic 
flaws, and more.
All issues found are further deciphered by our SSL Scanner and appropriately designed 
into a comprehensible report.

---

