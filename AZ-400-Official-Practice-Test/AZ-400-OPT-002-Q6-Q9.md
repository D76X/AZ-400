# PT00-Q02: 


## Complete Case Study

### Overview:

Company 1 is a retail company specialized in gaming furniture and runs its systems in Azure. 
To keep the sustainable growth of the business, the company wants to improve its 
**logging and telemetry** solutions.

You were hired as a **Site Reliability Engineer (SRE)** to improve the
 **logging and telemetry** solutions. 

---

### Environment:

The company has the following applications running **in production**:

1. Application 1: 
a microservices solution on  Azure Kubernetes Services (AKS).
This is hosted in the **Central US** region.  

2. Application 2: 
an ASP.NET Core 5 application running on Azure App Service to support 
an e-commenrce frontend. 
This is hosted in the **Central US** region.  

3. Application 3: 
a .NET 5 application running in Azure Functions.
The Azure Funtion is hosted in the **East US** region. 

---

### Requirements:

1. App1 uses **Prometheus** and **Log Analytics** to collect metrics and logs for individual services.
2. **Prometheus** collects metrics from the services using **pod annotations**.
3. All services in App1 should expose metrics to **Prometheous** using the 
   **/mymetrics** endpoint and the **StatsD exporter default port**.
4. App2, App3, and compatible microservices in App1 should be integrated with 
   the **Application Insights SDK server-side telemetry**.
5. **App2 requires allerts** bases on page load performance, excluding authentication errors.
6. Enable distributed tracing for a **Python microservice** in App1.
7. Implement 3  Log Analytics Workspaces as shown in the Log Analytics section.
8. Use a cetralized workspace for all applications.
9. Different teams maintain App1 & App2.
   Each team should have access to logs related to their applicayion only.
     

---

### Issues:

After implementing a proof of concept (POC), you identify the following issues:

1. A service in App1 does not have its metrics scraped. 
   **Prometheous** and the **pod annotations** for the service are configured as shown below:

- **Prometheous ConfigMap**:

```
prometheous-data-collection-settings: |-
[prometheous_data_collection_settings.cluster]
interval = "1m"
monitor_kubernetes_pods = true
```

- **Pod Annotations**:

```
- prometheous.io/scrape:"true"
- prometheous.io/scheme:"http"
- prometheous.io/path:"/mymetrics"
- prometheous.io/port:"8080"
```

2. App2 and App3 telemetry volumes are approximately 10 GB per day per application.
   This exceeds the solution budget.

---

### Log Analytics workspaces:

- Workspace1:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Require Workspace Permission |
| tags | none |

- Workspace2:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | East US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |


- Workspace3:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |

---

### Question1:

You need to select which workspaces can be used as centralized workspace.
Which workspace should you use?

 - workspace3 only
 - workspace2 or workspace3 only
 - workspace1, workspace2 or workspace3 
 - workspace1 or workspace3 only

---

### Answer:
workspace2 or workspace3 only

---

### Explanation:

9. Different teams maintain App1 & App2.
   Each team should have access to logs related to their applicayion only.

You **should not** use workspace1 because it is configured to **require workspace permission**.
With the access mode **require workspace permission** you cannot assign granular roled-based 
access control to resources. 
In this mode, you need to give access to **all resources vailable in the workspace** to both
teams, therefore workspace 1 cannot satify the requirement.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Require Workspace Permission |


Both **workspace2 and workspace3** are configured to allow the usage of 
**resource permission or workspace permission**.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Use resource or Workspace Permission |

You should use **resource permission** to implement **granular role-based access control**.
This will allow each team to have access to logs related to their application only, 
which is a requirement as shown above.

Log Analytics Workspaces **do not require** resources to be in the same Azure region 
to be integrated, therefore the fact that wsp2 & wsp3 may be located in a region different
from that of the applications that generate the logs is not a crireria to select only 
one of the two as the apps are also in two regions.

- App1 & App2:
Central US 

- App3: 
East US

- Workspace2: 
| Location          | East US |

- Workspace3:
| Location          | Central US |

In other words of the options:

 - workspace3 only
 - workspace2 or workspace3 only
 - workspace1, workspace2 or workspace3 
 - workspace1 or workspace3 only

Those that use ws1 are to be ruled out because ws1 cannot meet the R9, which leaves:

- workspace3 only
- workspace2 or workspace3 only

The `workspace3 only` is **false** as just stated as wsp2 may also be used.

---

### [Log Analytics Workspace Access modes](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#access-mode):

---

### References:

[Design a Log Analytics workspace architecture](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/workspace-design)   
[Manage access to Log Analytics workspaces](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal)  

---

### Question2:

You need to enable distributed tracing for a Python microservice.

Which three actions should you perform in sequence?
 
 - Configure the application with an Application Insights instrumentation Key. 
 - Define the exceptions and stack traces to the logs records.
 - Configure the application with an Application Insights API Key. 
 - Install the **OpenCensus Python SDK**.
 - Define the **trace ID, span ID and sampling flag** to the log records.

---

### Answer:

1. Install the **OpenCensus Python SDK**.
2. Configure the application with an Application Insights instrumentation Key. 
3. Define the **trace ID, span ID and sampling flag** to the log records.

---

### Explanation:

You should first **install the OpenCensus Python SDK**, which is the recommended library to 
**integrate Python applications with Application Insights** and use **distributed tracing**.

Next, you should **configure the application with an Application Insights instrumentation Key**. 
This allows the Python application to send logs to an Application Insights resorce on Azure.

Finally, you should **define the trace ID, span ID and sampling flag** to the log records.
These properties are necessary to enable distributed tracing accross applications.
After you implement these properties in your log records, the log records can be correlated
to tracing logs from other services, thereby configuring disributed tracing.

You **should not**:

**Configure the application with an Application Insights API Key.**
The API key is used to programmatically read Application Insights telemetry 
and not to integrate with applications. 

**Define the exceptions and stack traces to the logs records.**
This would allow you to see the exceptions and stack traces of the application 
in Application Insights. These properties are not necessary for implementing 
distributed tracing.

---

### References:

[OperationCorrelationTelemetryInitializer Class](https://learn.microsoft.com/en-us/dotnet/api/microsoft.applicationinsights.extensibility.operationcorrelationtelemetryinitializer?view=azure-dotnet)  

[Set up Azure Monitor for your Python application](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/app/opencensus-python)  


---

### Question 3:

You need to create the alerting rule for Application2 based on the following query:

```
requests
| summarize percentiles(duration, 50, 95, 99) by bin(timestamp, 1n)
| extend threshold=1500
render timechart
```

To answer, select the appropriate options from the drop-down menus.

1. To determine the page load time for as many users as possible,
   the alert should be based on:
   
   ```
   percentile_duration_50
   percentile_duration_95
   percentile_duration_99
   threshold
   ```

2. To exclude authentication errors from the alert,
   the query must filter the reuslts based on:

   ```
   operation_Name
   performanceBucket
   resultCode
   ```

---

### Answer:

1. `percentile_duration_99`
2. `resultCode`

---

### Explanation:

To determine the page load for as many users as possible, the alert should be based on the 
`percentile_duration_99` property. Ths represents the load time for the 99th percentile, 
which represents the vast majority of the users of te application.

You **should not** use:

`percentile_duration_50`, `percentile_duration_95` to create the alert.
Based on the query these percentiles do not represent ans many users as `percentile_duration_99`.

You **should not** use:

`threshold` to create the alert. This is a `hard-coded` property defined in the query that 
represent an acceptable threshold.

To **exclude authentication errors from the alert** the query must filter the results based on
the `resultCode` property. You can use this property as a filter to exclude **http unauthorized**
errors **(status code 401)**.

You **should not** use:

the `operation_Name` property as this allows you to filter which page performance you want to monitor.

You **should not** use:

the `performanceBucket` property as this property allows you to filter the page duration performance 
in chunks of intervals. It is useful in order to filter a request that took longer.

---

### References:

[Monitor app performance](https://learn.microsoft.com/en-us/training/modules/monitor-app-performance/)  
[Application Insights standard metrics](https://learn.microsoft.com/en-us/azure/azure-monitor/app/standard-metrics)  
[401 Unauthorized](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401)  

---

### Question 4:

You need to reduce Application2 telemetry volume to approximately 1 GB per day.
Which sampling should you use?

- Adaptive or ingestion only.
- Adaptive only.
- Adaptive, Ingestion or Fixed-rate
- Fixed-rate or Ingestion only

---

### Answer:

- Adaptive, Ingestion or Fixed-rate

---

### Explanation:

You should use **Adaptive, Ingestion or Fixed-rate** sampling for Application2.
All sampling configurations allow you to reduce the metric traffic to 10% and meet the budget.

**Fixed-rate sampling**:
is configured in the server-side SDK and only sends to Application Insights the 
configured sampling of the metrics.

**Ingestion sampling**:
is configured on the Application Insights side and it discards the metrics sent by the
application at a sampling rate set by you, which also reduces costs.

**Adaptive sampling**:
is **automatically configured in ASP.NET Core SDK** & **Azure Functions**.
It automatically discards telemetry data when the volume exceeds the count defined in the
**MaxTelemetryItemsPerSeconds** setting. 
The current configuration exceeds the budget, therefore you should reduce the value of
**MaxTelemetryItemsPerSeconds** accordingly to meet the target of 1 GB traffic per day.

---

### References:

[Sampling in Application Insights](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/app/sampling)  

There are three different types of sampling: 

[Types of sampling](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api?source=docs#types-of-sampling)  

- [adaptive sampling](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api?source=docs#adaptive-sampling)  
This is enabled by default in the App Insights SDK.
It adjusts the volume of telemetry sent from the SDK in your ASP.NET/ASP.NET Core app,
and from Azure Functions. 
Adaptive sampling is currently only available for ASP.NET/ASP.NET Core server-side 
telemetry, and for Azure Functions.
**The volume automatically adjusts to stay within** the `MaxTelemetryItemsPerSecond` rate limit.
As telemetry volume rises, it adjusts the sampling rate to hit the target volume.
This adjustment, recalculated at regular intervals, is based on the moving average 
of the outgoing transmission rate.
**To achieve the target volume, some of the generated telemetry is discarded**.

- [fixed-rate sampling:](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api?source=docs#fixed-rate-sampling)  
It is is available in recent versions of the Application Insights SDKs
for ASP.NET, ASP.NET Core, Java (both the agent and the SDK), JavaScript, 
and Python.
It reduces the volume of telemetry sent from both your ASP.NET or ASP.NET Core
or Java server **and from your users' browsers**. 
**You set the rate then the client and server synchronize their sampling** 
so that in Search, you can navigate between related page views and requests.
Fixed-rate sampling reduces the traffic sent **from your web server and web browsers**.
Unlike adaptive sampling, it reduces telemetry at a fixed rate decided by you. 

- [ingestion sampling:](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api?source=docs#ingestion-sampling)  
This works on the Application Insights service endpoint.
It only applies when no other sampling is in effect. 
If the SDK samples your telemetry, ingestion sampling is disabled.
**It happens at the Application Insights service endpoint**.
**It discards some of the telemetry that arrives from your app**, 
at a sampling rate that you set. 
**It doesn't reduce telemetry traffic sent from your app**, 
^^but helps you keep within your monthly quota**. 
**The main advantage of ingestion sampling is that you can set the sampling**
**rate without redeploying your app**. 
Ingestion sampling works uniformly for all servers and clients, but it doesn't
apply when any other types of sampling are in operation.
Ingestion sampling operates at the point where the telemetry from your web server, 
browsers, and devices reaches the Application Insights service endpoint.
**Use this type of sampling if your app often goes over its monthly quota** 
**and you don't have the option of using either of the SDK-based types of sampling**.

### Important
If adaptive or fixed rate sampling methods are enabled for a telemetry type, ingestion sampling is disabled for that telemetry. However, telemetry types that are excluded from sampling at the SDK level will still be subject to ingestion sampling at the rate set in the portal.

### [Which type of sampling should I use?](https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling-classic-api?source=docs#which-type-of-sampling-should-i-use)  

> Use ingestion sampling if:

You often use your monthly quota of telemetry.
You're getting too much telemetry from your users' web browsers.
You're using a version of the SDK that doesn't support sampling - 
for example ASP.NET versions earlier than 2.0. 

> Use fixed-rate sampling if:

You need synchronized sampling between client and server to navigate 
between related events. For example, page views and HTTP requests in
Search while investigating events.

> Use adaptive sampling:

If the conditions to use the other forms of sampling don't apply, 
we recommend adaptive sampling. This setting is enabled by default 
in the ASP.NET/ASP.NET Core SDK. It doesn't reduce traffic until a 
certain minimum rate is reached, therefore low-use sites are probably
not sampled at all.

---


### Question:
### Answer:
### Explanation:
### References:

---







