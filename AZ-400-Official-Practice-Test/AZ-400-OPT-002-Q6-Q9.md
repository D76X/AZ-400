# PT00-Q02: 


## Complete Case Study

### Overview:

Company 1 is a retail company specialized in gaming furniture and runs its systems in Azure. 
To keep the sustainable growth of the business, the company wants to improve its 
**logging and telemetry** solutions.

You were hired as a **Site Reliability Engineer (SRE)** to improve the
 **logging and telemetry** solutions. 

---

### Environment:

The company has the following applications running **in production**:

1. Application 1: 
a microservices solution on  Azure Kubernetes Services (AKS).
This is hosted in the **Central US** region.  

2. Application 2: 
an ASP.NET Core 5 application running on Azure App Service to support 
an e-commenrce frontend. 
This is hosted in the **Central US** region.  

3. Application 3: 
a .NET 5 application running in Azure Functions.
The Azure Funtion is hosted in the **East US** region. 

---

### Requirements:

1. App1 uses **Prometheus** and **Log Analytics** to collect metrics and logs for individual services.
2. **Prometheus** collects metrics from the services using **pod annotations**.
3. All services in App1 should expose metrics to **Prometheous** using the 
   **/mymetrics** endpoint and the **StatsD exporter default port**.
4. App2, App3, and compatible microservices in App1 should be integrated with 
   the **Application Insights SDK server-side telemetry**.
5. **App2 requires allerts** bases on page load performance, excluding authentication errors.
6. Enable distributed tracing for a **Python microservice** in App1.
7. Implement 3  Log Analytics Workspaces as shown in the Log Analytics section.
8. Use a cetralized workspace for all applications.
9. Different teams maintain App1 & App2.
   Each team should have access to logs related to their applicayion only.
     

---

### Issues:

After implementing a proof of concept (POC), you identify the following issues:

1. A service in App1 does not have its metrics scraped. 
   **Prometheous** and the **pod annotations** for the service are configured as shown below:

- **Prometheous ConfigMap**:

```
prometheous-data-collection-settings: |-
[prometheous_data_collection_settings.cluster]
interval = "1m"
monitor_kubernetes_pods = true
```

- **Pod Annotations**:

```
- prometheous.io/scrape:"true"
- prometheous.io/scheme:"http"
- prometheous.io/path:"/mymetrics"
- prometheous.io/port:"8080"
```

2. App2 and App3 telemetry volumes are approximately 10 GB per day per application.
   This exceeds the solution budget.

---

### Log Analytics workspaces:

- Workspace1:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Require Workspace Permission |
| tags | none |

- Workspace2:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | East US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |


- Workspace3:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |

---

### Question1:

You need to select which workspaces can be used as centralized workspace.
Which workspace should you use?

 - workspace3 only
 - workspace2 or workspace3 only
 - workspace1, workspace2 or workspace3 
 - workspace1 or workspace3 only

---

### Answer:
workspace2 or workspace3 only

---

### Explanation:

9. Different teams maintain App1 & App2.
   Each team should have access to logs related to their applicayion only.

You **should not** use workspace1 because it is configured to **require workspace permission**.
With the access mode **require workspace permission** you cannot assign granular roled-based 
access control to resources. 
In this mode, you need to give access to **all resources vailable in the workspace** to both
teams, therefore workspace 1 cannot satify the requirement.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Require Workspace Permission |


Both **workspace2 and workspace3** are configured to allow the usage of 
**resource permission or workspace permission**.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Use resource or Workspace Permission |

You should use **resource permission** to implement **granular role-based access control**.
This will allow each team to have access to logs related to their application only, 
which is a requirement as shown above.

Log Analytics Workspaces **do not require** resources to be in the same Azure region 
to be integrated, therefore the fact that wsp2 & wsp3 may be located in a region different
from that of the applications that generate the logs is not a crireria to select only 
one of the two as the apps are also in two regions.

- App1 & App2:
Central US 

- App3: 
East US

- Workspace2: 
| Location          | East US |

- Workspace3:
| Location          | Central US |

In other words of the options:

 - workspace3 only
 - workspace2 or workspace3 only
 - workspace1, workspace2 or workspace3 
 - workspace1 or workspace3 only

Those that use ws1 are to be ruled out because ws1 cannot meet the R9, which leaves:

- workspace3 only
- workspace2 or workspace3 only

The `workspace3 only` is **false** as just stated as wsp2 may also be used.

---

### References:

[Design a Log Analytics workspace architecture](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/workspace-design)   
[Manage access to Log Analytics workspaces](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal)  

---

### Question2:

You need to enable distributed tracing for a Python microservice.

Which three actions should you perform in sequence?
 
 - Configure the application with an Application Insights instrumentation Key. 
 - Define the exceptions and stack traces to the logs records.
 - Configure the application with an Application Insights API Key. 
 - Install the **OpenCensus Python SDK**.
 - Define the **trace ID, span ID and sampling flag** to the log records.

---

### Answer:

1. Install the **OpenCensus Python SDK**.
2. Configure the application with an Application Insights instrumentation Key. 
3. Define the **trace ID, span ID and sampling flag** to the log records.

---

### Explanation:

You should first **install the OpenCensus Python SDK**, which is the recommended library to 
integrate Python applications with Application Insights and use distributed tracing.

Next, you should **configure the application with an Application Insights instrumentation Key**. 
This allows the Python application to send logs to Application Insights.

Finally, you should **define the **trace ID, span ID and sampling flag** to the log records**.
These properties are necessary to enable distributed tracing accross applications.
After you implement these properties in your log records, the log records can be correlated to 
tracinglogs from other services, thereby configuring disributed tracing.

You **should not**:

**Configure the application with an Application Insights API Key.**
The API key is used to programmatically read Application Insights telemetry 
and not to integrate with applications. 

**Define the exceptions and stack traces to the logs records.**
This would allow you to see the exceptions and stack traces of the application 
in Application Insights. These properties are not necessary for implementing 
distributed tracing.

---

### References:

[OperationCorrelationTelemetryInitializer Class](https://learn.microsoft.com/en-us/dotnet/api/microsoft.applicationinsights.extensibility.operationcorrelationtelemetryinitializer?view=azure-dotnet)  

[Set up Azure Monitor for your Python application](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/app/opencensus-python)  


---

### Question3:

You need to create the alerting rule for Application2 based on the following query:

```
requests
summarize percentiles(duration, 50, 95, 99) by bin(timestamp, 1n)
extend threshold=1500
render timechart
```

To answer, select the appropriate options from the drop-down menus.

1. To determine the page load time for as many users as possible,
   the alert should be based on:
   
   ```
   percentile_duration_50
   percentile_duration_95
   percentile_duration_99
   threshold
   ```

2. To exclude authentication errors from the alert,
   the query must filter the reuslts based on:

   ```
   operation_Name
   performanceBucket
   resultCode
   ```

---

### Answer:

1. `percentile_duration_99`
2. `resultCode`

---

### Explanation:

To determine the page load for as many users as possible, the alert should be based on the 
`percentile_duration_99` property. Ths represents the load time for the 99th percentile, 
which represents the vast majority of the users of te application.

You **should not** use:

`percentile_duration_50`, `percentile_duration_95` to create the alert.
Based on the query these percentiles do not represent ans many users as `percentile_duration_99`.

You **should not** use:

`threshold` to create the alert. This is a `hard-coded` property defined in the query that 
represent an acceptable threshold.

To **exclude authentication errors from the alert** the query must filter the results based on
the `resultCode` property. You can use this property as a filter to exclude **http unauthorized**
errors **(status code 401)**.

You **should not** use:

the `operation_Name` property as this allows you to filter which page performance you want to monitor.

You **should not** use:

the `performanceBucket` property as this property allows you to filter the page duration performance 
in chunks of intervals. It is useful in order to filter a request that took longer.

---

### References:

[Monitor app performance](https://learn.microsoft.com/en-us/training/modules/monitor-app-performance/)  
[Application Insights standard metrics](https://learn.microsoft.com/en-us/azure/azure-monitor/app/standard-metrics)  
[401 Unauthorized](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401)  

---

### Question4:

You need to reduce Application2 telemetry volume to approximately 1 GB per day.
Which sampling should you use?

- Adaptive or ingestion only.
- Adaptive only.
- Adaptive, Ingestion or Fixed-rate
- Fixed-rate or Ingestion only

---

### Answer:

- Adaptive, Ingestion or Fixed-rate

---

### Explanation:

You should use **Adaptive, Ingestion or Fixed-rate** sampling for Application2.
All sampling configurations allow you to reduce the metric traffic to 10% and meet the budget.

**Fixed-rate sampling**:
is configured in the server-side SDK and only sends to Application Insights the 
configured sampling of the metrics.

**Ingestion sampling**:
is configured on the Application Insights side and it discards the metrics sent by the
application at a sampling rate set by you, which also reduces costs.

**Adaptive sampling**:
is **automatically configured in ASP.NET Core SDK** & **Azure Functions**.
It automatically discards telemetry data when the volume exceeds the count defined in the
**MaxTelemetryItemsPerSeconds** setting. 
The current configuration exceeds the budget, therefore you should reduce the value of
**MaxTelemetryItemsPerSeconds** accordingly to meet the target of 1 GB traffic per day.

---

### References:

[Sampling in Application Insights](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/app/sampling)  

---


### Question:
### Answer:
### Explanation:
### References:

---







