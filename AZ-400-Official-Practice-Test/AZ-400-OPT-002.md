# PT00-Q02: 


## Complete Case Study

### Overview:

Company 1 is a retail company specialized in gaming furniture and runs its systems
in Azure. To keep the sustainable groth of the business, the company wants to improve its **logging and telemetry** solutions.

You were hired as a **Site Reliability Engineer (SRE)** to improve the **logging and telemetry** solutions. 

---

### Environment:

The company has the following applications running **in production**:

1. Application 1: a microservices solution on  Azure Kubernetes Services (AKS).
   This is hosted in the **Central US** region.  

2. Application 2: an ASP.NET Core 5 application running on Azure App Service
   to support an e-commenrce frontend. 
   This is hosted in the **Central US** region.  

3. Application 3: a .NET 5 application running in Azure Functions.
   The Azure Funtion is hosted in the **East US** region. 

---

### Requirements:

1. App1 uses **Prometheus** and  **Log Analytics** to collect metrics and logs for individual services.
2. **Prometheus** collects metrics from the services using **pod annotations**.
3. All services in App1 should expose metrics to **Prometheous** using the 
   **/mymetrics** endpoint and the **StatsD exporter default port**.
4. App2, App3, and compatible microservices in App1 should be integrated with 
   the **Application Insights SDK server-side telemetry**.
5. **App2 requires allerts** bases on page load performance, excluding authentication errors.
6. Enable distributed tracing for a **Python microservice** in App1.
7. Implement 3  Loag Analytics Workspaces as shown in the Log Analytics section.
8. Use a cetralized workspace for all applications.
9. Different teams maintain App1 & App2.
   Each team should have access to logs related to their applicayion only.
     

---

### Issues:

After implementing a proof of concept (POC), you identify the following issues:

1. A service in App1 does not have its metrics scraped. 
   **Prometheous** and the **pod annotations** for the service are configured as shown below:

- **Prometheous ConfigMap**:

```
prometheous-data-collection-settings: |-
[prometheous_data_collection_settings.cluster]
interval = "1m"
monitor_kubernetes_pods = true
```

- **Pod Annotations**:

```
- prometheous.io/scrape:"true"
- prometheous.io/scheme:"http"
- prometheous.io/path:"/mymetrics"
- prometheous.io/port:"8080"
```

2. App2 and App3 telemetry voulumes are approximately 10 GB per day per application.
   This exceeds the soulution budget.

---

### Log Analytics workspaces:

- Workspace1:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Require Workspace Permission |
| tags | none |

- Workspace2:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | East US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |


- Workspace3:

This Log Analytics Workspace is configured as follows:

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Resource group    | az-400 |
| Status            | active |
| Location          | Central US |
| Subscription      | Company1 |
| Pricing Tier      | Pay-as-you-go |
| Access Control Mode | Use resource or Workspace Permission |
| tags | none |

---

### Question1:

You need to select which workspaces can be used as centralized workspace.
Which workspace should you use?

 - workspace3 only
 - workspace2 or workspace3 only
 - workspace1, workspace2 or workspace3 
 - workspace1 or workspace3 only

---

### Answer:
workspace2 or workspace3 only

---

### Explanation:

Both **workspace2 and workspace3** are configured to allow the usage of 
resource permission or workspace permission.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Use resource or Workspace Permission |

You should use **resource permission** to implement **granular role-based access control**.
This will allow each team to have access to logs related to their application only, which is 
a requirement.

Log Analytics Workspaces **do not require** resources to be in the same Azure region to be 
integrated.

You **should not** use workspace1 because it is configured to **require workspace permission**.
With the access mode **require workspace permission** you cannot assign granular roled-based 
access control to resources. In this mode, you need to give access to **all resources vailable**
**in the workspace** to both teams.

| Property          | Value |
| ----------------- | --------------------------------------------------- |
| Access Control Mode | Require Workspace Permission |


---

### References:

[Design a Log Analytics workspace architecture](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/workspace-design)   
[Manage access to Log Analytics workspaces](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal)  

---

### Question2:

You need to enable distributed tracing for a Python microservice.

Which three actions should you perform in sequence?
 
 - Configure the application with an Application Insights instrumentation Key. 
 - Define the exceptions and stack traces to the logs records.
 - Configure the application with an Application Insights API Key. 
 - Install the **OpenCensus Python SDK**.
 - Define the **trace ID, span ID and sampling flag** to the log records.

---

### Answer:

1. Install the **OpenCensus Python SDK**.
2. Configure the application with an Application Insights instrumentation Key. 
3. Define the **trace ID, span ID and sampling flag** to the log records.

---

### Explanation:

You should first **install the OpenCensus Python SDK**, which is the recommended library to 
integrate Python applications with Application Insights and use distributed tracing.

Next, you should **configure the application with an Application Insights instrumentation Key**. 
This allows the Python application to send logs to Application Insights.

Finally, you should **define the **trace ID, span ID and sampling flag** to the log records**.
These properties are necessary to enable distributed tracing accross applications.
After you implement these properties in your log records, the log records can be correlated to 
tracinglogs from other services, thereby configuring disributed tracing.

You **should not**:

**Configure the application with an Application Insights API Key.**
The API key is used to programmatically read Application Insights telemetry 
and not to integrate with applications. 

**Define the exceptions and stack traces to the logs records.**
This would allow you to see the exceptions and stack traces of the application 
in Application Insights. These properties are not necessary for implementing 
distributed tracing.

---

### References:

[OperationCorrelationTelemetryInitializer Class](https://learn.microsoft.com/en-us/dotnet/api/microsoft.applicationinsights.extensibility.operationcorrelationtelemetryinitializer?view=azure-dotnet)  

[Set up Azure Monitor for your Python application](https://learn.microsoft.com/en-us/previous-versions/azure/azure-monitor/app/opencensus-python)  


---


### Question:
### Answer:
### Explanation:
### References:

---







