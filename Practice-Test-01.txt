----------------------------------------------------------------

Microsoft AZ-400 Exam Actual Questions
The questions for AZ-400 were last updated on Oct. 2, 2023.
Viewing page 1 out of 47 pages.
Viewing questions 1-10 out of 472 questions

https://www.examtopics.com/exams/microsoft/az-400/view/

----------------------------------------------------------------

-------------------------------------------------------------------------
PT0X-Q0X: 
question text..
Which of the following is the option you would choose?

A0X-1: 
A0X-2: 
A0X-3: 
A0X-4: 

MY ANSWER TO PT0X-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT0X-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

-------------------------------------------------------------------------
PT01-Q0X: 

?


MY ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

REFS:


-------------------------------------------------------------------------
PT01-Q11: 

You are working as a DevOps engineer to implement policies that control the
number of days that personal access tokens (PAT) are available.

Which policy option should you configure?

Select only one answer.

full-scoped
global
lifespan
revoke leaked PATs

MY ANSWER TO PT01-Q11
--------------------------------------------------------------------------
lifespan
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q11
--------------------------------------------------------------------------
lifespan

Lifespan can be used to define the maximum lifespan of a PAT and control its
lifecycle. 

The full-scoped 
option forces the use of defined sets of scopes. 

Global grants
access to all accessible organizations in Azure DevOps. 

Revoke leaked PATs
automatically revokes any PAT that is checked into a public GitHub repository.

--------------------------------------------------------------------------

REFS:

---------------------------------
Manage personal access tokens using policies - Azure DevOps | Microsoft Learn
Set maximum lifespan for new PATs
https://learn.microsoft.com/en-gb/azure/devops/organizations/accounts/manage-pats-with-policies-for-administrators?view=azure-devops#set-maximum-lifespan-for-new-pats
---------------------------------

---------------------------------
Explore an authorization and access strategy
https://learn.microsoft.com/en-gb/training/modules/migrate-to-devops/4-explore-authorization-access-strategy
---------------------------------

-------------------------------------------------------------------------
PT01-Q10: 

You are working on a project that must raise approvals before an 
Azure Resource Manager (ARM) service connection is used for release 
deployments.

What should you configure?


an Azure AD managed identity
Organization permissions
Pipeline permissions
Project permissions

MY ANSWER TO PT01-Q10
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q10
--------------------------------------------------------------------------
Pipeline permissions

------------------------------
Using Pipeline permissions: 
------------------------------
you can specify which pipeline can consume the service connection. 
If any other YAML pipelines refer to the service connection, an authorization
request is raised, which must be approved by a connection administrator. 

------------------------------
Project permissions:
------------------------------
are used to control service connection sharing between projects but not for pipelines. 

------------------------------
Organization permissions: 
------------------------------
are used to define security groups to control service connections inside of the 
organization. 

------------------------------
Azure AD managed identity:
------------------------------
is used instead of a service principal for a service connection, but not to 
request approval to use the service connection.

--------------------------------------------------------------------------

REFS:

---------------------
Pipeline permissions [of Service Connections]
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#pipeline-permissions
---------------------
Pipeline permissions control which YAML pipelines are authorized to use 
the service connection. 
Pipeline permissions do not restrict access from Classic pipelines.

You can choose from the following options:

------------
-1 Open access
------------
for all pipelines to consume the service connection from the more options
at top-right corner of the Pipeline permissions section in security tab
of a service connection.

------------
-2 Lock down 
------------
the service connection and only allow selected YAML pipelines to consume 
the service connection.
If any other YAML pipeline refers to the service connection, an 
authorization request gets raised, which must be approved by a 
connection Administrator.

----------------------
Project permissions - Cross project sharing of service connections
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#project-permissions---cross-project-sharing-of-service-connections
----------------------
Project permissions control which projects can use the service connection. 
By default, service connections aren't shared with any other projects.
-
Only the organization-level administrators from User permissions can share 
the service connection with other projects.
-
The user who's sharing the service connection with a project should have 
at least Create service connection permission in the target project.
-
The user who shares the service connection with a project becomes 
the project-level Administrator for that service connection. 
-
The service connection name is appended with the project name and it 
can be renamed in the target project scope.
-
Organization-level administrator can unshare a service connection from any shared project
-

-----------------------------
Organization-level permissions
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#organization-level-permissions
-----------------------------
Any permissions set at the organization-level reflect across 
all the projects where the service connection is shared. 
There's no inheritance for organization-level permissions.

The organization-level Administrator can do the following administrative tasks:

- Manage organization-level users
- Edit all the fields of a service connection
- Share and unshare a service connection with other project

-------------------------------------------------------------------------
PT01-Q09: 

You manage Azure Pipelines.

You need to implement a self-hosted agent pool.

Which authentication method should you use to connect the agent to Azure Pipelines?

Select only one answer.

Azure AD managed identity
Azure AD service principal
personal access token (PAT)
shared access signature (SAS)

MY ANSWER TO PT01-Q09
--------------------------------------------------------------------------
personal access token (PAT)
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q09
--------------------------------------------------------------------------
personal access token (PAT)

A PAT is used to connect a self-hosted agent to Azure Pipelines. 

A SAS is used to authorize access to Azure Storage, which is not applicable
in this scenario. 

Azure AD service principals and managed identities do not support connecting
self-hosted agents to Azure Pipelines.

--------------------------------------------------------------------------

REFS:

--------------------------------
Integrate with Azure Pipelines
https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/
--------------------------------

-------------------------------------------------------------------------
PT01-Q08: 

You plan to configure GitHub Actions to access GitHub secrets.
You have the following YAML.

```
01  steps:
02    - shell: pwsh
03        ?????
04        DB_PASSWORD: ${{ secrets.DBPassword }}
```
You need to complete the YAML to reference the secret. 
The solution must minimize the possibility of exposing secrets.
Which element should you use at line 03?

$env:
args:
env:
run:

MY ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
```
01  steps:
02    - shell: pwsh
03        args:
04        DB_PASSWORD: ${{ secrets.DBPassword }}
```

args:
-----------------------------------------------------------------------------
https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions
-----------------------------------------------------------------------------

The most secure way to pass secrets to run commands is to reference them as
environment variables, rather than arguments. 

This requires the use of the env: element. 

The $env: 
notation is used to reference an environment variable, but the 
intention of this question is to define rather than reference. 

The run: 
element defines which command to run, so it follows the env: notation.

--------------------------------------------------------------------------

REFS:

Learn  Training  Browse  Manage GitHub Actions in the enterprise 
Manage encrypted secrets
https://learn.microsoft.com/en-us/training/modules/manage-github-actions-enterprise/manage-encrypted-secrets

Secrets are ** encrypted environment variables ** you can create to store:
- tokens 
- credentials
- any other type of sensitive information your GitHub Actions workflows and actions might rely on. 

Once created, they become available to use in the workflows and actions that have access to the 
organization, repository, or repository environment where they are stored.

You can:

-1 Manage encrypted secrets at organization level
   This will minimize the management overhead in your enterprise.
   This way the credentials can be used in the workflows without being exposed.
   Settings >> Secrets and variables > Actions > New organization secret

-2 Manage encrypted secrets at repository level
   scoped to a specific repository
   GitHub Enterprise Cloud and GitHub Enterprise Server also let you create secrets at repository level.
   Settings >> Secrets and variables > Actions > New repository secret

-------------------------------------------------------
Access encrypted secrets within actions and workflows
-------------------------------------------------------

```
steps:
  - name: Hello world action
    with: # Set the secret as an input
      super_secret: ${{ secrets.SuperSecret }}
    env: # Or as an environment variable
      super_secret: ${{ secrets.SuperSecret }}
```
To access an encrypted secret in an action, you must specify the secret as an 
input parameter in the action.yml metadata file. 
If you need to access the encrypted secret in your action's code, the action 
code could read the value of the input using the $SUPER_SECRET environment variable.

```
inputs:
  super_secret:
    description: 'My secret token'
    required: true
```

----------------------------
Learn  Training  Browse  Learn continuous integration with GitHub Actions 
Use secrets in a workflow
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/9-use-secrets-workflow
----------------------------
Secrets aren't passed automatically to the runners when workflows are executed.
Instead, when you include an action that requires access to a secret, you use 
the secrets context to provide it.

```
steps:
  - name: Test Database Connectivity
    with:
      db_username: ${{ secrets.DBUserName }}
      db_password: ${{ secrets.DBPassword }}
```

--------------------
Command-line secrets
--------------------
Secrets shouldn't be passed directly as command-line arguments as they may be visible to others.
Instead, treat them like environment variables:

```
steps:
  - shell: pwsh
    env:
      DB_PASSWORD: ${{ secrets.DBPassword }}
    run: |
      db_test "$env:DB_PASSWORD"
```

--------------------------------------------------------------------------
PT01-Q07: 

You are working with Azure DevOps on a Scrum project. 
You need to monitor and count work items as they move to a different state.

What should you use to perform the monitoring activity?

cumulative flow diagrams (CFD)
cycle time
sprint burndown
velocity widget

MY ANSWER TO PT01-Q07
--------------------------------------------------------------------------
cumulative flow diagrams (CFD)
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q07
--------------------------------------------------------------------------
cumulative flow diagrams (CFD)
--------------------------------------------------------------------------

REFS:

Two CFD charts can be used to monitor the flow of work:
-1 in-context report:  you can view from a team backlog or Kanban board 
-2 CFD widget: you can add to a dashboard.

The velocity widget:
is used to track the amount of work that a team can achieve in a sprint. 

Sprint burndown:
helps monitor the remaining work in a sprint. 

Cycle time:
shows work items closed in a specified timeframe.

----------------------
View and configure the cumulative flow diagram (CFD) reports - Azure DevOps | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/report/dashboards/cumulative-flow?view=azure-devops
----------------------

You use cumulative flow diagrams (CFD) to monitor the flow of work through a system.
CFDs help teams monitor the count of work items as they progressively move through various 
workflow states. The CFD shows the count of items in each Kanban column for the 
selected time period. From this chart, you can gain an idea of 
------------------------------------ 
- the amount of work in progress =  counts unfinished requirements
- lead time = indicates the amount of time it takes to complete a requirement once work has started
------------------------------------ 

These diagrams can show the flow of ,=any of the following depending on the process \
selected for your project: 

(Agile | Basic | Scrum | CMMI=Capability Maturity Integration)

- epics 
- features  
- user stories
- issues 
- product backlog items  
- requirements

Two CFD charts can be used to monitor the flow of work:
----------------------------------------------------------------------
-1 in-context report:  you can view from a team backlog or Kanban board 
-2 CFD widget: you can add to a dashboard.
----------------------------------------------------------------------

To view the in-context reports for the product backlog select as the backlog level: 
---------------------
Stories for Agile
Issues for Basic
Backlog items for Scrum 
Requirements for CMMI .
---------------------

----------------------
Choose the right project - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/choose-right-project/
----------------------
- Understand different projects and systems to guide the journey
- Select a project to start the DevOps transformation
- Identify groups to minimize initial resistance
- Identify project metrics and Key Performance Indicators (KPI's)

-------------------------------------------------------------------------
PT01-Q06: 

You manage the deployment of an Azure App Service web app named App1 in
multiple Azure regions.

You plan to validate the availability of App1 by using an 
Application Insights availability test.

Which type of test should be implemented by using Microsoft Visual Studio?

- custom TrackAvailability
- Multi-step
- Standard
- URL ping

MY ANSWER TO PT01-Q06
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q06
--------------------------------------------------------------------------
Multi-step
Multi-step tests in Application Insights must be implemented by using Visual Studio.

URL ping and Standard tests: 
can be implemented directly from the Azure portal.

Custom TrackAvailability tests: 
can be implemented by using App Service editor in the Azure portal.

Monitor app performance - Training | Microsoft Learn

Application Insights availability tests - Azure Monitor | Microsoft Learn
--------------------------------------------------------------------------

REFS:
------------------------
Monitor app performance
https://learn.microsoft.com/en-gb/training/modules/monitor-app-performance/
------------------------
Application Insights feature overview
--------------------------------------
-1 Live Metrics:
Observe activity from your deployed application 
**in real time ** 
** with no effect on the host environment **.

-2 Availability:
Also known as “Synthetic Transaction Monitoring”, probe your applications 
external endpoint(s) to test over time:
**the overall availability**  
**the responsiveness **

-3 GitHub or Azure DevOps integration:	
Create GitHub or Azure DevOps work items in context of Application Insights data.

-4 Usage:	
Understand which features are popular with users and how users interact and use your application

-5 Smart Detection:	
**Automatic failure and anomaly detection** 
through 
**proactive telemetry analysis**.

-6 Application Map:	
A high level **top-down view** of the application architecture 
and 
at-a-glance visual references to component health and responsiveness.

-7 Distributed Tracing:
Search and visualize an **end-to-end flow** of a given execution or transaction.

----------------------------------
What Application Insights monitors
----------------------------------

1- Request rates, response times, and failure rates: 

Find out which pages are most popular, at what times of day, and where your users are. 
See which pages perform best. 
If your response times and failure rates go high when there are more requests, then 
perhaps you have a resourcing problem.

-2 Dependency rates, response times, and failure rates: 
 Find out whether external services are slowing you down.

-2 Exceptions:
Analyze the aggregated statistics, or pick specific instances and 
drill into the stack trace and related requests. 
Both server and browser exceptions are reported.

- Page views and load performance: reported by your users' browsers.
- AJAX calls from web pages: rates, response times, and failure rates.
- User and session counts.
- Performance counters:
  from your Windows or Linux server machines, such as CPU, memory, and network usage.
- Host diagnostics: from Docker or Azure.
- Diagnostic trace logs: 
  from your app - so that you can correlate trace events with requests.
-Custom events and metrics: 
 that you write yourself in the client or server code, to track business events
 such as items sold or games won.

---------------------------------------------
Getting started with Application Insights
---------------------------------------------
-At run time: 
instrument your web app on the server. Ideal for applications already deployed. 
Avoids any update to the code.

- At development time: 
add Application Insights to your code. 
Allows you to customize telemetry collection and send more telemetry.

- Instrument your web pages: 
for page view, AJAX, and other client-side telemetry.

-Analyze mobile app usage: by integrating with Visual Studio App Center.
*********************
-Availability tests - ping your website regularly from our servers.
*********************

----------------------------------------
Application Insights log-based metric
----------------------------------------
let you 
- analyze the health of your monitored apps
- create powerful dashboards
- configure alerts. 

There are two kinds of metrics:

---------------------
1- Standard metrics:
---------------------
   are stored as pre-aggregated time series.
   Since standard metrics are pre-aggregated during collection, they have 
   better performance at query time. 
   Standard metrics are a better choice for dashboarding and in real-time alerting.

   The pre-aggregated metrics aren't stored as individual events with lots of properties.
   Instead, they're stored as pre-aggregated time series, and only with key dimensions.
   This makes the new metrics superior at query time.
   This enables new scenarios such as near real-time alerting on dimensions of metrics, 
   more responsive dashboards, and more.

   The newer SDKs (Application Insights 2.7 SDK or later for .NET) pre-aggregate metrics during collection. 
   This applies to standard metrics sent by default so the accuracy isn't affected by sampling or filtering. 
   
   ------------------------------------
   Pre-Aggregration at the AI backend
   ------------------------------------
   For the SDKs that don't implement pre-aggregation the Application Insights backend 
   still populates the new metrics by aggregating the events received by the Application Insights event collection endpoint. 

---------------------
2- Log-based metrics:
---------------------
   behind the scene are translated into **Kusto queries** from stored events.
   The log-based metrics have more dimensions, which makes them the superior 
   option for data analysis and ad-hoc diagnostics.
   
   Developers can use the SDK to send events manually by writing code that explicitly
   invokes the SDK or they can rely on the automatic collection of events from 
   auto-instrumentation. 
 
   The Application Insights backend stores all collected events as logs.

    For situations when the volume of events is too high, Application Insights 
    implements several ** telemetry volume reduction techniques, such as:
    sampling and filtering **.
    Behind the scenes, must perform query-time aggregations of the events stored in logs.

------------------------
Application Insights availability tests
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-overview
------------------------


-------------------------------------------------------------------------
PT01-Q05: 

You plan to implement GitHub secrets.

At which two levels can secrets be created? 
Each correct answer presents a complete solution.


action
organization
repository
step
workflow

MY ANSWER TO PT01-Q05
--------------------------------------------------------------------------
(By ChatGPT)
repository
organization
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q05
--------------------------------------------------------------------------
organization
repository
--------------------------------------------------------------------------

REFS:

GitHub secrets can be created at the organization and repository levels.
You can use secrets in a step of an action within a workflow, but you cannot
create them at any of the other three levels.

-----------
Create encrypted secrets - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/8-create-encrypted-secrets
-----------
Learn continuous integration with GitHub Actions
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/
-----------


----------
ChatGPT
----------

GitHub secrets can be created at two levels:

-1 Repository Level: 

You can create secrets specific to a repository. 
These secrets are accessible to the workflows and actions within that particular 
repository.

-1 Organization Level: 

You can also create secrets at the organization level. 
These secrets are accessible to multiple repositories within the same organization, 
making it convenient to share common secrets among related projects.

So, the correct answers are:
repository
organization

-------------------------------------------------------------------------
PT00-Q04: 

You are developing the security validation plan for an application’s lifecycle.
For which activity should you include a ** passive penetration ** test?

 - continuous deployment
 - continuous integration
 - IDE/pull requests
 - nightly test runs

MY ANSWER TO PT00-Q04
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

CORRECT ANSWER TO PT00-Q04: continuous deployment
--------------------------------------------------------------------------


--------------------------------------------------------------------------

REFS:

---------------------------------------
 The IDE environment/pull request step should include:
 ---------------------------------------

- static code analysis 
- code reviews (manual check).

---------------------------------------
 Continuous integration should include:
 ---------------------------------------

 - Open-Source Software (OSS) vulnerability scan. 

---------------------------------------
Nightly test runs should include:
---------------------------------------

 - infrastructure scans 
 - **active** penetration tests 

---------------------------------------
Continuous deployment should include: 
---------------------------------------

- SSL scan 
- infrastructure scans. 
- **passive** penetration tests 

-----------------------------
Microsoft Security Development Lifecycle (SDL)
Threat modeling is a core element of the Microsoft Security Development Lifecycle (SDL).
https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle
-----------------------------

Explore key validation points - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/5-explore-key-validation-points?pivots=bash


Control releases with deployment gates - Azure Pipelines | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/pipelines/release/approvals/gates?view=azure-devops

-------
ChatGPT
-------

EXAM Q/A:

A **passive penetration test** is typically included in the **continuous integration (CI)** 
phase of an application's lifecycle. In CI, developers frequently integrate code changes into a 
shared repository, and automated security checks, including **passive penetration** tests, can
be performed to detect vulnerabilities as early as possible in the development process. 

**Passive penetration tests are non-intrusive** and focus on identifying vulnerabilities and 
security issues without actively exploiting them.

So, the correct answer is: continuous integration

-----------
QUESTION: 

What is the difference between a passive penetration test and active penetration test and in 
which steps of the development lifecycle each of these should be used according to the 
Microsoft Security Development Lifecycle (SDL)?
-----------

Passive and active penetration tests are two different approaches to testing the security of an
application or system, and they serve distinct purposes within the development lifecycle, 
as outlined in the Microsoft Security Development Lifecycle (SDL).

---------------------------
Passive Penetration Test:
---------------------------

Purpose: Passive penetration tests are non-intrusive tests designed to identify vulnerabilities
and security weaknesses without actively exploiting them. 
These tests involve the analysis of code, architecture, configurations, and network traffic to
find potential issues.

---------------
** When to Use: 
---------------
Passive penetration tests are typically used in the early stages of the development lifecycle, 
such as during the requirements and design phases of the SDL. They can also be used during the
continuous integration (CI) phase to identify security issues before they become more critical.

-----------------------
Active Penetration Test:
-----------------------

Purpose: Active penetration tests are more aggressive and involve simulating real-world attacks
to actively exploit vulnerabilities and assess the effectiveness of security controls. They aim
to provide a realistic view of how an application or system would withstand attacks.

---------------
** When to Use:
---------------

Active penetration tests are typically conducted in later phases of the development lifecycle 
when the application or system is closer to completion. This includes the testing and verification
phases of the SDL. They can also be performed during the operational phase to assess ongoing security.

-------------------------------------------------------------------------
PT00-Q03: 

https://learn.microsoft.com/en-gb/credentials/certifications/exams/az-400/practice/assessment?assessment-type=practice&assessmentId=56

You are developing a security validation plan for an application’s lifecycle.
For which activity should you include an 
automated Open-Source Software (OSS) vulnerability scan?

A03-1: continuous deployment
A03-2: continuous integration
A03-3: IDE/pull requests
A03-4: nightly test runs

MY ANSWER TO PT00-Q03
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

CORRECT ANSWER TO PT00-Q03
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

REFS:

--------------------------------------------------------------------
Continuous integration 
should include an OSS vulnerability scan. 
--------------------------------------------------------------------
The integrated development environment / pull request step 
should include 
static code analysis and code reviews. 
--------------------------------------------------------------------
Nightly test runs 
should include 
an infrastructure scan. 
--------------------------------------------------------------------
Continuous deployment 
should include 
passive penetration tests, an SSL scan, and an infrastructure scan.
--------------------------------------------------------------------

AZ-400: Implement security and validate code bases for compliance  Introduction to Secure DevOps 

Explore key validation points - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/5-explore-key-validation-points?pivots=bash


Continuous security validation should be added at each step from development 
through production to help ensure the application is always secure.
This approach aims to switch the conversation with the security team
from approving each release to consenting to the CI/CD process and monitor 
and audit the process at any time.

-------------------
IDE / pull request
-------------------
Validation in the CI/CD begins before the developer commits their code.
** Static code analysis tools in the IDE ** provide the first line of 
defense to help ensure that security vulnerabilities aren't introduced 
into the CI/CD process.

In ** Azure DevOps Enabling branch policies ** on the shared branch requires 
a ** pull request ** to start the merge process and ensure the execution of 
all defined controls.

The pull request should require a ** code review **, the one manual but 
important check for identifying new issues introduced into your code.

Along with **this manual check **, commits should be linked to **work items** 
for auditing why the code change was made and require a continuous integration (CI) 
build process to succeed before the push can be completed.

---------------------------------------
Explore continuous security validation
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/6-explore-continuous-security-validation?pivots=bash
---------------------------------------
developers don't hesitate to use components available in public package sources 
(such as npm or NuGet).
as the dependency on these third-party OSS components increases, the risk of security
vulnerabilities or hidden license requirements also increases compliance issues.
For a business, it's critical, as issues related to compliance, liabilities, and 
customer personal data can cause many privacy and security concerns.

Many tools can scan for these vulnerabilities within 
the build and release pipelines.
**
Once the merge is complete, the CI build should execute as part of the pull request (PR-CI) process.
These CI builds should run static code analysis tests to ensure that the code 
follows all rules for both maintenance and security.
**

----------------------
STATIC CODE ANALYSIS
----------------------
Many of the tools seamlessly integrate into the Azure Pipelines build process. 

-1 SonarQube.
-2 Visual Studio Code Analysis and the Roslyn Security Analyzers.
-3 Checkmarx - A Static Application Security Testing (SAST) tool.
-4 BinSkim - A binary static analysis tool that provides security and correctness results for Windows portable executables and many more.

----------------------
3rd Party Packages Vulnerability scanner
&
OSS licenses
----------------------

Organizations try to manage third-party packages vulnerabilities or OSS licenses. 
** Mend Software's tools** can make this identification process almost instantaneous.

-----------------------------
Understand threat modeling
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/7-understand-threat-modeling?pivots=bash
-----------------------------
Microsoft Security Development Lifecycle (SDL)
Threat modeling is a core element of the Microsoft Security Development Lifecycle (SDL).
https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle
-----------------------------

It's an engineering technique you can use to help you identify threats, attacks, vulnerabilities, 
and countermeasures that could affect your application.
It is used to REDUCE RISK With non-security experts in mind.
The tool makes threat modeling easier for all developers by providing clear guidance on creating
and analyzing threat models. 
iT MAKES IT easier for all developers through a **standard notation** 
for ** visualizing **:
-------------------------------------------------------
- system components
- data flows
- security boundaries.
-------------------------------------------------------

Helps threat modelers identify classes of threats they should consider based 
on the structure of their software design.

-----------------------------------------------------------------------
The Threat Modeling Tool enables any developer or software architect to:
-----------------------------------------------------------------------

-1 Communicate about the security design of their systems.
-2 Analyze those designs for potential security issues using a proven methodology.
-3 Suggest and manage mitigation for security issues.

-------------------------------------------
There are five major threat modeling steps:
https://learn.microsoft.com/en-us/azure/security/develop/threat-modeling-tool
Exercise threat modeling
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/8-exercise-threat-modeling?pivots=bash
-------------------------------------------

-1 Defining security requirements.
-2 Creating an application diagram.
-3 Identifying threats.
-4 Mitigating threats.
-5 Validating that threats have been mitigated.

---------------------------------------------------------
Review code coverage - Azure Pipelines | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/review-code-coverage-results?view=azure-devops#artifacts
---------------------------------------------------------

-------------------------------------------------------------------------
PT00-Q02: 


You need to implement a Git hook that will be triggered automatically in 
response to a Git commit being run on a Windows server.

To what should you set the first line of the predefined Git hook script 
so that you can execute the script?

A. #!/bin/sh
B. #!\bin\sh
C. #!C:/Program\ Files/Git/usr/bin/sh.exe
D. #!C:\Program Files\Git\usr\bin\sh.exe

MY ANSWER:
--------------------------------------------------------------------------
 
--------------------------------------------------------------------------

CORRECT ANSWER: C: #!C:/Program\ Files/Git/usr/bin/sh.exe

--------------------------------------------------------------------------

To execute any of the predefined Git hook scripts, the existing line of 
#!/bin/sh 

needs to be replaced with the one pointing to the location of the Bash 
script interpreter. The notation must use the forward slashes and an escape
 character to account for the space character in the path.

Implement Git hooks - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/3-implement

Service hooks events - Azure DevOps | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/service-hooks/events?view=azure-devops

--------------------------------------------------------------------------

REFS:

https://learn.microsoft.com/en-us/training/modules/explore-git-hooks/

Git hooks are a mechanism that allows code to be run before or after certain
Git lifecycle events. For example, one could hook into the commit-msg event 
to validate that the commit message structure follows the recommended format.

The hooks can be any executable code, including shell, PowerShell, Python, 
or other scripts. 
Or they may be a binary executable. 
Anything goes!

The only criteria are that hooks must be stored in the .git/hooks folder in 
the repo root. 
Also, they must be named to match the related events (Git 2.x):

applypatch-msg
pre-applypatch
post-applypatch
pre-commit
prepare-commit-msg
commit-msg
post-commit
pre-rebase
post-checkout
post-merge
pre-receive
update
post-receive
post-update
pre-auto-gc
post-rewrite
pre-push

you can use hooks to:

-1 enforce policies
-2 ensure consistency 
-3 control your environment:

Examples:

-1 Enforcing preconditions for merging
-2 Verifying work Item ID association in your commit message
-3 Preventing you & your team from committing faulty code
-4 Sending notifications to your team's chat room (Teams, Slack, HipChat, etc.)

----------------------
Implement Git hooks
https://learn.microsoft.com/en-us/training/modules/explore-git-hooks/3-implement
----------------------

------------------------------------
Client-Side Git Hooks on Windows.
------------------------------------

Git ships with several sample hook scripts in the repo .git\hooks directory. 

If you open that folder, you'll find a file called 
 ** pre-commit.sample ** 
To enable it, rename it to 
** pre-commit ** 
by removing the .sample extension 
and making the script executable.

The script is found and executed when you attempt to commit using git commit.
You commit successfully if your pre-commit script exits with a 0 (zero). 

**************************************************************
If you're using Windows, simply renaming the file won't work.
**************************************************************
Git will fail to find the shell in the chosen path specified in the script.
The problem is lurking in the first line of the script, 
the shebang declaration:
-----------
#!/bin/sh
-----------

On Unix-like OSs, the SheBang: #! 

Tells the program loader that it's a script to be interpreted, and /bin/sh 
is the path to the interpreter you want to use, sh in this case.

---------------------------------
Windows isn't a Unix-like OS. 
---------------------------------

Git for Windows supports Bash commands and shell scripts via ** Cygwin **.

Fix it by providing the path to the sh executable on your system. 
It's using the 64-bit version of Git for Windows, so the baseline looks like this:

------------------------------------------
#!C:/Program\ Files/Git/usr/bin/sh.exe
------------------------------------------

```
#!C:/Program\ Files/Git/usr/bin/sh.exe
matches=$(git diff-index --patch HEAD | grep '^+' | grep -Pi 'password|keyword2|keyword3')
if [ ! -z "$matches" ]
then
    cat <<\EOT
Error: Words from the blocked list were present in the diff:
EOT
    echo $matches
    exit 1
fi
```

The repo ** .git\hooks ** folder isn't committed into source control. 
You may wonder how you share the goodness of the automated scripts you create with the team.
The good news is that, from Git version 2.9, you can now map Git hooks to a folder that 
can be committed into source control.

You could do that by updating the global settings configuration for your Git repository:
-------------------------------------------------------
Git config --global core.hooksPath '~/.githooks'
-------------------------------------------------------

----------------------------------------------
Server-side service hooks with Azure Repos
https://learn.microsoft.com/en-us/azure/devops/service-hooks/events?view=azure-devops
----------------------------------------------

So far, we've looked at the client-side Git Hooks on Windows. 
Azure Repos also exposes server-side hooks. 
Azure DevOps uses the exact mechanism itself to create Pull requests. 
You can read more about it at the Server hooks event reference.


-------------------------------------------------------------------------
PT00-Q01: 

You need to minimize the response time when using Azure DevOps Git-based 
repositories that contain large files.

Which Git extension should you use?

A. Git LFS
B. Git Machete
C. GitFlow
D. GitX

MY ANSWER:
--------------------------------------------------------------------------
A00-1: Git LFS
--------------------------------------------------------------------------

CORRECT ANSWER:
--------------------------------------------------------------------------
A00-1: Git LFS

-1 Git LFS 
is a Git extension that provides the fastest response time when using 
Azure DevOps Git-based repositories that contain large files. 

-2 GitFlow 
is a Git extension that implements the GitFlow branching model. 

-3 Git Machete 
is a Git extension that simplifies and automates repository organization.

-4 GitX 
is a Git extension that provides an improved development workflow.
--------------------------------------------------------------------------

REFS:
Learn  Training  Browse  AZ-400: Development for enterprise DevOps  Manage Git repositories 
Work with large repositories
https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/2-work-large-repositories

Why repositories become large?
There are two primary causes for large repositories:

-1 Long history
-2 Large binary files
-3 You can also reduce clones by filtering branches or cloning only a single branch.

--------------
Shallow clone
--------------

If developers don't need all the available history in their local repositories, a good option is 
to implement a shallow clone. It saves both space on local development systems and the time it 
takes to sync.

-------------------------------------
git clone --depth [depth] [clone-url]
-------------------------------------



------------
VFS for Git
https://github.com/microsoft/VFSForGit
VFS stands for Virtual File System. 
------------

VFS for Git helps with large repositories. It requires a Git LFS client.
VFS for Git virtualizes the file system beneath your Git repository so that Git and all tools
see what appears to be a regular working directory, but VFS for Git only downloads objects as 
they are needed.

Typical Git commands are unaffected, but the Git LFS works with the standard filesystem to 
download necessary files in the background when you need files from the server.

---------
Scalar
Introducing Scalar: Git at scale for everyone
https://devblogs.microsoft.com/devops/introducing-scalar/
---------

Scalar is a .NET Core application available for Windows and macOS. 
With tools and extensions for Git to allow very large repositories to maximize 
your Git command performance.

By default each Git repository has a copy of all files in the entire history.

It achieves by enabling some advanced Git features, such as:

- Partial clone: 
reduces time to get a working repository by not downloading all Git objects right away.

-Background prefetch: 
downloads Git object data from all remotes every hour, reducing the time for foreground git fetch calls.

-Sparse-checkout: limits the size of your working directory.

-File system monitor: 
tracks the recently modified files and eliminates the need for Git to scan the entire work tree.

-Commit-graph: 
accelerates commit walks and reachability calculations, speeding up commands like git log.

-Multi-pack-index: enables fast object lookups across many pack files.

-Incremental repack: 
Repacks the packed Git data into fewer pack files without disrupting concurrent commands using the multi-pack-index.

--------------------------------------------------------------------------