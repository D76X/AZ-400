----------------------------------------------------------------

https://www.examtopics.com/exams/microsoft/az-400/view/
Practice Assessment for Exam AZ-400: Designing and Implementing Microsoft DevOps Solutions
https://learn.microsoft.com/en-us/credentials/certifications/exams/az-400/
https://learn.microsoft.com/en-gb/credentials/certifications/exams/az-400/practice/assessment?assessment-type=practice&assessmentId=56

----------------------------------------------------------------

-------------------------------------------------------------------------
PT0X-Q0X: 
question text..
Which of the following is the option you would choose?

A0X-1: 
A0X-2: 
A0X-3: 
A0X-4: 

MY ANSWER TO PT0X-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT0X-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

-------------------------------------------------------------------------
PT01-Q0X: 

?


MY ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

REFS:


-------------------------------------------------------------------------
PT01-Q33: 

?

MY ANSWER TO PT01-Q33
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q33
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q32: 

?

MY ANSWER TO PT01-Q32
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q32
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q31: 

?

MY ANSWER TO PT01-Q31
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q31
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q30: 

You have an Azure DevOps project that uses packages from 
NuGet and Maven public registries.

You need to verify that project-level package feeds use original packages 
rather than copies.

Which Azure Artifacts feature should you implement?

Select only one answer.

public feeds
security policies
upstream sources
WinDbg

MY ANSWER TO PT01-Q30
--------------------------------------------------------------------------
upstream sources
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q30
--------------------------------------------------------------------------
upstream sources
One of the advantages of upstream sources is the control over which a package
is downloaded, allowing you to verify that project-level package feeds use 
original packages. Public feeds are used to show and control packages, but upstream sources are not allowed. A security policy is used inside of a project-scoped feed, allowing you to control how you can get access to the feed, but it does not provide public registry access or control. To debug Azure artifacts by using symbol servers, you can use WinDbg. This feature does not provide upstream source control.

--------------------------------------------------------------------------

REFS:

-----------------------
Upstream sources
https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-sources?view=azure-devops#advantages
-----------------------

-----------------------
Configure upstream behavior
https://learn.microsoft.com/en-gb/azure/devops/artifacts/concepts/upstream-behavior?view=azure-devops&tabs=nuget%2Cget
-----------------------

-----------------------
Explore package dependencies
https://learn.microsoft.com/en-gb/training/modules/explore-package-dependencies/
-----------------------


-------------------------------------------------------------------------
PT01-Q29: 

You are creating an Azure Pipelines artifact.
You need to provide assurances of backward compatibility.

Which element of semantic versioning should you use?


label
major
minor
patch

MY ANSWER TO PT01-Q29
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q29
--------------------------------------------------------------------------
patch

The patch element is the only one that provides assurances of backward compatibility.
Other answer choices either do not convey versioning, such as label, 
or do not provide any assurances regarding backward compatibility.

--------------------------------------------------------------------------

REFS:

------------------------------------
Publish and download Universal Packages with Azure Pipelines
https://learn.microsoft.com/en-gb/azure/devops/pipelines/artifacts/universal-packages?view=azure-devops&tabs=yaml
------------------------------------

------------------------------------
Understand versioning of artifacts
https://learn.microsoft.com/en-gb/training/modules/implement-versioning-strategy/2-understand-versioning-of-artifacts
------------------------------------
A component and its package can have independent versions and versioning schemes.
The versioning scheme can differ per package type. 
Typically, it uses a scheme that can indicate the kind of change that is made.

------------------------------------------------
Most commonly, it involves three types of changes:
------------------------------------------------

---------------------
-1 Major change:
---------------------
Major indicates that the package and its contents have changed significantly. 
It often occurs at the introduction of a new version of the package. 
It can be a redesign of the component. 
Major changes aren't guaranteed to be compatible and usually have breaking changes
from older versions. 
Major changes might require a large amount of work to adopt the 
consuming codebase to the new version.

---------------------
-2 Minor change:
---------------------
Minor indicates that the package and its contents have extensive modifications but
are smaller than a major change. These changes **can be backward compatible** with
the previous version, although **they aren't guaranteed to be**.

---------------------
-3 Patch:
---------------------
A patch or revision is used to indicate that a flaw, bug, or malfunctioning part of 
the component has been fixed. **Usually, It's a backward-compatible version compared**
to the previous version.

------------------------------------
Explore semantic versioning
https://learn.microsoft.com/en-gb/training/modules/implement-versioning-strategy/3-explore-semantic-versioning
------------------------------------

It isn't a standard but does offer a consistent way of expressing 
the intent and semantics of a particular version.
It describes a version for its backward compatibility with previous versions.
Semantic versioning uses a three-part version number and an extra label.

----------
Examples:
----------
These versions don't have any labels.
1.0.0 and 3.7.129. 

For prerelease versions, it's customary to use a label after the regular 
version number. A label is a textual suffix separated by a hyphen from 
the rest of the version number.
The label itself can be any text describing the nature of the prerelease.
Early adopters can take a dependency on a prerelease version to build using 
the new package.

rc1, beta27, alpha
1.0.0-rc1

-------------------------------------------------------------------------
PT01-Q28: 

You plan to create a package that will be made available to the public.
The package will contain .NET code artifacts.

Which package management tool should you use?

Docker
Maven
NuGet
PyPi

MY ANSWER TO PT01-Q28
--------------------------------------------------------------------------
NuGet
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q28
--------------------------------------------------------------------------
NuGet

NuGet is the standard package management tool for .NET code artifacts.

Maven:
is a tool used for the management of Java-based projects. 

PyPi:
provides a software repository for Python packages. 

Docker:
is a tool used for the management of container images.
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q27: 

You are configuring an Azure DevOps self-hosted agent to implement UI testing
for a pipeline.
What should you do first to allow UI testing?

Select only one answer.

Enable auto-logon.
Run a screen resolution task.
Run a unit test.
Run tests in parallel.

MY ANSWER TO PT01-Q27
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q27
--------------------------------------------------------------------------
Enable auto-logon.

When self-hosted agents are used, auto-logon must be enabled to allow 
UI tests to run. 

A screen resolution task: 
allows additional configurations to be done, but an auto-logon configuration
is needed first to allow the test to run. 

Run tests in parallel:
To reduce the time of test activities, running tests in parallel can be useful,
but this strategy does not address this scenario. 

A unit test is the first step to add testing to the development process.
--------------------------------------------------------------------------

REFS:

---------------------------
UI testing considerations
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/ui-testing-considerations?view=azure-devops&tabs=mstest#setting-screen-resolution
---------------------------

When running automated tests in the CI/CD pipeline, you may need 
a special configuration in order to run UI tests such as tests in

- Selenium
- Appium 
- Coded UI 


------------------------------------
Headless mode or visible UI mode?
Headless: faster.

Visible UI mode: slower.
In this mode, the browser runs normally and the UI components are visible. 
When running tests in this mode on Windows, special configuration of the agents is required.
------------------------------------
Visible UI testing using self-hosted Windows agents
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/ui-testing-considerations?view=azure-devops&tabs=mstest#visible-ui-testing-using-self-hosted-windows-agents
------------------------------------

Microsoft-hosted agents are pre-configured for UI testing and UI tests for both 
web apps and desktop apps. 
Microsoft-hosted agents are also pre-configured with popular browsers and 
matching web-driver versions that can be used for running Selenium tests.

Agents that are configured to run as service can run Selenium tests only with headless browsers.
If you are not using a headless browser, or if you are running UI tests for desktop apps, 
Windows agents **must** be configured to run as an interactive process with 
**auto-logon enabled**.

***
When configuring agents, select 'No' when prompted to run as a service. 
Subsequent steps then allow you to configure the agent with auto-logon. 
When your UI tests run, applications and browsers are launched in the 
context of the user specified in the auto-logon settings.
***

---------------------------
Setting screen resolution
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/ui-testing-considerations?view=azure-devops&tabs=mstest#setting-screen-resolution
---------------------------
Before running UI tests you may need to adjust the screen resolution so 
that apps render correctly. 

---------------------------------
Screen resolution utility task
https://marketplace.visualstudio.com/items?itemName=ms-autotest.screen-resolution-utility-task
---------------------------------
For this, a screen resolution utility task is available from Marketplace.
By default, this utility sets the resolution to the optimal value supported by the agent machine.


-----------------------------------------------
Provisioning agents in Azure VMs for UI testing
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/ui-testing-considerations?view=azure-devops&tabs=mstest#provisioning-agents-in-azure-vms-for-ui-testing
-----------------------------------------------
If you are provisioning virtual machines (VMs) on Azure, agent configuration 
for UI testing is available through the:

Agent artifact for DevTest Labs.
https://github.com/Azure/azure-devtestlab/tree/master/Artifacts/windows-vsts-build-agent

-------------------------------------
Troubleshooting failures in UI tests
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/ui-testing-considerations?view=azure-devops&tabs=mstest#troubleshooting-failures-in-ui-tests
-------------------------------------

When you run UI tests in an unattended manner, capturing diagnostic data such as 
screenshots or video is useful for discovering the state of the application when
the failure was encountered.

Most UI testing frameworks provide the ability to capture screenshots. 
The screenshots collected are available as an attachment to the test results when 
these results are published to the server.

--------------------------
the Visual Studio test task
--------------------------
If you use the Visual Studio test task to run tests, captured screenshots must be
added as a result file in order to be available in the test report. 
https://learn.microsoft.com/en-us/azure/devops/pipelines/tasks/reference/vstest-v2?view=azure-pipelines

For this, use the following code:

--------------------------------------------------------------------
Run quality tests in your build pipeline by using Azure Pipelines
https://learn.microsoft.com/en-gb/training/modules/run-quality-tests-build-pipeline/
--------------------------------------------------------------------


-------------------------------------------------------------------------
PT01-Q26: 

You need to create custom availability tests for Azure Monitor.
What should you use?

Select only one answer.

Azure App Configuration
Azure DevOps Starter
Azure DevTest Labs
Azure Functions

MY ANSWER TO PT01-Q26
--------------------------------------------------------------------------
Azure Functions
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q26
--------------------------------------------------------------------------
Azure Functions

Azure Functions provides the ability to create and run custom availability
tests by relying on the TrackAvailability() method (included in the Azure SDK for .NET).


--------------------------------------------------------------------------

REFS:

--------------------------------------------------------
Multi-step web tests will be retired on 31 August 2024
https://azure.microsoft.com/en-gb/updates/retirement-notice-transition-to-custom-availability-tests-in-application-insights/
--------------------------------------------------------
Multi-step web tests depend on Visual Studio webtest files, and it has been announced 
that Visual Studio 2019 will be the last version with webtest functionality. 
Therefore, we are retiring multi-step web tests within Application Insights 
on 31 August 2024.  

Recommended action:
Rewrite your existing multi-step web tests as custom availability tests
before 31 August 2024.
-----------------------------------------
Review TrackAvailability() test results
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-azure-functions
-----------------------------------------

-----------------------------------------
Set up and run availability tests
https://learn.microsoft.com/en-gb/training/modules/configure-provision-environments/6-set-up-run-availability-tests
-----------------------------------------

Some applications have specific Health endpoints that an automated process can check.
The Health endpoint can be an HTTP status or a complex computation that uses and
consumes crucial parts of your application.

For example, you can create a Health endpoint that queries the database. 
This way, you can check that your application is still accessible, but also 
the database connection is verified.

------------------------------------------------------------
Azure has the functionality to develop Availability tests. 
You can use these tests in the pipeline and as release gates.
In Azure, you can set up availability tests for any HTTP or HTTPS 
endpoint accessible from the public internet.
You don't have to add anything to the website you're testing. 
------------------------------------------------------------

There are two types of availability tests:

-1 URL ping test: 
a simple test that you can create in the Azure portal. 
You can check the URL and check the response and status code of the response.

-2 Multi-step web test: 
Several HTTP calls that are executed in sequence.

--------------------------------------------------------



-------------------------------------------------------------------------
PT01-Q25: 

You manage the deployment of an Azure App Service web app named App1 to 
multiple on-premises locations.
You plan to implement Application Insights to monitor App1.
You need to authorize App1 to access to an Application Insights resource.

Which authorization method should you use?

Select only one answer.

access key
Azure AD managed identity
Azure AD security principal
instrumentation key

MY ANSWER TO PT01-Q25
--------------------------------------------------------------------------
instrumentation key
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q25
--------------------------------------------------------------------------
instrumentation key

An instrumentation key: 
is used to authorize access to an Application Insights resource. 

An access key:
is used to authorize access to an Azure Storage account. 

Azure AD security principals:
AND 
managed identities:
are not supported for authorizing access to an Application Insights resource.

--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q24: 

You manage the deployment of an Azure App Service web app named App1 
in multiple Azure regions.

You need to identify an Azure service that includes built-in functionality, 
allowing you to validate the availability of App1 from multiple locations 
around the world.

What should you identify?

Select only one answer.

Application Insights
Azure Advisor
Azure App Configuration
Azure Service Health

MY ANSWER TO PT01-Q24
--------------------------------------------------------------------------
Application Insights
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q24
--------------------------------------------------------------------------
Application Insights

Application Insights includes built-in functionality that allows you to validate
the availability of App1 from multiple locations around the world. 

App Configuration: 
is a key/value store that you can use to facilitate the implementation of 
feature flags. 

Azure Advisor: detects misconfiguration issues. 

Service Health: 
provides information about the status of platform services, not individual apps.

--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q23: 

Your Azure DevOps YAML pipeline needs to use multiple repositories to build artifacts.
Which step should you include in the pipeline?

checkout
pool
script
trigger

MY ANSWER TO PT01-Q23
--------------------------------------------------------------------------
checkout
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q23
--------------------------------------------------------------------------
checkout
The checkout step is required to gain access to repositories from a YAML pipeline.
The trigger, pool, and script steps are all optional in this scenario.
They are not required to gain access to repositories from a YAML pipeline.
--------------------------------------------------------------------------

REFS:

-------------------------------------------
Use multiple repositories in your pipeline
https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/6-use-multiple-repositories-your-pipeline
-------------------------------------------

Pipelines often rely on various repositories.
By using multiple checkout steps in your pipeline, you can fetch and check 
out other repositories to the one you use to store your YAML pipeline.

----------------------------------------
Specify multiple repositories
----------------------------------------
Supported repositories are 
----------------------
- Azure Repos Git 
- GitHub
- BitBucket Cloud
----------------------

Repositories can be specified as a 
----------------------
-1 repository resource   
-2 in line with the checkout step. 
----------------------

The following combinations of checkout steps are supported.

-If there are no checkout steps, the default behavior is checkout: self is the first step.
-If there's a single checkout: none step, no repositories are synced or checked out.
-If there's a single checkout: self step, the current repository is checked out.
-If there's a single checkout step that isn't self or none, that repository is checked out instead of self.
-If there are multiple checkout steps, each named repository is checked out to a folder named after the repository. 
 Unless a different path is specified in the checkout step, 
 use checkout: self as one of the checkout steps.

----------------------------------------
Repository resource 
When is it necessary?
How to do it?
----------------------------------------

If your repository type requires 
- a service connection or 
- other extended resources field

you must use a repository resource.
Even if your repository type doesn't require a service connection, 
you may use a repository resource. For example, you have a repository resource defined
already for templates in a different repository.

-------------
Example YAML
-------------
If the self-repository is named CurrentRepo, the script command produces the following output:
CurrentRepo MyAzureReposGitRepo MyBitBucketRepo MyGitHubRepo.
In this example, the repositories' names are used for the folders because no path is specified
in the checkout step.

```
resources:
  repositories:

  - repository: MyGitHubRepo # The name used to reference this repository in the checkout step.
    type: github
    endpoint: MyGitHubServiceConnection
    name: MyGitHubOrgOrUser/MyGitHubRepo

  - repository: MyBitBucketRepo
    type: bitbucket
    endpoint: MyBitBucketServiceConnection
    name: MyBitBucketOrgOrUser/MyBitBucketRepo

  - repository: MyAzureReposGitRepository
    type: git
    name: MyProject/MyAzureReposGitRepo

trigger:

- main

pool:
  vmImage: 'ubuntu-latest'

steps:

- checkout: self
- checkout: MyGitHubRepo
- checkout: MyBitBucketRepo
- checkout: MyAzureReposGitRepository


- script: dir $(Build.SourcesDirectory)
```


------------------------
Inline
How to do it?
------------------------

If your repository doesn't require a service connection, you can declare it according to your checkout step.
The default branch is checked out unless you choose a specific ref.

```
steps:

- checkout: git://MyProject/MyRepo # Azure Repos Git repository in the same organization
```

If you're using inline syntax, choose the ref by appending @ref. For example:

```
- checkout: git://MyProject/MyRepo@features/tools # checks out the features/tools branch
- checkout: git://MyProject/MyRepo@refs/heads/features/tools # also checks out the features/tools branch.
- checkout: git://MyProject/MyRepo@refs/tags/MyTag # checks out the commit referenced by MyTag.
```

------------------
GitHub repository
------------------
Azure Pipelines can automatically build and validate every pull request and commit to your GitHub repository.
When creating your new pipeline, you can select a GitHub repository and then a YAML file in that repository 
(self repository). By default, this is the repository that your pipeline builds.

Azure Pipelines must be granted access to your repositories to trigger their 
builds and fetch their code during builds.

There are three authentication types for granting Azure Pipelines access to your GitHub
repositories while creating a pipeline.
----------------------------------------
-1 GitHub App.
-2 OAuth.
-3 Personal access token (PAT).
----------------------------------------

-----------------------
Configuring CI Triggers
-----------------------

YAML pipelines are configured by default with a CI trigger on all branches.

YAML
```
trigger:
    - main
    - releases/*

```

You can configure complex triggers that use exclude or batch.

```
# specific branches build
trigger:
    branches:
        include:
            - master
            - releases/*
        exclude:
            - releases/old*
```

-----------------------
Configuring PR Triggers
-----------------------

it's possible to configure pull request (PR) triggers to run whenever a pull request
is opened with one of the specified target branches or when updates are made to such
 a pull request.

To validate pull requests that target main and releases/* and start a new run 
- the first time a new pull request is created and
- after every update made to 
the pull request: 

```
pr:
    - main
    - releases/*
```
You can specify the full name of the branch or a wildcard.


-------------------------
Explore YAML resources
-------------------------

Resources in YAML represent 
 - sources of pipelines
 - repositories 
 - containers. 

----------------------------------------------
For more information on Resources, see here.
https://learn.microsoft.com/en-us/azure/devops/pipelines/process/resources?view=azure-devops&tabs=schema&source=docs
----------------------------------------------

```
resources:
  pipelines: [ pipeline ]  
  builds: [ build ]
  repositories: [ repository ]
  containers: [ container ]
  packages: [ package ]
  webhooks: [ webhook ]
```

If you have an Azure pipeline that produces **artifacts**, your pipeline can consume 
the artifacts by using the pipeline keyword to define a pipeline resource.

```
resources:
  pipelines:

  - pipeline: MyAppA
    source: MyCIPipelineA

  - pipeline: MyAppB
    source: MyCIPipelineB
    trigger: true

  - pipeline: MyAppC
    project:  DevOpsProject
    source: MyCIPipelineC
    branch: releases/M159
    version: 20190718.2
    trigger:
      branches:
        include:

        - master
        - releases/*
        exclude:

        - users/*
```

------------------
Container resource
------------------
Container jobs let you isolate your tools and dependencies inside a container. 
The agent launches an instance of your specified container then runs steps inside it. 
The container keyword lets you specify your container images.

```
resources:
  containers:

  - container: linux
    image: ubuntu:16.04

  - container: windows
    image: myprivate.azurecr.io/windowsservercore:1803
    endpoint: my_acr_connection

  - container: my_service
    image: my_service:tag
    ports:

    - 8080:80 # bind container port 80 to 8080 on the host machine
    - 6379 # bind container port 6379 to a random available port on the host machine
    volumes:

    - /src/dir:/dst/dir # mount /src/dir on the host into /dst/dir in the container
```

------------------
Repository resource
------------------

Let the system know about the repository if:

- If your pipeline has templates in another repository.
- If you want to use multi-repo checkout with a repository that requires a service connection.

The repository keyword lets you specify an external repository.

```
resources:
  repositories:

  - repository: common
    type: github
    name: Contoso/CommonTools
    endpoint: MyContosoServiceConnection

```
----------------------------------------
SPECIAL USE CASE: no Repo to check out!
----------------------------------------
There's also a use case for not checking out any repository in the pipeline. 
It can be helpful in cases where you're setting up a pipeline to do a job 
that has no dependency on any repository.

----------------------------------------
Detail templates/Pipeline Templates
https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/4-detail-templates
----------------------------------------



-------------------------------------------------------------------------
PT01-Q22: 

You have been working as a DevOps engineer on a project for the last two years.
Due to the amount of work, you need to have your default branch histories clean
and easy to follow.

Which branch policy should you implement?

Select only one answer.

Automatically include code reviewers.
Check for comment resolution.
Limit allowed merge types.
Require a minimum number of reviewers.

MY ANSWER TO PT01-Q22
--------------------------------------------------------------------------
Limit allowed merge types.
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q22
--------------------------------------------------------------------------
Limit allowed merge types.

**squash merge,** 
You can limit the allowed merge types to only the use of **squash merge,** 
which can be used to condense the history of changes in your default branch.

Requiring a minimum number of reviewers is not used to reduce the history 
on merge actions. 

Checking for comment resolution is useful to verify that reviewers’ comments
are resolved, but not to condense the history of changes. 

A policy can be defined to include specific reviewers, but it is not able
to clean the branch history.

--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q21:

You are implementing a forking workflow.
What is the minimum number of repositories that each developer should use?
Select only one answer.

1
2
3
4

MY ANSWER TO PT01-Q21
--------------------------------------------------------------------------
2
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q21
--------------------------------------------------------------------------
2
When using a forking workflow, each developer should have two repositories, 
one private local side and the other public server-side. 
While it is technically possible to use only a server-side repository, 
this violates the principle of the forking workflow.
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q20: 

In a trunk-based development workflow, which step immediately follows 
creating a feature branch?

Select only one answer.

adding commits
code review
deployment
opening a pull request

MY ANSWER TO PT01-Q20
--------------------------------------------------------------------------
adding commits
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q20
--------------------------------------------------------------------------
adding commits

in a trunk-based development workflow
creating a feature branch =>
Adding commits => 
Opening a pull request => 
Code review => 
Deployment 
--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q19: 

In which Git-based branch workflow does each developer have their own server-side repository?

Select only one answer.

centralized
forking
GitFlow
trunk-based

MY ANSWER TO PT01-Q19
--------------------------------------------------------------------------
forking
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q19
--------------------------------------------------------------------------
forking
A forking workflow is the only Git-based workflow in which each developer has
their own server-side repository. 
For all others, developers share the same server-side repository.
--------------------------------------------------------------------------

REFS:

--------------------------------------------------------
Explore branch workflow types - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/2-explore-branch-workflow-types
--------------------------------------------------------

--------------------------------------------------------
Branch strategically - Azure Repos | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/repos/tfvc/branch-strategically?view=azure-devops
--------------------------------------------------------

-------------------------------------------------------------------------
PT01-Q18: 

You need to implement a Git hook that will be triggered automatically in 
response to a Git commit being run on a Linux server.

What should you do to ensure that the predefined Git hook script can be executed?

Select only one answer.

Add an extension to the script file.
Modify the location of the file.
Modify the permissions on the file.
Remove the extension of the script file.

MY ANSWER TO PT01-Q18
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q18
--------------------------------------------------------------------------
Remove the extension of the script file.
--------------------------------------------------------------------------

REFS:

To ensure that the predefined Git hook script can be executed, you need to 
remove its existing extension (.sample), rather than adding the extension. 
**On a Windows server**, there is no need to modify the existing permissions
on the script file. 
The file must reside in its default location.

--------------------
Implement Git hooks
https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/3-implement
--------------------

------------
On Linux
------------
Navigate to the repo ** .git\hooks ** directory.

If you open that folder, you'll find a file called pre-commit.sample.
To enable it, ** rename it** to pre-commit by removing the .sample 
extension 
** and ** 
making the script executable.


------------
On Windows
Windows isn't a Unix-like OS. 
------------

STEP-1:
Change the SheBang: 
---------
#!/bin/sh
---------
Providing the path to the sh executable on your system. 
It's using the 64-bit version of Git for Windows, so the baseline looks like this:
------------------------------------------
#!C:/Program\ Files/Git/usr/bin/sh.exe
------------------------------------------

The repo .git\hooks folder isn't committed into source control. 
From Git version 2.9, you can now map Git hooks to a folder that can be committed 
into source control.

updating the **global settings configuration for your Git repository**
```
git config --global core.hooksPath '~/.githooks'
```
If you ever need to overwrite the Git hooks you have set up on the client-side,
you can do so by using the **no-verify switch**:
```
git commit --no-verify
```

-------------------------------------------------------------------------
PT01-Q17: 

You manage 100 on-premises servers that run Linux and Windows.

You need to collect logs from all the servers and make the logs available for
analysis directly from the Azure portal. The solution must meet the following 
requirements:

- Provide the ability to send data from Linux virtual machines to 
  multiple Log Analytics workspaces.

- Support the use of XPATH queries to filter Windows events for collection.
- Minimize the number of agents installed on the servers.

Which agent should you install?

Select only one answer.

the Azure Monitor agent
the Connected Machine agent
the Dependency agent
the Telegraph agent

MY ANSWER TO PT01-Q17
--------------------------------------------------------------------------
the Azure Monitor agent
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q17
--------------------------------------------------------------------------
the Azure Monitor agent

The Azure Monitor agent replaces: 
the Log Analytics agent. 
The diagnostic extension. t
he Telegraph agent. 

It can centrally configure data collection for different data from different
sets of virtual machines, sending the data from Linux virtual machines to 
multiple workspaces, and using XPATH queries to filter Windows events for
collection. 

The Telegraph agent supports only Linux operating systems. 

The Connected Machine agent is used by Azure Arc, which is not required in
this scenario. 

Using the Dependency agent increases the number of agents to be installed
on the target servers.

--------------------------------------------------------------------------

REFS:

----------------------------
Learn  Training  Browse  Azure Monitor fundamentals  Introduction to Azure Monitor 
https://learn.microsoft.com/en-gb/training/modules/intro-to-azure-monitor/
Metrics & Logs
https://learn.microsoft.com/en-gb/training/modules/intro-to-azure-monitor/3-how-azure-monitor-works
----------------------------

----------------------
Metrics
----------------------
Azure Monitor can collect several types of metrics, including:

----------------------
- Azure platform metrics: 
----------------------
Azure Monitor starts collecting metrics data from Azure resources as soon as they're added to a subscription.
A list of resource-specific metrics is automatically available for each Azure resource type.

----------------------
- Custom metrics: 
----------------------
Azure Monitor can also collect metrics from other sources, including applications and agents running on VMs.
**You can send custom metrics to Azure Monitor via the Azure Monitor Agent**, other agents and extensions, 
or directly to the **Azure Monitor REST API**.

----------------------
-Prometheus metrics:
----------------------
Azure Monitor managed service for Prometheus collects metrics from Azure Kubernetes Service (AKS) 
or other Kubernetes clusters. Prometheus metrics share some characteristics with platform and custom metrics,
but have different features to support open-source analysis and alerting tools like PromQL and Grafana.

----------------------------
Collect events and performance counters from virtual machines with Azure Monitor Agent
https://learn.microsoft.com/en-gb/azure/azure-monitor/agents/data-collection-rule-azure-monitor-agent?tabs=portal
----------------------------

----------------------
Logs
----------------------
Logs are textual records of events, actions, and messages that occur in a resource or application. 
Logs can capture information about errors, warnings, user actions, and application state changes.
Logs are essential for retrospective analysis of issues, helping to reconstruct the chain of events 
that led up to a problem.
Azure Monitor Logs is a feature of Azure Monitor that lets you store, manage, and analyze log and 
performance data from monitored resources. 
You set up a common workspace called a ** Log Analytics workspace **.

While metrics are numeric, logs can include the following data:

- Text: 
  Human-readable text entries that provide context, details, and descriptions of events.

- Unstructured data: 
  Log entries in various formats that don't fit neatly into predefined numerical values.

- Contextual information: 
  Insights into the context surrounding an event, which is invaluable for root cause analysis.

You can use log queries in the following scenarios:

 - Use a basic query to answer a common question.
 - Do complex data analysis to identify critical patterns in your monitoring data.
 - Use queries in alert rules to be proactively notified of issues.
 - Visualize query results in a workbook or dashboard.

-------------------------------------------------------------------------
PT01-Q16: 

You have an Azure DevOps organization.
You plan to use a new Git repository.
You need to define a branch strategy. 
The solution must track all detected bugs in the code in an isolated code configuration.

Which branch strategy should you implement?

Select only one answer.

development
feature
release
tags

MY ANSWER TO PT01-Q16
--------------------------------------------------------------------------
feature
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q16
--------------------------------------------------------------------------
feature

Feature: 
provides isolation to work with new features or bugs. 

Development:
is used in Team Foundation version control and is not specific to bug fixes isolation. 

Release:
is used when the code is ready to be implemented. 

Tags:
are useful to mark a point in the code history as important. 
Tags introduce extra steps that are not necessary if branches are used for releases.

--------------------------------------------------------------------------

REFS:

---------------------------
Manage Git branches and workflows - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/
---------------------------
Explore feature branch workflow
https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/3-explore-feature-branch-workflow
---------------------------
The core idea behind the Feature Branch Workflow is that all feature development 
should take place in a dedicated branch instead of the main branch.

-------------------------------
1-MAIN BRANCH CAN NEVER BREAK
-------------------------------
It also means the main branch will never contain broken code, a huge advantage
for continuous integration environments.

-------------------------------
2-ENABLE PULL REQUESTS: 
discussion + pulling-out + support + sharing
-------------------------------
Encapsulating feature development also makes it possible to use pull requests, 
which are a way to start discussions around a branch.
They allow other developers to sign out on a feature before it integrates into 
the official project.
Or, if you get stuck in the middle of a feature, you can open a pull request 
asking for suggestions from your colleagues.
Pull requests make it incredibly easy for your team to comment on each other's work. 
 It allows sharing a feature with other developers without touching any official code.

-----------------------------------
Trunk-based development workflow
-----------------------------------
Instead of committing directly to their local main branch, developers create a 
new branch whenever they start working on a new feature.
Feature branches should have descriptive names, like new-banner-images or bug-91. 
The idea is to give each branch a clear, highly focused purpose.

-1 Create a branch: feature or bug
-2 Add commits

-3 Open a pull request
  You can open a Pull Request at any point during the development process when:
  -You've little or no code but want to share screenshots or general ideas.
  -You're stuck and need help or advice.
  -You're ready for someone to review your work.
  -@mention system in your Pull Request message, you can ask for feedback 
   from specific people or teams, whether they're down the hall or 10 time zones away.
  ---------------------------------------------------------------------------------  
  **Fork & Pull Model**
  -If you're using a **Fork & Pull Model**, Pull Requests provide a way to notify 
   project maintainers about the changes you'd like them to consider.
   ---------------------------------------------------------------------------------
   ** Shared Repository Model **
   If you're using a Shared Repository Model, Pull Requests help start code review 
   and conversation about proposed changes before they're merged into the main branch.
   ---------------------------------------------------------------------------------

-4 Discuss and review your code

You can also continue to push to your branch, considering discussion and feedback about your commits.
Pull Request comments are written in Markdown, so you can embed images and emojis, use pre-formatted 
text blocks, and other lightweight formatting.

-5 Deploy

With Git, you can deploy from a branch for final testing in an environment before merging to the main.

-6 Merge (the PR)
Once your changes have been verified, it's time to merge your code into the main branch.
You can associate issues with code by incorporating specific keywords into your Pull Request text. 

----------------------
OTHER GIT WORKFLOWS
----------------------
Other Git workflows, like 

-1 the Git Forking Workflow 
-2 Gitflow Workflow

these are repo-focused and can use the Git Feature Branch Workflow to manage their branching models.

---------------------------
the Git Forking Workflow 
---------------------------
The Forking Workflow is fundamentally different than the other workflows discussed in this tutorial.
Instead of using a single server-side repository to act as the "central" codebase, it gives 
** every developer ** a server-side repository.

It means that each contributor has two Git repositories:
- A private local one.
- A public server-side one.


---------------------------
Adopt a Git branching strategy
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#use-feature-branches-for-your-work
Git branching guidance - Azure Repos | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#use-feature-branches-for-your-work
---------------------------
-1 Name your feature branches by convention
Some suggestions for naming your feature branches:

users/username/description
users/username/workitem
bugfix/description
feature/feature-name
feature/feature-area/feature-name
hotfix/description

-2 Use feature flags to manage long-running branches

-3 Review and merge code with pull requests
Some suggestions for successful pull requests:

  - Two reviewers is an optimal number based on research.
  - If your team already has a code review process, bring pull requests into what you're already doing.
  - Take care assigning the same reviewers to a large number of pull requests. 
    Pull requests work better when reviewer responsibilities are shared across the team.
  - Provide enough detail in the description to quickly bring reviewers up to speed with your changes.
  - Include a build or linked version of your changes running in a staged environment with your pull request. Others can easily test the changes.

------------------------------------------------
-4 Keep a high quality, up-to-date main branch

Set up a branch policy for your main branch that:
https://learn.microsoft.com/en-gb/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser
------------------------------------------------
UI: Project Settings > Repository > Policies > Branch Policies > <Branch Name>.
** You can also use AzureDevOps CLI **

- Require a minimum number of reviewers
- Check for linked work items
- Check for comment resolution
- Limit merge types
  -------------------------------------------------------
  -1 Basic merge:
   (no fast-forward) creates a merge commit in the target 
   whose parents are the target and source branches.

  -2 Squash merge:
  creates a linear history with a single commit in the target
  branch with the changes from the source branch. 
  Learn more about squash merging and how it affects branch history.
  https://learn.microsoft.com/en-gb/azure/devops/repos/git/merging-with-squash?view=azure-devops

  -3 Rebase and fast-forward:
  creates a linear history by replaying source commits onto 
  the target branch with no merge commit.

  -4 Rebase with merge commit:
  replays the source commits onto the target and also creates a merge commit.
  -------------------------------------------------------

-5 Build validation

You can set a policy requiring PR changes to build successfully before the PR can complete.
A build validation policy queues a new build when a new PR is created or changes are pushed 
to an existing PR that targets the branch. 
The build policy evaluates the build results to determine whether the PR can be completed.

-----------------------------------
-6 Status checks: EXTERNAL SERVICES
-----------------------------------
External services can use the PR Status API to post detailed status to your PRs. 

-7 Automatically include code reviewers

-8 Bypass branch policies* 
https://learn.microsoft.com/en-gb/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser#bypass-branch-policies

-9 Path Filters
https://learn.microsoft.com/en-gb/azure/devops/repos/git/branch-policies?view=azure-devops&tabs=browser#path-filters
Several branch policies offer path filters. 
 If a path filter is set, the policy applies only to files that match the path filter. 
------------------------------- 
/WebApp/Models/Data.cs
/WebApp/*
*/Models/Data.cs
*.cs
-------------------------------
You can specify multiple paths using ; as a separator. Example:
/WebApp/Models/Data.cs;/ClientApp/Models/Data.cs

Paths prefixed with ! are excluded
/WebApp/*;!/WebApp/Tests/*

----------------
Manage releases
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#manage-releases
----------------
Use release branches to coordinate and stabilize changes in a release of your code.
Create as many release branches as you need.
Keep in mind that each active release branch represents another version of the code you need to support. 
Lock release branches when you're ready to stop supporting a particular release.
Create a release branch from the main branch when you get close to your release or other milestone, 
such as the end of a sprint. 

Give this branch a clear name associating it with the release, for example: 
release/20

******
This branch is long-lived 
and 
isn't merged back into the main branch in a pull request, 
unlike the feature branches. 
******
BUGS ON RELEASES
Create branches to fix bugs from the release branch and merge them back 
into the release branch in a pull request.
******

-------------------------------------
Port changes back to the main branch
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#port-changes-back-to-the-main-branch
-------------------------------------
Make sure that fixes land in both your release branch and your main branch. 
To prevent regression in your code.

---------------
APPROACH-1
---------------
make fixes in the release branch, then bring changes into your main branch.
Use **cherry-picking instead of merging ** so that you have exact control 
over which commits are ported back to the main branch. 

Merging the release branch into the main branch can bring over release-specific
changes you don't want in the main branch.

Update the main branch with a change made in the release branch with these steps:

STEP-1:
Create a new feature branch off the main branch to port the changes.
STEP-2:
Cherry-pick the changes from the release branch to your new feature branch.
STEP-3:
Merge the feature branch back into the main branch in a second pull request.

This **release branch workflow** keeps the pillars of the basic workflow intact:
- feature branches
- pull requests 
- a strong main branch that always has the latest version of the code.

The release branch strategy extends the basic feature branch workflow to handle releases.
Your team doesn't have to adopt any new version control process other than
-the cherry-pick to port changes.

---------------
**APPROACH-2: employed by the Azure DevOps team!**
---------------

is to always make changes in the mainline, then port those to the release branch.

---------------------------------
Why not use tags for releases?
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#why-not-use-tags-for-releases
---------------------------------

Other branching workflows use Git tags to mark a specific commit as a release.
Tags are useful for marking points in your history as important. 
Tags introduce extra steps in your workflow that aren't necessary if you're 
using branches for your releases.
*****
Tags are maintained and pushed separately from your commits!
Team members can easily miss tagging a commit and then have to go back through
the history afterwards to fix the tag.
You can also forget the extra step to push the tag! 
Leaving the next developer working from an older version of the code when 
supporting the release.
*****

-------------------
Manage deployments
https://learn.microsoft.com/en-gb/azure/devops/repos/git/git-branching-guidance?view=azure-devops#manage-deployments
-------------------

You can handle multiple deployments of your code in the same way you handle 
multiple releases.
Treat the environment branches like release branches. 
Create a clear naming convention, such as:
---------------------------
deploy/performance-test
---------------------------

---------------------------
Main Branch => Deployment Branch
Deployment Branch => Main Branch 
---------------------------
Your team should agree on a process to update deployment branches with the 
code from your main branch. 
**Cherry-pick bug fixes in the deployment branch back to the main branch.**
**Use the same steps as porting changes from a release branch.**

STEP-1:
Create a new feature branch off the main branch to port the changes.
STEP-2:
Cherry-pick the changes from the DEPLOYMENT branch to your new feature branch.
STEP-3:
Merge the feature branch back into the main branch in a second pull request.

----------------------------------
Merge strategies and squash merge
https://learn.microsoft.com/en-gb/azure/devops/repos/git/merging-with-squash?view=azure-devops
------------

-------------
-1 Basic merge:
   https://learn.microsoft.com/en-gb/azure/devops/repos/git/merging-with-squash?view=azure-devops#squash-merge
------------     
  (no fast-forward) creates a merge commit in the target 
  whose parents are the target and source branches.
  This merge adds the commits of the topic branch to your main branch and 
  creates a merge commit to reconcile any conflicts between the default and
  topic branch. 

  --------------------------------
  The problem with basic merge:  
  --------------------------------
  The default branch is an accurate representation of the history of 
  ** each topic branch **, but it's difficult to use to answer broader questions
  about your project's development.

-2 Squash merge:
------------
Squash merge
https://learn.microsoft.com/en-gb/azure/devops/repos/git/merging-with-squash?view=azure-devops#squash-merge
------------
  creates a linear history with a single commit in the target
  branch with the changes from the source branch. 
  Learn more about squash merging and how it affects branch history.
  https://learn.microsoft.com/en-gb/azure/devops/repos/git/merging-with-squash?view=azure-devops

  Instead of each commit on the topic branch being added to the history of the default branch,
  a squash merge adds all the file changes to a single new commit on the default branch. 
  Squash merge commit doesn't have a reference to the topic branch, it will produce a new commit
  that contains all changes from the topic branch.
  ***
  Furthermore it is recommended to delete the topic branch to prevent any confusion.
  ***

  -------------------------------------------------------------------------
  Note:
  A simple way to think about this is the following.. 
  squash merge: gives you just the file changes 
  regular merge: gives you the file changes and the commit history.
  -------------------------------------------------------------------------

  -------------------------------
  How is a squash merge helpful?
  -------------------------------
  Keeps your default branch histories clean and easy to follow,
  without demanding any workflow changes on your team!
  The commit history of a main branch updated with squash merges has 
  **one commit for each merged branch**.   
  Contributors to the topic branch work how they want in the topic branch.

-3 Rebase and fast-forward:
  creates a linear history by replaying source commits onto 
  the target branch with no merge commit.

-4 Rebase with merge commit:
  replays the source commits onto the target and also creates a merge commit.


-------------------------------------------------------------------------
PT01-Q15: 

In a trunk-based development workflow, 
which step immediately follows creating a feature branch?

Select only one answer.

adding commits
code review
deployment
opening a pull request

MY ANSWER TO PT01-Q15
--------------------------------------------------------------------------
adding commits
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q15
--------------------------------------------------------------------------
adding commits: 
follows creating a feature branch in a trunk-based development workflow.

Opening a pull request: 
follows adding commits in a trunk-based development workflow. 

Code review: 
follows opening a pull request in a trunk-based development workflow.

Deployment: 
follows code review in a trunk-based development workflow.

--------------------------------------------------------------------------

REFS:

---------------------------------
Learn  Training  Browse  AZ-400: Development for enterprise DevOps  Manage Git branches and workflows 
Explore feature branch workflow
https://learn.microsoft.com/en-gb/training/modules/manage-git-branches-workflows/3-explore-feature-branch-workflow
---------------------------------


-------------------------------------------------------------------------
PT01-Q14: 

You are working as a DevOps engineer on a project that stores images in 
the JPEG format in a Git repository. 
The JPEG files are large.
A new team member is working on the project.

The team member cannot see binary files when cloning the repository.

Which verification step should you perform to resolve the issue?

Select only one answer.

az repos create
git clone git@ssh.dev.azure.com
git commit -m "Initial commit"
check the Git LFS client is installed

MY ANSWER TO PT01-Q14
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q14
--------------------------------------------------------------------------
check the Git LFS client is installed

The Git LFS client must be installed and configured properly to allow you 
to see binary files committed. 


git commit -m "Initial commit" 
is used to create a repository with code in an existing folder but not to
clone and work with large binary files. 

az repos create 
creates a new Git repository by using Azure Repos. 

git clone git@ssh.dev.azure.com 
clones a repository by using SSH, which is **unsupported** in Azure Repos when
Git LFS is used to track files.

--------------------------------------------------------------------------

REFS:

-----------------------------
Learn  Training  Browse  AZ-400: Development for enterprise DevOps 
Manage Git repositories
https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/
-----------------------------

-----------------------------
Use Git Large File Storage (LFS)
https://learn.microsoft.com/en-gb/azure/devops/repos/git/manage-large-files?view=azure-devops#limitations
-----------------------------


-------------------------------------------------------------------------
PT01-Q13: 

Your team is working on an Azure DevOps project using a Git repository. 
The generation of personal access tokens (PAT) is not allowed.

You need to define an authentication method to allow other services and 
applications to access your Azure DevOps account.

Which authentication method should you implement?

Active Directory
Basic
OAuth
SSH

MY ANSWER TO PT01-Q13
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q13
--------------------------------------------------------------------------

*****************
SSH authentication: 
*****************
is used when **Git Credential Manager or PATs** are not allowed. 

Basic:
is an authorization main access level, not an authentication method. 

OAuth:
is used **for REST APIs only**. 

Active Directory: is used **for on-premises deployments only**.

--------------------------------------------------------------
Guidance for authentication - Azure DevOps | Microsoft Learn
Choose the right authentication mechanism
https://learn.microsoft.com/en-gb/azure/devops/integrate/get-started/authentication/authentication-guidance?view=azure-devops
--------------------------------------------------------------
--------------------------------------------------------------
About security, authentication, authorization, and security policies - Azure DevOps | Microsoft Learn
--------------------------------------------------------------

--------------------------------------------------------------
Manage Git repositories - Training | Microsoft Learn
--------------------------------------------------------------

--------------------------------------------------------------------------

REFS:

-------------------------------------------------------------------------
PT01-Q12: 

You need to implement an automated, customized response to Git lifecycle events.
What is the location of files you should configure?
Select only one answer.

.git/hooks
.git/info
.git/objects
.git/refs

MY ANSWER TO PT01-Q12
--------------------------------------------------------------------------
.git/hooks
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q12
--------------------------------------------------------------------------
.git/hooks

The .git/hooks folder: 
contains files that can be used to implement an automated, customized response
to Git lifecycle events. 

The .git/objects folder:
contains the commit, tree, and blob objects. 

The .git/info folder: 
contains the exclude file that can be used to designate files to be excluded
from the local repo. 

The .git/refs folder: 
contains references to objects in the repositories.

--------------------------------------------------------------------------

REFS:

Introduction to Git hooks - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/2-introduction-to

Implement Git hooks
https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/3-implement

-------------------------------------------------------------------------
PT01-Q11: 

You are working as a DevOps engineer to implement policies that control the
number of days that personal access tokens (PAT) are available.

Which policy option should you configure?

Select only one answer.

full-scoped
global
lifespan
revoke leaked PATs

MY ANSWER TO PT01-Q11
--------------------------------------------------------------------------
lifespan
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q11
--------------------------------------------------------------------------
lifespan

Lifespan can be used to define the maximum lifespan of a PAT and control its
lifecycle. 

The full-scoped 
option forces the use of defined sets of scopes. 

Global grants
access to all accessible organizations in Azure DevOps. 

Revoke leaked PATs
automatically revokes any PAT that is checked into a public GitHub repository.

--------------------------------------------------------------------------

REFS:

---------------------------------
Manage personal access tokens using policies - Azure DevOps | Microsoft Learn
Set maximum lifespan for new PATs
https://learn.microsoft.com/en-gb/azure/devops/organizations/accounts/manage-pats-with-policies-for-administrators?view=azure-devops#set-maximum-lifespan-for-new-pats
---------------------------------

---------------------------------
Explore an authorization and access strategy
https://learn.microsoft.com/en-gb/training/modules/migrate-to-devops/4-explore-authorization-access-strategy
---------------------------------

-------------------------------------------------------------------------
PT01-Q10: 

You are working on a project that must raise approvals before an 
Azure Resource Manager (ARM) service connection is used for release 
deployments.

What should you configure?


an Azure AD managed identity
Organization permissions
Pipeline permissions
Project permissions

MY ANSWER TO PT01-Q10
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q10
--------------------------------------------------------------------------
Pipeline permissions

------------------------------
Using Pipeline permissions: 
------------------------------
you can specify which pipeline can consume the service connection. 
If any other YAML pipelines refer to the service connection, an authorization
request is raised, which must be approved by a connection administrator. 

------------------------------
Project permissions:
------------------------------
are used to control service connection sharing between projects but not for pipelines. 

------------------------------
Organization permissions: 
------------------------------
are used to define security groups to control service connections inside of the 
organization. 

------------------------------
Azure AD managed identity:
------------------------------
is used instead of a service principal for a service connection, but not to 
request approval to use the service connection.

--------------------------------------------------------------------------

REFS:

---------------------
Pipeline permissions [of Service Connections]
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#pipeline-permissions
---------------------
Pipeline permissions control which YAML pipelines are authorized to use 
the service connection. 
Pipeline permissions do not restrict access from Classic pipelines.

You can choose from the following options:

------------
-1 Open access
------------
for all pipelines to consume the service connection from the more options
at top-right corner of the Pipeline permissions section in security tab
of a service connection.

------------
-2 Lock down 
------------
the service connection and only allow selected YAML pipelines to consume 
the service connection.
If any other YAML pipeline refers to the service connection, an 
authorization request gets raised, which must be approved by a 
connection Administrator.

----------------------
Project permissions - Cross project sharing of service connections
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#project-permissions---cross-project-sharing-of-service-connections
----------------------
Project permissions control which projects can use the service connection. 
By default, service connections aren't shared with any other projects.
-
Only the organization-level administrators from User permissions can share 
the service connection with other projects.
-
The user who's sharing the service connection with a project should have 
at least Create service connection permission in the target project.
-
The user who shares the service connection with a project becomes 
the project-level Administrator for that service connection. 
-
The service connection name is appended with the project name and it 
can be renamed in the target project scope.
-
Organization-level administrator can unshare a service connection from any shared project
-

-----------------------------
Organization-level permissions
https://learn.microsoft.com/en-gb/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml#organization-level-permissions
-----------------------------
Any permissions set at the organization-level reflect across 
all the projects where the service connection is shared. 
There's no inheritance for organization-level permissions.

The organization-level Administrator can do the following administrative tasks:

- Manage organization-level users
- Edit all the fields of a service connection
- Share and unshare a service connection with other project

-------------------------------------------------------------------------
PT01-Q09: 

You manage Azure Pipelines.

You need to implement a self-hosted agent pool.

Which authentication method should you use to connect the agent to Azure Pipelines?

Select only one answer.

Azure AD managed identity
Azure AD service principal
personal access token (PAT)
shared access signature (SAS)

MY ANSWER TO PT01-Q09
--------------------------------------------------------------------------
personal access token (PAT)
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q09
--------------------------------------------------------------------------
personal access token (PAT)

A PAT is used to connect a self-hosted agent to Azure Pipelines. 

A SAS is used to authorize access to Azure Storage, which is not applicable
in this scenario. 

Azure AD service principals and managed identities do not support connecting
self-hosted agents to Azure Pipelines.

--------------------------------------------------------------------------

REFS:

--------------------------------
Integrate with Azure Pipelines
https://learn.microsoft.com/en-gb/training/modules/integrate-azure-pipelines/
--------------------------------

-------------------------------------------------------------------------
PT01-Q08: 

You plan to configure GitHub Actions to access GitHub secrets.
You have the following YAML.

```
01  steps:
02    - shell: pwsh
03        ?????
04        DB_PASSWORD: ${{ secrets.DBPassword }}
```
You need to complete the YAML to reference the secret. 
The solution must minimize the possibility of exposing secrets.
Which element should you use at line 03?

$env:
args:
env:
run:

MY ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q0X
--------------------------------------------------------------------------
```
01  steps:
02    - shell: pwsh
03        args:
04        DB_PASSWORD: ${{ secrets.DBPassword }}
```

args:
-----------------------------------------------------------------------------
https://docs.github.com/en/actions/creating-actions/metadata-syntax-for-github-actions
-----------------------------------------------------------------------------

The most secure way to pass secrets to run commands is to reference them as
environment variables, rather than arguments. 

This requires the use of the env: element. 

The $env: 
notation is used to reference an environment variable, but the 
intention of this question is to define rather than reference. 

The run: 
element defines which command to run, so it follows the env: notation.

--------------------------------------------------------------------------

REFS:

Learn  Training  Browse  Manage GitHub Actions in the enterprise 
Manage encrypted secrets
https://learn.microsoft.com/en-us/training/modules/manage-github-actions-enterprise/manage-encrypted-secrets

Secrets are ** encrypted environment variables ** you can create to store:
- tokens 
- credentials
- any other type of sensitive information your GitHub Actions workflows and actions might rely on. 

Once created, they become available to use in the workflows and actions that have access to the 
organization, repository, or repository environment where they are stored.

You can:

-1 Manage encrypted secrets at organization level
   This will minimize the management overhead in your enterprise.
   This way the credentials can be used in the workflows without being exposed.
   Settings >> Secrets and variables > Actions > New organization secret

-2 Manage encrypted secrets at repository level
   scoped to a specific repository
   GitHub Enterprise Cloud and GitHub Enterprise Server also let you create secrets at repository level.
   Settings >> Secrets and variables > Actions > New repository secret

-------------------------------------------------------
Access encrypted secrets within actions and workflows
-------------------------------------------------------

```
steps:
  - name: Hello world action
    with: # Set the secret as an input
      super_secret: ${{ secrets.SuperSecret }}
    env: # Or as an environment variable
      super_secret: ${{ secrets.SuperSecret }}
```
To access an encrypted secret in an action, you must specify the secret as an 
input parameter in the action.yml metadata file. 
If you need to access the encrypted secret in your action's code, the action 
code could read the value of the input using the $SUPER_SECRET environment variable.

```
inputs:
  super_secret:
    description: 'My secret token'
    required: true
```

----------------------------
Learn  Training  Browse  Learn continuous integration with GitHub Actions 
Use secrets in a workflow
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/9-use-secrets-workflow
----------------------------
Secrets aren't passed automatically to the runners when workflows are executed.
Instead, when you include an action that requires access to a secret, you use 
the secrets context to provide it.

```
steps:
  - name: Test Database Connectivity
    with:
      db_username: ${{ secrets.DBUserName }}
      db_password: ${{ secrets.DBPassword }}
```

--------------------
Command-line secrets
--------------------
Secrets shouldn't be passed directly as command-line arguments as they may be visible to others.
Instead, treat them like environment variables:

```
steps:
  - shell: pwsh
    env:
      DB_PASSWORD: ${{ secrets.DBPassword }}
    run: |
      db_test "$env:DB_PASSWORD"
```

--------------------------------------------------------------------------
PT01-Q07: 

You are working with Azure DevOps on a Scrum project. 
You need to monitor and count work items as they move to a different state.

What should you use to perform the monitoring activity?

cumulative flow diagrams (CFD)
cycle time
sprint burndown
velocity widget

MY ANSWER TO PT01-Q07
--------------------------------------------------------------------------
cumulative flow diagrams (CFD)
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q07
--------------------------------------------------------------------------
cumulative flow diagrams (CFD)
--------------------------------------------------------------------------

REFS:

Two CFD charts can be used to monitor the flow of work:
-1 in-context report:  you can view from a team backlog or Kanban board 
-2 CFD widget: you can add to a dashboard.

The velocity widget:
is used to track the amount of work that a team can achieve in a sprint. 

Sprint burndown:
helps monitor the remaining work in a sprint. 

Cycle time:
shows work items closed in a specified timeframe.

----------------------
View and configure the cumulative flow diagram (CFD) reports - Azure DevOps | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/report/dashboards/cumulative-flow?view=azure-devops
----------------------

You use cumulative flow diagrams (CFD) to monitor the flow of work through a system.
CFDs help teams monitor the count of work items as they progressively move through various 
workflow states. The CFD shows the count of items in each Kanban column for the 
selected time period. From this chart, you can gain an idea of 
------------------------------------ 
- the amount of work in progress =  counts unfinished requirements
- lead time = indicates the amount of time it takes to complete a requirement once work has started
------------------------------------ 

These diagrams can show the flow of ,=any of the following depending on the process \
selected for your project: 

(Agile | Basic | Scrum | CMMI=Capability Maturity Integration)

- epics 
- features  
- user stories
- issues 
- product backlog items  
- requirements

Two CFD charts can be used to monitor the flow of work:
----------------------------------------------------------------------
-1 in-context report:  you can view from a team backlog or Kanban board 
-2 CFD widget: you can add to a dashboard.
----------------------------------------------------------------------

To view the in-context reports for the product backlog select as the backlog level: 
---------------------
Stories for Agile
Issues for Basic
Backlog items for Scrum 
Requirements for CMMI .
---------------------

----------------------
Choose the right project - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/choose-right-project/
----------------------
- Understand different projects and systems to guide the journey
- Select a project to start the DevOps transformation
- Identify groups to minimize initial resistance
- Identify project metrics and Key Performance Indicators (KPI's)

-------------------------------------------------------------------------
PT01-Q06: 

You manage the deployment of an Azure App Service web app named App1 in
multiple Azure regions.

You plan to validate the availability of App1 by using an 
Application Insights availability test.

Which type of test should be implemented by using Microsoft Visual Studio?

- custom TrackAvailability
- Multi-step
- Standard
- URL ping

MY ANSWER TO PT01-Q06
--------------------------------------------------------------------------
?
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q06
--------------------------------------------------------------------------
Multi-step
Multi-step tests in Application Insights must be implemented by using Visual Studio.

URL ping and Standard tests: 
can be implemented directly from the Azure portal.

Custom TrackAvailability tests: 
can be implemented by using App Service editor in the Azure portal.

Monitor app performance - Training | Microsoft Learn

Application Insights availability tests - Azure Monitor | Microsoft Learn
--------------------------------------------------------------------------

REFS:
------------------------
Monitor app performance
https://learn.microsoft.com/en-gb/training/modules/monitor-app-performance/
------------------------
Application Insights feature overview
--------------------------------------
-1 Live Metrics:
Observe activity from your deployed application 
**in real time ** 
** with no effect on the host environment **.

-2 Availability:
Also known as “Synthetic Transaction Monitoring”, probe your applications 
external endpoint(s) to test over time:
**the overall availability**  
**the responsiveness **

-3 GitHub or Azure DevOps integration:	
Create GitHub or Azure DevOps work items in context of Application Insights data.

-4 Usage:	
Understand which features are popular with users and how users interact and use your application

-5 Smart Detection:	
**Automatic failure and anomaly detection** 
through 
**proactive telemetry analysis**.

-6 Application Map:	
A high level **top-down view** of the application architecture 
and 
at-a-glance visual references to component health and responsiveness.

-7 Distributed Tracing:
Search and visualize an **end-to-end flow** of a given execution or transaction.

----------------------------------
What Application Insights monitors
----------------------------------

1- Request rates, response times, and failure rates: 

Find out which pages are most popular, at what times of day, and where your users are. 
See which pages perform best. 
If your response times and failure rates go high when there are more requests, then 
perhaps you have a resourcing problem.

-2 Dependency rates, response times, and failure rates: 
 Find out whether external services are slowing you down.

-2 Exceptions:
Analyze the aggregated statistics, or pick specific instances and 
drill into the stack trace and related requests. 
Both server and browser exceptions are reported.

- Page views and load performance: reported by your users' browsers.
- AJAX calls from web pages: rates, response times, and failure rates.
- User and session counts.
- Performance counters:
  from your Windows or Linux server machines, such as CPU, memory, and network usage.
- Host diagnostics: from Docker or Azure.
- Diagnostic trace logs: 
  from your app - so that you can correlate trace events with requests.
-Custom events and metrics: 
 that you write yourself in the client or server code, to track business events
 such as items sold or games won.

---------------------------------------------
Getting started with Application Insights
---------------------------------------------
-At run time: 
instrument your web app on the server. Ideal for applications already deployed. 
Avoids any update to the code.

- At development time: 
add Application Insights to your code. 
Allows you to customize telemetry collection and send more telemetry.

- Instrument your web pages: 
for page view, AJAX, and other client-side telemetry.

-Analyze mobile app usage: by integrating with Visual Studio App Center.
*********************
-Availability tests - ping your website regularly from our servers.
*********************

----------------------------------------
Application Insights log-based metric
----------------------------------------
let you 
- analyze the health of your monitored apps
- create powerful dashboards
- configure alerts. 

There are two kinds of metrics:

---------------------
1- Standard metrics:
---------------------
   are stored as pre-aggregated time series.
   Since standard metrics are pre-aggregated during collection, they have 
   better performance at query time. 
   Standard metrics are a better choice for dashboarding and in real-time alerting.

   The pre-aggregated metrics aren't stored as individual events with lots of properties.
   Instead, they're stored as pre-aggregated time series, and only with key dimensions.
   This makes the new metrics superior at query time.
   This enables new scenarios such as near real-time alerting on dimensions of metrics, 
   more responsive dashboards, and more.

   The newer SDKs (Application Insights 2.7 SDK or later for .NET) pre-aggregate metrics during collection. 
   This applies to standard metrics sent by default so the accuracy isn't affected by sampling or filtering. 
   
   ------------------------------------
   Pre-Aggregration at the AI backend
   ------------------------------------
   For the SDKs that don't implement pre-aggregation the Application Insights backend 
   still populates the new metrics by aggregating the events received by the Application Insights event collection endpoint. 

---------------------
2- Log-based metrics:
---------------------
   behind the scene are translated into **Kusto queries** from stored events.
   The log-based metrics have more dimensions, which makes them the superior 
   option for data analysis and ad-hoc diagnostics.
   
   Developers can use the SDK to send events manually by writing code that explicitly
   invokes the SDK or they can rely on the automatic collection of events from 
   auto-instrumentation. 
 
   The Application Insights backend stores all collected events as logs.

    For situations when the volume of events is too high, Application Insights 
    implements several ** telemetry volume reduction techniques, such as:
    sampling and filtering **.
    Behind the scenes, must perform query-time aggregations of the events stored in logs.

------------------------
Application Insights availability tests
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-overview
------------------------
After you've deployed your web app or website, you can set up recurring tests 
to monitor availability and responsiveness.  Application Insights sends web requests
to your application at regular intervals from points around the world. It can alert
you if your application isn't responding or responds too slowly.

You can set up availability tests for any HTTP or HTTPS endpoint that's 
accessible from the public internet. You don't have to make any changes 
to the website you're testing. In fact, it doesn't even have to be a site
that you own.

-----------------------------
Types of Availability Tests
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-overview#types-of-tests
-----------------------------
-----------------
1-Standard Tests
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-standard-tests
-----------------
This single request test is similar to the URL ping test. It includes 
- TLS/SSL certificate validity
- proactive lifetime check 
- HTTP request verb (for example, GET, HEAD, or POST)
- custom headers
- custom data associated with your HTTP request.

----------------------------------
2-Custom TrackAvailability test:
https://learn.microsoft.com/en-gb/azure/azure-monitor/app/availability-azure-functions
----------------------------------
If **you decide to create a custom application to run availability tests**.
You can use the TrackAvailability() method to send the results to Application Insights.

----------------------------------
3-Classic tests (older versions of availability tests

3a-Monitor availability with URL ping tests
https://learn.microsoft.com/en-gb/previous-versions/azure/azure-monitor/app/monitor-web-app-availability

The name URL ping test is a bit of a misnomer. These tests don't use the 
Internet Control Message Protocol (ICMP) to check your site's availability.

3b-Multi-step web test (deprecated): 
https://learn.microsoft.com/en-gb/previous-versions/azure/azure-monitor/app/availability-multistep
----------------------------------

The older classic tests, URL ping test and multi-step web test, rely on the 
DNS infrastructure of the public internet to resolve the domain names of 
the tested endpoints. If you're using private DNS, you must ensure that 
the public domain name servers can resolve every domain name of your test. 
***************************************************************************
When that's not possible, you can use custom TrackAvailability tests instead.
***************************************************************************

-------------------------------------------------------------------------
PT01-Q05: 

You plan to implement GitHub secrets.

At which two levels can secrets be created? 
Each correct answer presents a complete solution.


action
organization
repository
step
workflow

MY ANSWER TO PT01-Q05
--------------------------------------------------------------------------
(By ChatGPT)
repository
organization
--------------------------------------------------------------------------

CORRECT ANSWER TO PT01-Q05
--------------------------------------------------------------------------
organization
repository
--------------------------------------------------------------------------

REFS:

GitHub secrets can be created at the organization and repository levels.
You can use secrets in a step of an action within a workflow, but you cannot
create them at any of the other three levels.

-----------
Create encrypted secrets - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/8-create-encrypted-secrets
-----------
Learn continuous integration with GitHub Actions
https://learn.microsoft.com/en-gb/training/modules/learn-continuous-integration-github-actions/
-----------


----------
ChatGPT
----------

GitHub secrets can be created at two levels:

-1 Repository Level: 

You can create secrets specific to a repository. 
These secrets are accessible to the workflows and actions within that particular 
repository.

-1 Organization Level: 

You can also create secrets at the organization level. 
These secrets are accessible to multiple repositories within the same organization, 
making it convenient to share common secrets among related projects.

So, the correct answers are:
repository
organization

-------------------------------------------------------------------------
PT00-Q04: 

You are developing the security validation plan for an application’s lifecycle.
For which activity should you include a ** passive penetration ** test?

 - continuous deployment
 - continuous integration
 - IDE/pull requests
 - nightly test runs

MY ANSWER TO PT00-Q04
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

CORRECT ANSWER TO PT00-Q04: continuous deployment
--------------------------------------------------------------------------


--------------------------------------------------------------------------

REFS:

---------------------------------------
 The IDE environment/pull request step should include:
 ---------------------------------------

- static code analysis 
- code reviews (manual check).

---------------------------------------
 Continuous integration should include:
 ---------------------------------------

 - Open-Source Software (OSS) vulnerability scan. 

---------------------------------------
Nightly test runs should include:
---------------------------------------

 - infrastructure scans 
 - **active** penetration tests 

---------------------------------------
Continuous deployment should include: 
---------------------------------------

- SSL scan 
- infrastructure scans. 
- **passive** penetration tests 

-----------------------------
Microsoft Security Development Lifecycle (SDL)
Threat modeling is a core element of the Microsoft Security Development Lifecycle (SDL).
https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle
-----------------------------

Explore key validation points - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/5-explore-key-validation-points?pivots=bash


Control releases with deployment gates - Azure Pipelines | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/pipelines/release/approvals/gates?view=azure-devops

-------
ChatGPT
-------

EXAM Q/A:

A **passive penetration test** is typically included in the **continuous integration (CI)** 
phase of an application's lifecycle. In CI, developers frequently integrate code changes into a 
shared repository, and automated security checks, including **passive penetration** tests, can
be performed to detect vulnerabilities as early as possible in the development process. 

**Passive penetration tests are non-intrusive** and focus on identifying vulnerabilities and 
security issues without actively exploiting them.

So, the correct answer is: continuous integration

-----------
QUESTION: 

What is the difference between a passive penetration test and active penetration test and in 
which steps of the development lifecycle each of these should be used according to the 
Microsoft Security Development Lifecycle (SDL)?
-----------

Passive and active penetration tests are two different approaches to testing the security of an
application or system, and they serve distinct purposes within the development lifecycle, 
as outlined in the Microsoft Security Development Lifecycle (SDL).

---------------------------
Passive Penetration Test:
---------------------------

Purpose: Passive penetration tests are non-intrusive tests designed to identify vulnerabilities
and security weaknesses without actively exploiting them. 
These tests involve the analysis of code, architecture, configurations, and network traffic to
find potential issues.

---------------
** When to Use: 
---------------
Passive penetration tests are typically used in the early stages of the development lifecycle, 
such as during the requirements and design phases of the SDL. They can also be used during the
continuous integration (CI) phase to identify security issues before they become more critical.

-----------------------
Active Penetration Test:
-----------------------

Purpose: Active penetration tests are more aggressive and involve simulating real-world attacks
to actively exploit vulnerabilities and assess the effectiveness of security controls. They aim
to provide a realistic view of how an application or system would withstand attacks.

---------------
** When to Use:
---------------

Active penetration tests are typically conducted in later phases of the development lifecycle 
when the application or system is closer to completion. This includes the testing and verification
phases of the SDL. They can also be performed during the operational phase to assess ongoing security.

-------------------------------------------------------------------------
PT00-Q03: 

https://learn.microsoft.com/en-gb/credentials/certifications/exams/az-400/practice/assessment?assessment-type=practice&assessmentId=56

You are developing a security validation plan for an application’s lifecycle.
For which activity should you include an 
automated Open-Source Software (OSS) vulnerability scan?

A03-1: continuous deployment
A03-2: continuous integration
A03-3: IDE/pull requests
A03-4: nightly test runs

MY ANSWER TO PT00-Q03
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

CORRECT ANSWER TO PT00-Q03
--------------------------------------------------------------------------
continuous integration
--------------------------------------------------------------------------

REFS:

--------------------------------------------------------------------
Continuous integration 
should include an OSS vulnerability scan. 
--------------------------------------------------------------------
The integrated development environment / pull request step 
should include 
static code analysis and code reviews. 
--------------------------------------------------------------------
Nightly test runs 
should include 
an infrastructure scan. 
--------------------------------------------------------------------
Continuous deployment 
should include 
passive penetration tests, an SSL scan, and an infrastructure scan.
--------------------------------------------------------------------

AZ-400: Implement security and validate code bases for compliance  Introduction to Secure DevOps 

Explore key validation points - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/5-explore-key-validation-points?pivots=bash


Continuous security validation should be added at each step from development 
through production to help ensure the application is always secure.
This approach aims to switch the conversation with the security team
from approving each release to consenting to the CI/CD process and monitor 
and audit the process at any time.

-------------------
IDE / pull request
-------------------
Validation in the CI/CD begins before the developer commits their code.
** Static code analysis tools in the IDE ** provide the first line of 
defense to help ensure that security vulnerabilities aren't introduced 
into the CI/CD process.

In ** Azure DevOps Enabling branch policies ** on the shared branch requires 
a ** pull request ** to start the merge process and ensure the execution of 
all defined controls.

The pull request should require a ** code review **, the one manual but 
important check for identifying new issues introduced into your code.

Along with **this manual check **, commits should be linked to **work items** 
for auditing why the code change was made and require a continuous integration (CI) 
build process to succeed before the push can be completed.

---------------------------------------
Explore continuous security validation
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/6-explore-continuous-security-validation?pivots=bash
---------------------------------------
developers don't hesitate to use components available in public package sources 
(such as npm or NuGet).
as the dependency on these third-party OSS components increases, the risk of security
vulnerabilities or hidden license requirements also increases compliance issues.
For a business, it's critical, as issues related to compliance, liabilities, and 
customer personal data can cause many privacy and security concerns.

Many tools can scan for these vulnerabilities within 
the build and release pipelines.
**
Once the merge is complete, the CI build should execute as part of the pull request (PR-CI) process.
These CI builds should run static code analysis tests to ensure that the code 
follows all rules for both maintenance and security.
**

----------------------
STATIC CODE ANALYSIS
----------------------
Many of the tools seamlessly integrate into the Azure Pipelines build process. 

-1 SonarQube.
-2 Visual Studio Code Analysis and the Roslyn Security Analyzers.
-3 Checkmarx - A Static Application Security Testing (SAST) tool.
-4 BinSkim - A binary static analysis tool that provides security and correctness results for Windows portable executables and many more.

----------------------
3rd Party Packages Vulnerability scanner
&
OSS licenses
----------------------

Organizations try to manage third-party packages vulnerabilities or OSS licenses. 
** Mend Software's tools** can make this identification process almost instantaneous.

-----------------------------
Understand threat modeling
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/7-understand-threat-modeling?pivots=bash
-----------------------------
Microsoft Security Development Lifecycle (SDL)
Threat modeling is a core element of the Microsoft Security Development Lifecycle (SDL).
https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle
-----------------------------

It's an engineering technique you can use to help you identify threats, attacks, vulnerabilities, 
and countermeasures that could affect your application.
It is used to REDUCE RISK With non-security experts in mind.
The tool makes threat modeling easier for all developers by providing clear guidance on creating
and analyzing threat models. 
iT MAKES IT easier for all developers through a **standard notation** 
for ** visualizing **:
-------------------------------------------------------
- system components
- data flows
- security boundaries.
-------------------------------------------------------

Helps threat modelers identify classes of threats they should consider based 
on the structure of their software design.

-----------------------------------------------------------------------
The Threat Modeling Tool enables any developer or software architect to:
-----------------------------------------------------------------------

-1 Communicate about the security design of their systems.
-2 Analyze those designs for potential security issues using a proven methodology.
-3 Suggest and manage mitigation for security issues.

-------------------------------------------
There are five major threat modeling steps:
https://learn.microsoft.com/en-us/azure/security/develop/threat-modeling-tool
Exercise threat modeling
https://learn.microsoft.com/en-gb/training/modules/introduction-to-secure-devops/8-exercise-threat-modeling?pivots=bash
-------------------------------------------

-1 Defining security requirements.
-2 Creating an application diagram.
-3 Identifying threats.
-4 Mitigating threats.
-5 Validating that threats have been mitigated.

---------------------------------------------------------
Review code coverage - Azure Pipelines | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/pipelines/test/review-code-coverage-results?view=azure-devops#artifacts
---------------------------------------------------------

-------------------------------------------------------------------------
PT00-Q02: 


You need to implement a Git hook that will be triggered automatically in 
response to a Git commit being run on a Windows server.

To what should you set the first line of the predefined Git hook script 
so that you can execute the script?

A. #!/bin/sh
B. #!\bin\sh
C. #!C:/Program\ Files/Git/usr/bin/sh.exe
D. #!C:\Program Files\Git\usr\bin\sh.exe

MY ANSWER:
--------------------------------------------------------------------------
 
--------------------------------------------------------------------------

CORRECT ANSWER: C: #!C:/Program\ Files/Git/usr/bin/sh.exe

--------------------------------------------------------------------------

To execute any of the predefined Git hook scripts, the existing line of 
#!/bin/sh 

needs to be replaced with the one pointing to the location of the Bash 
script interpreter. The notation must use the forward slashes and an escape
 character to account for the space character in the path.

Implement Git hooks - Training | Microsoft Learn
https://learn.microsoft.com/en-gb/training/modules/explore-git-hooks/3-implement

Service hooks events - Azure DevOps | Microsoft Learn
https://learn.microsoft.com/en-gb/azure/devops/service-hooks/events?view=azure-devops

--------------------------------------------------------------------------

REFS:

https://learn.microsoft.com/en-us/training/modules/explore-git-hooks/

Git hooks are a mechanism that allows code to be run before or after certain
Git lifecycle events. For example, one could hook into the commit-msg event 
to validate that the commit message structure follows the recommended format.

The hooks can be any executable code, including shell, PowerShell, Python, 
or other scripts. 
Or they may be a binary executable. 
Anything goes!

The only criteria are that hooks must be stored in the .git/hooks folder in 
the repo root. 
Also, they must be named to match the related events (Git 2.x):

applypatch-msg
pre-applypatch
post-applypatch
pre-commit
prepare-commit-msg
commit-msg
post-commit
pre-rebase
post-checkout
post-merge
pre-receive
update
post-receive
post-update
pre-auto-gc
post-rewrite
pre-push

you can use hooks to:

-1 enforce policies
-2 ensure consistency 
-3 control your environment:

Examples:

-1 Enforcing preconditions for merging
-2 Verifying work Item ID association in your commit message
-3 Preventing you & your team from committing faulty code
-4 Sending notifications to your team's chat room (Teams, Slack, HipChat, etc.)

----------------------
Implement Git hooks
https://learn.microsoft.com/en-us/training/modules/explore-git-hooks/3-implement
----------------------

------------------------------------
Client-Side Git Hooks on Windows.
------------------------------------

Git ships with several sample hook scripts in the repo .git\hooks directory. 

If you open that folder, you'll find a file called 
 ** pre-commit.sample ** 
To enable it, rename it to 
** pre-commit ** 
by removing the .sample extension 
and making the script executable.

The script is found and executed when you attempt to commit using git commit.
You commit successfully if your pre-commit script exits with a 0 (zero). 

**************************************************************
If you're using Windows, simply renaming the file won't work.
**************************************************************
Git will fail to find the shell in the chosen path specified in the script.
The problem is lurking in the first line of the script, 
the shebang declaration:
-----------
#!/bin/sh
-----------

On Unix-like OSs, the SheBang: #! 

Tells the program loader that it's a script to be interpreted, and /bin/sh 
is the path to the interpreter you want to use, sh in this case.

---------------------------------
Windows isn't a Unix-like OS. 
---------------------------------

Git for Windows supports Bash commands and shell scripts via ** Cygwin **.

Fix it by providing the path to the sh executable on your system. 
It's using the 64-bit version of Git for Windows, so the baseline looks like this:

------------------------------------------
#!C:/Program\ Files/Git/usr/bin/sh.exe
------------------------------------------

```
#!C:/Program\ Files/Git/usr/bin/sh.exe
matches=$(git diff-index --patch HEAD | grep '^+' | grep -Pi 'password|keyword2|keyword3')
if [ ! -z "$matches" ]
then
    cat <<\EOT
Error: Words from the blocked list were present in the diff:
EOT
    echo $matches
    exit 1
fi
```

The repo ** .git\hooks ** folder isn't committed into source control. 
You may wonder how you share the goodness of the automated scripts you create with the team.
The good news is that, from Git version 2.9, you can now map Git hooks to a folder that 
can be committed into source control.

You could do that by updating the global settings configuration for your Git repository:
-------------------------------------------------------
Git config --global core.hooksPath '~/.githooks'
-------------------------------------------------------

----------------------------------------------
Server-side service hooks with Azure Repos
https://learn.microsoft.com/en-us/azure/devops/service-hooks/events?view=azure-devops
----------------------------------------------

So far, we've looked at the client-side Git Hooks on Windows. 
Azure Repos also exposes server-side hooks. 
Azure DevOps uses the exact mechanism itself to create Pull requests. 
You can read more about it at the Server hooks event reference.


-------------------------------------------------------------------------
PT00-Q01: 

You need to minimize the response time when using Azure DevOps Git-based 
repositories that contain large files.

Which Git extension should you use?

A. Git LFS
B. Git Machete
C. GitFlow
D. GitX

MY ANSWER:
--------------------------------------------------------------------------
A00-1: Git LFS
--------------------------------------------------------------------------

CORRECT ANSWER:
--------------------------------------------------------------------------
A00-1: Git LFS

-1 Git LFS 
is a Git extension that provides the fastest response time when using 
Azure DevOps Git-based repositories that contain large files. 

-2 GitFlow 
is a Git extension that implements the GitFlow branching model. 

-3 Git Machete 
is a Git extension that simplifies and automates repository organization.

-4 GitX 
is a Git extension that provides an improved development workflow.
--------------------------------------------------------------------------

REFS:
Learn  Training  Browse  AZ-400: Development for enterprise DevOps  Manage Git repositories 
Work with large repositories
https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/2-work-large-repositories

Why repositories become large?
There are two primary causes for large repositories:

-1 Long history
-2 Large binary files
-3 You can also reduce clones by filtering branches or cloning only a single branch.

--------------
Shallow clone
--------------

If developers don't need all the available history in their local repositories, a good option is 
to implement a shallow clone. It saves both space on local development systems and the time it 
takes to sync.

-------------------------------------
git clone --depth [depth] [clone-url]
-------------------------------------



------------
VFS for Git
https://github.com/microsoft/VFSForGit
VFS stands for Virtual File System. 
------------

VFS for Git helps with large repositories. It requires a Git LFS client.
VFS for Git virtualizes the file system beneath your Git repository so that Git and all tools
see what appears to be a regular working directory, but VFS for Git only downloads objects as 
they are needed.

Typical Git commands are unaffected, but the Git LFS works with the standard filesystem to 
download necessary files in the background when you need files from the server.

---------
Scalar
Introducing Scalar: Git at scale for everyone
https://devblogs.microsoft.com/devops/introducing-scalar/
---------

Scalar is a .NET Core application available for Windows and macOS. 
With tools and extensions for Git to allow very large repositories to maximize 
your Git command performance.

By default each Git repository has a copy of all files in the entire history.

It achieves by enabling some advanced Git features, such as:

- Partial clone: 
reduces time to get a working repository by not downloading all Git objects right away.

-Background prefetch: 
downloads Git object data from all remotes every hour, reducing the time for foreground git fetch calls.

-Sparse-checkout: limits the size of your working directory.

-File system monitor: 
tracks the recently modified files and eliminates the need for Git to scan the entire work tree.

-Commit-graph: 
accelerates commit walks and reachability calculations, speeding up commands like git log.

-Multi-pack-index: enables fast object lookups across many pack files.

-Incremental repack: 
Repacks the packed Git data into fewer pack files without disrupting concurrent commands using the multi-pack-index.

-----------------------
Purge repository data
https://learn.microsoft.com/en-gb/training/modules/manage-git-repositories/3-purge-repository-data
-----------------------

While one of the benefits of Git is its ability to hold long histories 
for repositories efficiently, there are times when you need to purge data.

The most common situations are where you want to:

-1 Significantly reduce the size of a repository by removing history.
-2 Remove a large file that was accidentally uploaded.
-3 Remove a sensitive file that shouldn't have been uploaded.

If you commit sensitive data (for example, password, key) to Git, it can be removed from history.
Two tools are commonly used:

---------------------------------------------
TOOL-1
https://github.com/newren/git-filter-repo
```
git filter-repo tool
```
---------------------------------------------
The **git filter-repo** is a tool for **rewriting history.**

Its core filter-repo contains a library for creating history rewriting tools. 
Users with specialized needs can quickly create entirely new history rewriting tools.

---------------------------------------------
TOOL-2
BFG Repo-Cleaner
---------------------------------------------
BFG Repo-Cleaner is a commonly used open-source tool for deleting or "fixing" 
content in repositories. 
It's easier to use than the **git filter-branch** command. 

For a single file or set of files, use the --delete-files option:

```
$ bfg --delete-files file_I_should_not_have_committed
```

The following bash shows how to find all the places that a file called passwords.txt exists 
in the repository. 
Also, to replace all the text in it, you can execute the **--replace-text** option:

```
$ bfg --replace-text passwords.txt
```

---------------------------------
GIT FILTER BRANCH: git filter-branch 
https://git-scm.com/docs/git-filter-branch

---------------------------------
GIT FILTER REPO: git filter-repo  
https://github.com/newren/git-filter-repo/
---------------------------------

---------------------------------
BFG Repo-Cleaner
https://rtyley.github.io/bfg-repo-cleaner/
---------------------------------

---------------------------------
GitHub Docs:
Removing files from Git Large File Storage
https://docs.github.com/en/repositories/working-with-files/managing-large-files/removing-files-from-git-large-file-storage
---------------------------------
Remove the file from the repository's Git history using either 
the filter-repo command or BFG Repo-Cleaner.

---------------------------------
GitHub Docs:
Removing sensitive data from a repository
---------------------------------
If you commit sensitive data, such as a password or SSH key into a Git repository,
you can remove it from the history. To entirely remove unwanted files from a 
repository's history you can use either the git filter-repo tool or the 
BFG Repo-Cleaner open source tool.

The git **filter-repo** tool and the **BFG Repo-Cleaner** rewrite your repository's
history, which changes the SHAs for existing commits that you alter and any 
dependent commits. 

Changed commit SHAs may affect open pull requests in your repository. 
We recommend merging or closing all open pull requests before removing 
files from your repository.

----------------------------------------------------------
Removing a file added in the most recent unpushed commit
https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-a-file-added-in-the-most-recent-unpushed-commit
----------------------------------------------------------

--------------------------------------
What is continuous delivery?
Progressive exposure techniques: Controlling the Blast Radius
https://learn.microsoft.com/en-us/devops/deliver/what-is-continuous-delivery#progressive-exposure-techniques
--------------------------------------
CD supports several patterns for progressive exposure, also called "controlling the blast radius." 
These practices limit exposure to deployments to avoid risking problems with the overall user base.

----------------------------
-1 multiple deployment rings:
CANARY => following rings
----------------------------
A ring tries a deployment on a user group, and monitors their experience. 

RING-1
The first deployment ring can be a canary to test new versions in production 
before a broader rollout. 
CD automates deployment from one ring to the next.

RING-2
Deployment to the next ring can optionally depend on a manual approval step, 
where a decision maker signs off on the changes electronically. 

CD can create an auditable record of the approval to satisfy regulatory 
procedures or other control objectives.

----------------------------
-2 Blue/green deployment
via Load Balancing
----------------------------
relies on keeping an existing blue version live while a new green version deploys.
This practice typically uses load balancing to direct increasing amounts of traffic
to the green deployment. 

If monitoring discovers an incident, traffic can be rerouted to the blue 
deployment still running.

----------------------------
-3 Feature Flags/Toggles
----------------------------

Feature flags or feature toggles are another technique for experimentation 
and dark launches. Feature flags turn features on or off for different user
groups based on identity and group membership.


--------------------------------------------------------------------------

